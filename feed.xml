<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.6.3">Jekyll</generator><link href="/feed.xml" rel="self" type="application/atom+xml" /><link href="/" rel="alternate" type="text/html" /><updated>2019-08-21T18:08:51+02:00</updated><id>/</id><title type="html">ing. Andrea Maglie</title><subtitle></subtitle><author><name>Andrea Maglie</name></author><entry><title type="html">Pure Data e i modelli audio</title><link href="/pure-data-modelli-audio.html" rel="alternate" type="text/html" title="Pure Data e i modelli audio" /><published>2019-01-29T00:00:00+01:00</published><updated>2019-01-29T00:00:00+01:00</updated><id>/pure-data-modelli-audio</id><content type="html" xml:base="/pure-data-modelli-audio.html">&lt;h2 id=&quot;il-modello-di-rotolamento&quot;&gt;Il modello di rotolamento&lt;/h2&gt;

&lt;p&gt;Tra le comuni interazioni meccaniche che coinvolgono oggetti solidi, il
rotolamento forma una categoria interessante anche dal punto di vista
dell’audio: l’esperienza di tutti i giorni ci dice che il suono prodotto
da un oggetto rotolante viene spesso riconosciuto come tale, e in
generale è distinto da altri suoni come quelli dovuti allo sfregamento
anche degli stessi oggetti. Ciò potrebbe essere dovuto alla natura del
rotolamento come un processo di interazione continua, dove la forza
mutua sugli oggetti coinvolti è descritta come un impatto senza
l’aggiunta di forze di frizione perpendicolari. Oltre ad essere
caratteristici, i suoni di rotolamento portano importanti informazioni:
in aggiunta alle caratteristiche di risonanza degli oggetti coinvolti
(che dipendono da forma, dimensione e materiale), altri attributi
vengono espressi nel suono, attributi &lt;em&gt;di trasformazione&lt;/em&gt;, come
velocità, gravità o accelerazione/decelerazione. Lo sviluppo di un
modello di rotolamento espressivo e in tempo reale da presupposti
fisici, acustici e implementativi è descritto di seguito.&lt;/p&gt;

&lt;h3 id=&quot;linterazione-di-rotolamento-con-il-modello-di-impatto-come-blocco-di-base&quot;&gt;L’interazione di rotolamento con il modello di impatto come blocco di base&lt;/h3&gt;

&lt;figure class=&quot;image&quot;&gt;
  &lt;img src=&quot;images/rolling1.jpg&quot; alt=&quot;Figura 1: Tracciato del movimento di una palla che segue un profilo di
superficie s(x). Non si tratta del moto reale ma di una idealizzazione utile per ricavare la curva usata dal modello di impatto&quot; /&gt;
  &lt;figcaption&gt;Figura 1: Tracciato del movimento di una palla che segue un profilo di
superficie s(x). Non si tratta del moto reale ma di una idealizzazione utile per ricavare la curva usata dal modello di impatto&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Contrariamente ad azioni quali lo sfregare o il grattare, la forza di
interazione dei due oggetti coinvolta in un semplice scenario di
rotolamento (l’oggetto rotolante e il piano) è perpendicolare alla
superficie di contatto (la curva media macroscopica), diretta lungo la
linea che connette il punto di contatto e il centro di gravità
dell’oggetto rotolante. Le condizioni di contatto devono essere
modificate per riflettere le varie distanze della superficie di contatto.
L’oggetto rotolante è assunto come localmente sferico, senza dettagli
macroscopici sulla superficie. E’ possibile fare queste assunzioni dal
momento che i dettagli microscopici della superficie dell’oggetto
rotolante possono essere semplicemente aggiunti alla superficie sulla
quale l’oggetto rotola, e può essere variato il raggio di curvatura
della superficie stessa; vedremo che anche l’assumere un raggio costante
può essere soddisfacente per la maggior parte degli scopi. E’ importante
notare che il contatto tra i due oggetti durante il rotolamento è
ristretto a punti distinti: il piano non viene seguito nella sua
interezza.&lt;/p&gt;

&lt;p&gt;Il movimento reale dell’oggetto rotolante si differenzia da questa
idealizzazione a causa dell’elasticità e dell’inerzia. In buona
approssimazione, il movimento verticale del centro della palla è
calcolato con un modello di impatto unidimensionale con la curva in figura 1. I
punti di contatto e la traiettoria risultante, che idealmente dovrebbe
essere applicata al modello di impatto unidimensionale, sono
rappresentati in figura 2. Il calcolo esatto dei punti di contatto è
dispendioso in termini di risorse computazionali: in ogni punto &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt;
lungo la curva della superficie, cioè per ogni punto di campionamento
nel caso discreto (dove la frequenza del campionamento è la stessa del
campionamento audio), deve essere calcolata la seguente funzione che
descrive l’attuale punto &lt;script type=&quot;math/tex&quot;&gt;p_x&lt;/script&gt; :&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f_x(p_x) \stackrel{!}{=} max_{q\in[x-r,x+r]}f_x(q) \hspace{0,5cm}, \label{eq:rolling1}&lt;/script&gt;

&lt;p&gt;dove&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f_x(q) = s(q) + \sqrt{r^2 - (q-x)^2} \hspace{0,5cm},\hspace{0,5cm} q \in [x-r, x+r] \hspace{0,5cm}. \label{eq:rolling2}&lt;/script&gt;

&lt;p&gt;La curva ideale viene poi calcolata da questi punti di contatto. Una tecnica più semplice (e quindi anche meno dispendiosa in termini di risorse di calcolo) è rappresentata in figura 3.
La traiettoria in figura 2 converge alla curva ideale di figura 3 per raggi molto grandi se comparati alla ruvidità della superficie. 
Infatti, in una prima implementazione, anche le forti semplificazioni (riportate figura 4 realizzate con un algoritmo molto semplice, hanno dato risultati
convincenti.&lt;/p&gt;

&lt;figure class=&quot;image&quot;&gt;
  &lt;img src=&quot;images/rolling2.jpg&quot; alt=&quot;Figura 2: Tracciato della curva di offset effettiva risultante dalla superficie s(x).&quot; /&gt;
  &lt;figcaption&gt;Figura 2: Tracciato della curva di offset effettiva risultante dalla superficie s(x).&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;figure class=&quot;image&quot;&gt;
  &lt;img src=&quot;images/rolling3.jpg&quot; alt=&quot;Figura 3: Approssimazione del tracciato compiuto dalla palla durante il rotolamento.&quot; /&gt;
  &lt;figcaption&gt;Figura 3: Approssimazione del tracciato compiuto dalla palla durante il rotolamento.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;figure class=&quot;image&quot;&gt;
  &lt;img src=&quot;images/rolling4.jpg&quot; alt=&quot;Figura 4: Ulteriore approssimazione del tracciato di rotolamento.&quot; /&gt;
  &lt;figcaption&gt;Figura 4: Ulteriore approssimazione del tracciato di rotolamento.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h3 id=&quot;sec:superficie&quot;&gt;La superficie&lt;/h3&gt;

&lt;p&gt;Esistono diverse tecniche per realizzare il profilo della superficie
alla base del modello di rotolamento. Una possibilità è quella di
campionare o effettuare una scansione di superfici reali e usare tali
segnali come input per lo stadio seguente del modello; questo approccio
però non si adatta ai nostri obiettivi: noi siamo interessati ad un
modello parametrico, flessibile ed efficiente piuttosto che ad una
singola simulazione realistica. Inoltre i segnali memorizzati sono
difficili da adattare alle variazioni degli attributi del modello;
preferiamo quindi usare modelli statistici di superfici che possano
efficientemente generare segnali per i vari attributi.&lt;/p&gt;

&lt;p&gt;E’ comune nella computer graphics descrivere le superfici tramite metodi
frattali. L’applicazione di questa idea al nostro modello
unidimensionale conduce all’utilizzo di un segnale di rumore con spettro
di potenza &lt;script type=&quot;math/tex&quot;&gt;1/f^{\beta}&lt;/script&gt;, o equivalentemente rumore bianco filtrato con
queste caratteristiche. Il parametro reale &lt;script type=&quot;math/tex&quot;&gt;\beta&lt;/script&gt; riflette la
dimensione frattale (o ruvidità). I risultati pratici di questo tipo di
modello sono diventati più convincenti quando la banda del segnale della
superficie è stata fortemente limitata; ciò non deve sorprendere se
pensiamo che solitamente le superfici coinvolte nel rotolamento sono
molto smussate. Smussare su larga scala (che può essere assimilato al
tagliare pezzi di pietra per pavimentazioni) corrisponde ad un
filtraggio passa–alto, mentre smussare a livello microscopico (come
lucidare una pietra) può essere visto come un filtraggio di tipo
passa–basso. Tramite queste elaborazioni però si possono perdere le
caratteristiche del rumore &lt;script type=&quot;math/tex&quot;&gt;1/f^{\beta}&lt;/script&gt; di partenza. Perciò optiamo per
una approssimazione di questa curva con un filtro del secondo ordine la
cui ripidità è proporzionale al grado di ruvidità a livello
microscopico.&lt;/p&gt;

&lt;p&gt;Tutte le frequenze in questo modello di basso livello devono variare
proporzionalmente ai parametri di velocità, perciò l’ampiezza del
segnale di superficie deve essere mantenuta costante. Naturalmente i
parametri dell’impatto, in particolare la costante di elasticità &lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt;,
devono essere variati opportunamente a seconda della superficie che si
vuole simulare (cioè in base alle proprietà del materiale), in quanto
contribuiscono fortemente alla espressività del modello.&lt;/p&gt;

&lt;h3 id=&quot;il-modello-di-impatto&quot;&gt;Il modello di impatto&lt;/h3&gt;

&lt;p&gt;Un suono di contatto è descritto tramite due sistemi, uno per l’oggetto
risonante e uno per l’oggetto percussore. Supposto che la superficie di
contatto sia piccola, la forza di contatto viene espressa come:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
f(x(t),v(t)) = \left\{
                                            \begin{array}{ll}
                                            kx(t)^{\alpha}+\lambda x(t)^{\alpha}\cdot v(t) = kx(t)^{\alpha}(1+\mu v(t)) &amp; x &gt; 0\\
                                            0 &amp; x \leq 0
                                            \end{array}
                                \right.
\hspace{0,5cm}, \label{eq:impact1} %]]&gt;&lt;/script&gt;

&lt;p&gt;dove &lt;script type=&quot;math/tex&quot;&gt;v(t) = \dot{x}(t)&lt;/script&gt; è la velocità di compressione, &lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt; è il
coefficiente di rigidità, &lt;script type=&quot;math/tex&quot;&gt;\alpha&lt;/script&gt; è un parametro che descrive la
geometria locale dell’impatto (nel caso di due perfette sfere vale 1.5),
&lt;script type=&quot;math/tex&quot;&gt;\lambda&lt;/script&gt; è un coefficiente di smorzamento e &lt;script type=&quot;math/tex&quot;&gt;\mu = \lambda/k&lt;/script&gt; è un
termine matematico (senza significato fisico) detto &lt;em&gt;caratteristica
viscoelastica&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Il percussore è considerato una massa ideale, quindi l’unico parametro
che lo caratterizza è la massa; il risonatore invece è un oggetto modale
ed è caratterizzato dai parametri di frequenza, tempi di decadimento,
&lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;\alpha&lt;/script&gt; e &lt;script type=&quot;math/tex&quot;&gt;\lambda&lt;/script&gt;. Si assume inoltre che il percussore abbia un
elevato coefficiente di smorzamento: in tal modo diventa trascurabile
l’energia acustica delle sue vibrazioni, e l’energia viene trasferita al
risonatore che emette il suono. Per una descrizione matematica vengono
sintetizzati i modi di vibrazione (teoricamente infiniti), ognuno dei
quali fornisce un contributo allo spettro del segnale &lt;a class=&quot;citation&quot; href=&quot;#art:soundobj&quot;&gt;(Avanzini, F., Rath, M., &amp;amp; Rocchesso, D., 2003)&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;caratteristiche-di-alto-livello&quot;&gt;Caratteristiche di alto livello&lt;/h3&gt;

&lt;p&gt;Oltre ai parametri di basso livello visti nella sezione precedente, i
tipici moti di rotolamento posseggono caratteristiche a livello
macroscopico che contribuiscono fortemente alla percezione acustica, e
non possono essere descritti come fatto in precedenza. Molte superfici
contengono dei pattern più o meno regolari che non possono essere
classificati come rumore frattale filtrato, e tali periodicità possono
essere verificate attraverso l’esperienza di tutti i giorni: i pavimenti
in pietra, o i solchi pseudoperiodici in molte tavole di legno. Le
singole irregolarità sulla superficie dell’oggetto rotolante possono
essere raggruppate in una sola categoria, dal momento che sono
richiamate periodicamente nel movimento rotatorio. Tale caratteristica
può essere modellata con segnali impulsivi di frequenza costante o
variante in un piccolo intervallo; potrebbero essere utili delle
approssimazioni sinusoidali o polinomiali, con un parametro di
smussamento legato al grado di approssimazione della funzione. Ancora,
le frequenze devono variare proporzionalmente alla velocità.&lt;/p&gt;

&lt;p&gt;Dev’essere fatta un’altra osservazione a livello macroscopico: per
oggetti rotolanti che non sono perfettamente sferici (in maniera
rilevante per il movimento) la velocità del punto di contatto su
entrambe le superfici e l’effettiva forza che preme l’oggetto rotolante
sulla superficie variano periodicamente; devono essere variati questi
due parametri per modellare tale deviazione dalla sfericità perfetta.&lt;/p&gt;

&lt;p&gt;Infine notiamo che, come nell’ascolto di tutti i giorni, gli scenari
acustici del rotolamento di oggetti sono riconosciuti e accettati più
facilmente se sono presenti dinamiche tipiche; ad esempio pensiamo al
suono di una palla che cade e che rimbalza fino a quando non raggiunge
un contatto costante con il suolo: a questo punto il rotolamento diventa
chiaro dal punto di vista uditivo e la velocità media lentamente
diminuisce fino diventare nulla.&lt;/p&gt;

&lt;h2 id=&quot;sec:tessiture_superficie&quot;&gt;Tessiture della superficie&lt;/h2&gt;

&lt;p&gt;Molti dei suoni di contatto ai quali siamo interessati non possono
essere ricreati in modo convincente usando solo modelli deterministici,
come nel caso dei suoni di rotolamento risultanti dalla sequenza di
micro impatti tra due oggetti risonanti, determinati dal profilo della
superficie di contatto. Affrontiamo quindi il problema di effettuare il
rendering delle tessiture di superfici attraverso processi frattali;
tali processi sono molto usati nella computer graphics, dal momento che
forniscono tessiture che sembrano naturali all’occhio umano. Dato che
nei modelli fisici le proprietà delle superfici vengono tradotte
direttamente in segnali di forza e, di conseguenza, in suoni, sembra
naturale seguire lo stesso approccio per modellare le superfici.&lt;/p&gt;

&lt;p&gt;I frattali sono definiti &lt;a class=&quot;citation&quot; href=&quot;#book:fractal&quot;&gt;(Hastings, H. M. &amp;amp; Sugihara, G., 1993)&lt;/a&gt; come geometrie invarianti
rispetto alla scalatura. Sono auto–simili se la scalatura è isotropica
o uniforme in tutte le direzioni, auto–affini se la scalatura è
anisotropica o dipendente dalla direzione, staticamente auto–simili se
sono l’unione di copie di se stessi scalate statisticamente. Più
formalmente, un processo frattale unidimensionale può essere definito
come una generalizzazione della definizione di moto standard Browniano
&lt;a class=&quot;citation&quot; href=&quot;#book:brownian&quot;&gt;(Resnick, S., 1992)&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Il processo stocastico &lt;script type=&quot;math/tex&quot;&gt;x = \{x(t),t \geq 0\}&lt;/script&gt; è un moto standard Browniano se:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;il processo stocastico &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; ha incrementi indipendenti;&lt;/li&gt;
  &lt;li&gt;vale la proprietà
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
x(t) - x(s) \sim N(0,t-s) \hspace{0,5cm} per \hspace{0,5cm} 0 \leq s &lt; t; %]]&gt;&lt;/script&gt;
cioè l’incremento &lt;script type=&quot;math/tex&quot;&gt;x(t) - x(s)&lt;/script&gt; è normalmente distribuito con media
nulla e varianza &lt;script type=&quot;math/tex&quot;&gt;(t-s)&lt;/script&gt;;&lt;/li&gt;
  &lt;li&gt;è vero che &lt;script type=&quot;math/tex&quot;&gt;x(0) = 0.&lt;/script&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;La definizione di moto standard Browniano può essere generalizzata alla
definizione di &lt;em&gt;processo frattale&lt;/em&gt; se l’incremento &lt;script type=&quot;math/tex&quot;&gt;x(t)-x(s)&lt;/script&gt; è
normalmente distribuito con media 0 e varianza proporzionale a
&lt;script type=&quot;math/tex&quot;&gt;(t-s)^{2H}&lt;/script&gt;. Il parametro &lt;em&gt;H&lt;/em&gt; è chiamato &lt;em&gt;esponente di Hurst&lt;/em&gt; e
caratterizza il comportamento del processo frattale rispetto alla
scalatura: se &lt;script type=&quot;math/tex&quot;&gt;x=\{x(t),t \geq 0\}&lt;/script&gt; è un processo frattale con esponente
di Hurst &lt;script type=&quot;math/tex&quot;&gt;H&lt;/script&gt;, allora, per ogni reale &lt;script type=&quot;math/tex&quot;&gt;a &gt; 0&lt;/script&gt;, obbedisce alla seguente
relazione di scala:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;x(t) \stackrel{P}{=} a^{-H}x(at) \hspace{0,5cm} , \label{eq:surface1}&lt;/script&gt;

&lt;p&gt;dove &lt;script type=&quot;math/tex&quot;&gt;\stackrel{P}{=}&lt;/script&gt; denota l’uguaglianza statistica. Questa è la
definizione formale di &lt;em&gt;auto–similirarità statistica&lt;/em&gt;. La famiglia di
processi &lt;script type=&quot;math/tex&quot;&gt;1/f&lt;/script&gt; statisticamente auto–simili, nota anche come rumore &lt;script type=&quot;math/tex&quot;&gt;1/f&lt;/script&gt;, è
composta da processi aventi densità di spettro di potenza &lt;script type=&quot;math/tex&quot;&gt;S_x(\omega)&lt;/script&gt;
proporzionale a &lt;script type=&quot;math/tex&quot;&gt;1/ \omega^{\beta}&lt;/script&gt;, con &lt;script type=&quot;math/tex&quot;&gt;\beta&lt;/script&gt; legato all’esponente di
Hurst &lt;script type=&quot;math/tex&quot;&gt;H&lt;/script&gt; dalla relazione &lt;script type=&quot;math/tex&quot;&gt;\beta = 2H + 1&lt;/script&gt;. Per &lt;script type=&quot;math/tex&quot;&gt;\beta = 0&lt;/script&gt; la
definizione corrisponde al rumore bianco, per &lt;script type=&quot;math/tex&quot;&gt;\beta = 2&lt;/script&gt; si ottiene il
rumore Browniano, e per &lt;script type=&quot;math/tex&quot;&gt;\beta = 1&lt;/script&gt; il rumore risultante è rumore rosa.
Il parametro &lt;script type=&quot;math/tex&quot;&gt;\beta&lt;/script&gt; è in relazione anche con la dimensione frattale. La
dimensione frattale &lt;a class=&quot;citation&quot; href=&quot;#book:wornell&quot;&gt;(Wornell, G. W., 1998)&lt;/a&gt; di una funzione è un parametro reale
che determina l’irregolarità di un oggetto frattale, è legata al grafico
della funzione ed è usata nella computer graphics per controllare la
ruvidità percepita &lt;a class=&quot;citation&quot; href=&quot;#art:pentland&quot;&gt;(Pentland, A. P., 1988)&lt;/a&gt;. Per i processi &lt;script type=&quot;math/tex&quot;&gt;1/f&lt;/script&gt;, tale dimensione
è inversamente proporzionale all’esponente di Hurst &lt;script type=&quot;math/tex&quot;&gt;H&lt;/script&gt;: valori maggiori
di &lt;script type=&quot;math/tex&quot;&gt;H&lt;/script&gt; corrispondono a valori minori della dimensione frattale; &lt;script type=&quot;math/tex&quot;&gt;H&lt;/script&gt; è
proporzionale a &lt;script type=&quot;math/tex&quot;&gt;\beta&lt;/script&gt;. Perciò, incrementando &lt;script type=&quot;math/tex&quot;&gt;\beta&lt;/script&gt; possiamo
raggiungere una redistribuzione della potenza dalle alte alle basse
frequenze, con uno smussamento complessivo della forma d’onda.&lt;/p&gt;

&lt;p&gt;Il problema di generare il rumore &lt;script type=&quot;math/tex&quot;&gt;1/f&lt;/script&gt; è stato trattato estensivamente.
Uno degli approcci più comuni risulta quello di filtrare una sorgente di
rumore bianco per ottenere lo spettro &lt;script type=&quot;math/tex&quot;&gt;1/f&lt;/script&gt;; seguendo questo procedimento
utilizzeremo il modello riportato in &lt;a class=&quot;citation&quot; href=&quot;#art:saletti&quot;&gt;(Saletti, R., Novembre 1986)&lt;/a&gt; e &lt;a class=&quot;citation&quot; href=&quot;#art:corsini&quot;&gt;(Corsini, G. &amp;amp; Saletti, R., Dicembre 1988)&lt;/a&gt;. Il
filtro è una cascata di N filtri del primo ordine, ognuno con una coppia
di poli e zeri; la funzione di trasferimento &lt;script type=&quot;math/tex&quot;&gt;H(s)&lt;/script&gt; nel dominio di
Laplace è la seguente:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;H(s)=A\frac{\prod_{i=1}^{N}(s-s_{0i})}{\prod_{i=1}^{N}(s-s_{pi})} \hspace{0,5cm} ,  \label{eq:surface2}&lt;/script&gt;

&lt;p&gt;dove &lt;script type=&quot;math/tex&quot;&gt;A&lt;/script&gt; è una costante. Il generatore di rumore frattale è ottenuto
impostando opportunamente i poli e gli zeri dei filtri nella cascata
&lt;a class=&quot;citation&quot; href=&quot;#art:saletti&quot;&gt;(Saletti, R., Novembre 1986)&lt;/a&gt;. In particolare, il polo e lo zero alle frequenze
&lt;script type=&quot;math/tex&quot;&gt;f_{pi}&lt;/script&gt; e &lt;script type=&quot;math/tex&quot;&gt;f_{0i}&lt;/script&gt; possono essere computati come funzioni di &lt;script type=&quot;math/tex&quot;&gt;\beta&lt;/script&gt;
con le seguenti formule:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f_{pi} = -\frac{s_{pi}}{2\pi} = f_{p(i-1)}10^{\frac{1}{h}} \hspace{0,5cm} , \label{eq:surface3a}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f_{0i} = - \frac{s_{0i}}{2\pi} = f_{pi}10^{\frac{\beta}{2h}} \hspace{0,5cm} , \label{eq:surface3b}&lt;/script&gt;

&lt;p&gt;dove &lt;script type=&quot;math/tex&quot;&gt;f_{p1}&lt;/script&gt; è il polo di frequenza più bassa del filtro; perciò il
limite inferiore della banda di frequenza per l’approssimazione è
&lt;script type=&quot;math/tex&quot;&gt;f_{p1}&lt;/script&gt;. La densità &lt;script type=&quot;math/tex&quot;&gt;h&lt;/script&gt; (densità dei poli per decade di frequenze) può
essere usata per controllare l’errore tra lo spettro desiderato e lo
spettro approssimato ottenuto dal filtraggio del rumore bianco. La
dipendenza dell’errore in relazione alla densità dei poli del filtro è
discussa in &lt;a class=&quot;citation&quot; href=&quot;#art:corsini&quot;&gt;(Corsini, G. &amp;amp; Saletti, R., Dicembre 1988)&lt;/a&gt;. La figura mostra uno spettro &lt;script type=&quot;math/tex&quot;&gt;1/f^{\beta}&lt;/script&gt; ottenuto usando
il filtro &lt;script type=&quot;math/tex&quot;&gt;f_{pi}&lt;/script&gt;, con due diversi valori per &lt;script type=&quot;math/tex&quot;&gt;h&lt;/script&gt;.&lt;/p&gt;

&lt;figure class=&quot;image&quot;&gt;
  &lt;img src=&quot;images/noise.jpg&quot; alt=&quot;Spettro di ampiezza del rumore frattale generato con $$\beta=1.81$$, $$h=2$$ a sinistra e $$h=6$$ a destra.&quot; /&gt;
  &lt;figcaption&gt;Spettro di ampiezza del rumore frattale generato con $$\beta=1.81$$, $$h=2$$ a sinistra e $$h=6$$ a destra.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;La funzione di trasferimento nel dominio discreto del tempo può essere
computata con il metodo della varianza della risposta all’impulso
 &lt;a class=&quot;citation&quot; href=&quot;#book:mitra&quot;&gt;(Mitra, S. K., 1998)&lt;/a&gt;; ciò corrisponde a mappare poli e zeri della funzione di
trasferimento &lt;script type=&quot;math/tex&quot;&gt;H(s)&lt;/script&gt; su poli e zeri della funzione di trasferimento
&lt;script type=&quot;math/tex&quot;&gt;H(z)&lt;/script&gt; nel dominio discreto del tempo attraverso la seguente
sostituzione:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;s-s_x \rightarrow 1-e^{s_xT_s}z^{-1} \hspace{0,5cm} , \label{eq:surface4}&lt;/script&gt;

&lt;p&gt;dove &lt;script type=&quot;math/tex&quot;&gt;T_s&lt;/script&gt; è il periodo di campionamento e &lt;script type=&quot;math/tex&quot;&gt;s_x&lt;/script&gt; indica un polo &lt;script type=&quot;math/tex&quot;&gt;s_{pi}&lt;/script&gt;
o uno zero &lt;script type=&quot;math/tex&quot;&gt;s_{0i}&lt;/script&gt;. Si ottiene la seguente funzione di trasferimento
discreta:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;H(z)=A' \frac { \prod^{N}_{i=1}1-e^{s_{0i}T}z^{-1} }{ \prod^{N}_{i=1}1-e^{s_{pi}T}z^{-1} } \hspace{0,5cm} , \label{eq:surface5}&lt;/script&gt;

&lt;p&gt;dove &lt;script type=&quot;math/tex&quot;&gt;A'&lt;/script&gt; è una costante di normalizzazione. In conclusione, lo spettro
&lt;script type=&quot;math/tex&quot;&gt;1/f^{\beta}&lt;/script&gt; è approssimato da una cascata di filtri del primo ordine,
ognuno con la seguente funzione di trasferimento discreta:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
H^{(i)}(z)=\frac{1+b_iz^{-1}}{1+a_iz^{-1}} \hspace{0,5cm} , \hspace{0,5cm} con \hspace{0,5cm}
 \left\{
 \begin{array}{ll}
 a_i=e^{-2{\pi}f_{pi}T}, &amp; b_i=e^{-2{\pi}f_{0i}T} \\
 \\
 f_{pi}=f_{p(i-1)}10^{\frac{1}{h}}, &amp; f_{0i}=f_{pi}10^{\frac{\beta}{2h}} \\
 \end{array}
 %\hspace{0,5cm}
 \right .
 \label{eq:surface6} %]]&gt;&lt;/script&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;implementazioni-dei-modelli-in-pure-data&quot;&gt;Implementazioni dei modelli in Pure Data&lt;/h2&gt;

&lt;h3 id=&quot;cosè-pure-data&quot;&gt;Cos’è Pure Data&lt;/h3&gt;

&lt;figure class=&quot;image&quot;&gt;
  &lt;img src=&quot;images/pd1_bn.jpg&quot; alt=&quot;L'interfaccia grafica di Pure Data&quot; /&gt;
  &lt;figcaption&gt;L'interfaccia grafica di Pure Data&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Pure Data è un software ideato da Miller Puckette: si tratta di un
ambiente di programmazione visuale in real–time per l’elaborazione di
audio e grafica, basato sul sistema &lt;em&gt;Max/MSP&lt;/em&gt; ma più semplice e
portabile di questo. Sono presenti due caratteristiche in PD molto
importanti: la possibilità di gestire contemporaneamente la simulazione
video e la simulazione audio utilizzando il pacchetto &lt;em&gt;GEM&lt;/em&gt; di Mark Dank
e delle facilitazioni nelle definizioni nell’accesso alle strutture
dati.&lt;/p&gt;

&lt;p&gt;Ogni documento di PD è chiamato &lt;em&gt;patch&lt;/em&gt;; una volta che tale file viene
aperto si presenta composto di una finestra principale e di eventuali
sotto–finestre (che possono essere visualizzate o nascoste ma sono
sempre in esecuzione). In ogni finestra compaiono dei blocchi collegati
tra loro; i blocchi possono essere di quattro tipi:&lt;/p&gt;

&lt;h4 id=&quot;oggetti&quot;&gt;Oggetti&lt;/h4&gt;
&lt;p&gt;Un oggetto viene creato scrivendo del testo all’interno del blocco;
il testo viene diviso in &lt;em&gt;atomi&lt;/em&gt;: il primo atomo definisce il tipo
di oggetto che viene creato, i successivi costituiscono gli
argomenti di creazione, i quali servono ad inizializzare l’oggetto.&lt;/p&gt;

&lt;p&gt;Ogni oggetto può possedere zero o più &lt;em&gt;inlet&lt;/em&gt; (collegamenti in
input) e zero o più &lt;em&gt;outlet&lt;/em&gt; (collegamenti in output); il numero di
questi dipende dal tipo di oggetto. Ci sono due tipi di
collegamento: &lt;em&gt;collegamenti di segnale&lt;/em&gt; e &lt;em&gt;collegamenti di
controllo&lt;/em&gt;; i primi sono rappresentati da una linea marcata, mentre
i secondi da una linea sottile. La scelta del tipo di collegamento
dipende dall’outlet dal quale provengono; un outlet può essere
collegato ad un inlet solo se entrambi accettano collegamenti dello
stesso tipo (o entrambi di segnale o entrambi di controllo).&lt;/p&gt;

&lt;p&gt;In figura è riportato un esempio di oggetto: l’atomo 1+1 definisce
la tipologia di oggetto (un blocco sommatore), mentre il secondo
atomo 13 indica il valore da sommare all’ingresso.&lt;/p&gt;

&lt;figure class=&quot;image&quot;&gt;
  &lt;img src=&quot;images/obj1.jpg&quot; alt=&quot;&quot; /&gt;
  &lt;figcaption&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h4 id=&quot;messaggi&quot;&gt;Messaggi&lt;/h4&gt;
&lt;p&gt;I blocchi di messaggio interpretano il testo come un messaggio da
inviare ogni volta che il blocco viene attivato; l’invio è verso il
blocco al quale l’outlet è collegato e può avvenire un numero
qualsiasi di volte durante l’esecuzione della patch. Il blocco di
messaggio possiede sempre un inlet e un outlet. Nell’esempio
seguente il primo blocco, quando viene attivato dal click del mouse,
invia il messaggio 21 all’oggetto che lo sommerà a 13 all’ultimo
blocco viene inviato il risultato dell’operazione.&lt;/p&gt;

&lt;figure class=&quot;image&quot;&gt;
  &lt;img src=&quot;images/mess1.jpg&quot; alt=&quot;Semplice esempio di patch per Pure Data.&quot; /&gt;
  &lt;figcaption&gt;Semplice esempio di patch per Pure Data.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Un messaggio può essere attivato cliccandoci sopra, da un altro
messaggio in ingresso o da un particolare blocco chiamato &lt;em&gt;bang&lt;/em&gt;.&lt;/p&gt;

&lt;h4 id=&quot;gui&quot;&gt;GUI&lt;/h4&gt;
&lt;p&gt;Il terzo blocco dell’esempio precedente fa parte dei blocchi GUI
(graphical user interface); tra questi sono inclusi i blocchi
numerici, blocchi contenenti simboli, controlli scorrevoli e
pulsanti. Mentre gli oggetti rimangono immutati durante l’esecuzione
di una patch, i blocchi GUI aggiornano il loro stato in base al
valore che contengono.&lt;/p&gt;

&lt;h4 id=&quot;commenti&quot;&gt;Commenti&lt;/h4&gt;

&lt;p&gt;I commenti sono costituiti da semplice testo e non sono contenuti
all’interno di nessun rettangolo. Nella figura precedente i
testi alla destra dei blocchi sono commenti.&lt;/p&gt;

&lt;p&gt;Una patch può essere in modalità &lt;em&gt;edit&lt;/em&gt; oppure in modalità &lt;em&gt;running&lt;/em&gt;:
nel primo caso la patch non è in esecuzione ed è permessa la creazione o
modifica dei blocchi e dei collegamenti; nel secondo caso la patch è in
esecuzione, è possibile ancora modificare i collegamenti, mentre la
modifica dei blocchi GUI ha l’effetto di variare i parametri di
controllo della patch.&lt;/p&gt;

&lt;h3 id=&quot;lincapsulamento-in-pure-data&quot;&gt;L’incapsulamento in Pure Data&lt;/h3&gt;

&lt;p&gt;Come avviene con i linguaggi di programmazione quali C, C++ e Java, con
Pure Data è possibile scrivere del codice che può poi essere
riutilizzato in qualsiasi momento; uno o più oggetti infatti possono
essere costituiti da &lt;em&gt;subpatch&lt;/em&gt;, ovvero delle patch separate che vengono
incapsulate all’interno dell’oggetto. Si possono distinguere due tipi di
incapsulamento:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;one–off subpatch – se l’oggetto viene chiamato &lt;code class=&quot;highlighter-rouge&quot;&gt;pd&lt;/code&gt; o &lt;code class=&quot;highlighter-rouge&quot;&gt;pd my-name&lt;/code&gt;, viene creata una subpatch il cui contenuto viene salvato come parte della patch genitore che può essere riutilizzata e modificata più volte all’interno di quest’ultima;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;astrazione – se l’oggetto ha il nome di una patch già presente come file (omettendo l’estensione &lt;code class=&quot;highlighter-rouge&quot;&gt;.pd&lt;/code&gt;), PD caricherà il contenuto del file all’interno della subpatch; in tal caso un cambiamento alla patch si propaga a tutte le chiamate alla sua astrazione.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Per definire il numero di inlet e outlet che deve possedere l’oggetto
contenente la subpatch è sufficiente utilizzare all’interno di
quest’ultima i blocchi &lt;code class=&quot;highlighter-rouge&quot;&gt;inlet&lt;/code&gt; e &lt;code class=&quot;highlighter-rouge&quot;&gt;outlet&lt;/code&gt; (oppure &lt;code class=&quot;highlighter-rouge&quot;&gt;inlet~&lt;/code&gt; e &lt;code class=&quot;highlighter-rouge&quot;&gt;outlet~&lt;/code&gt;
per i collegamenti di segnale).&lt;/p&gt;

&lt;h3 id=&quot;gestione-dei-segnali-audio&quot;&gt;Gestione dei segnali audio&lt;/h3&gt;

&lt;figure class=&quot;image&quot;&gt;
  &lt;img src=&quot;images/logicaltime.jpg&quot; alt=&quot;Linee del tempo per la computazione audio e la computazione di controllo in Pure Data con (a) blocchi di un campione e (b) blocchi di quattro campioni.&quot; /&gt;
  &lt;figcaption&gt;Linee del tempo per la computazione audio e la computazione di controllo in Pure Data con (a) blocchi di un campione e (b) blocchi di quattro campioni.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;In Pure Data i segnali audio vengono memorizzati come numeri in virgola
mobile a 32 bit; a seconda dell’hardware utilizzato però l’output viene
limitato a 16 o 24 bit. L’input è sempre compreso tra i valori 1 e -1,
mentre l’output viene tagliato al fine di restare compreso tra questi
due limiti. La frequenza di campionamento di default è 44100 Hz
(modificabile da riga di comando o nel menù &lt;em&gt;audio setup&lt;/em&gt;).&lt;/p&gt;

&lt;p&gt;Le computazioni audio vengono eseguite dai &lt;em&gt;blocchi tilde&lt;/em&gt;, cioè quelli
che, per convenzione, hanno il nome seguito da una tilde, come &lt;code class=&quot;highlighter-rouge&quot;&gt;sc~&lt;/code&gt;;
essi comunicano attraverso connessioni di segnale. All’avvio della
computazione, o quando vengono cambiati i collegamenti, gli oggetti
tilde vengono ordinati secondo un ordine di esecuzione lineare; tale
lista viene poi eseguita in blocchi di 64 campioni ciascuno (a 44.1 KHz
significa che l’intera rete di blocchi audio viene eseguita una volta
ogni 1.45 millisecondi). Le connessioni nella rete audio devono essere
acicliche; la presenza di eventuali cicli viene rilevata al momento del
riordino dei blocchi. Ogni subpatch può avere dei collegamenti di
segnale in entrata e in uscita tramite i blocchi “inlet&lt;script type=&quot;math/tex&quot;&gt;\sim&lt;/script&gt;” e
“outlet&lt;script type=&quot;math/tex&quot;&gt;\sim&lt;/script&gt;”.&lt;/p&gt;

&lt;p&gt;La computazione dei segnali non avviene in &lt;em&gt;real time&lt;/em&gt;, ma in &lt;em&gt;logical
time&lt;/em&gt;: quest’ultimo è definito come l’istante del successivo campione
audio che verrà elaborato, ed è sempre precedente al real time, definito
come l’istante in cui il campione arriva all’output. Tutto questo serve
a far sì che la computazione audio sia indipendente dal tempo effettivo
di esecuzione del processore, il quale può variare per molte ragioni. Si
può dedurre che una computazione audio, se eseguita nel modo corretto, è
deterministica: due esecuzioni dello stesso calcolo, una in tempo reale
e l’altra no, devono dare lo stesso risultato. In
figura si vede come la computazione dell’audio
viene svolta rispetto all’elaborazione dei segnali di controllo: i
campioni audio vengono calcolati a scadenze regolari, ma prima di ogni
scadenza devono essere effettuati tutti i calcoli di controllo che
possono influenzare il campione audio in quella scadenza. Se &lt;script type=&quot;math/tex&quot;&gt;N&lt;/script&gt; è il
numero di campioni in un blocco, la prima computazione audio riguarda i
campioni da &lt;script type=&quot;math/tex&quot;&gt;0&lt;/script&gt; a &lt;script type=&quot;math/tex&quot;&gt;N-1&lt;/script&gt;, i quali vengono inviati in output tutti insieme
all’istante &lt;script type=&quot;math/tex&quot;&gt;N&lt;/script&gt; (logical time); prima di questo istante vengono
effettuate tutte le elaborazioni di controllo per gli &lt;script type=&quot;math/tex&quot;&gt;N&lt;/script&gt; campioni.&lt;/p&gt;

&lt;h4 id=&quot;conversione-tra-segnali-audio-e-segnali-di-controllo&quot;&gt;Conversione tra segnali audio e segnali di controllo&lt;/h4&gt;

&lt;p&gt;La conversione da segnale di controllo a segnale audio è possibile
utilizzando l’oggetto &lt;code class=&quot;highlighter-rouge&quot;&gt;sig~&lt;/code&gt;. Per quanto riguarda la conversione inversa
(da segnale a controllo) deve essere specificato l’istante nel quale il
segnale viene campionato; questo può essere gestito tramite l’oggetto
&lt;code class=&quot;highlighter-rouge&quot;&gt;snapshot~&lt;/code&gt; che campiona il segnale ogni volta che riceve in input un
&lt;em&gt;bang&lt;/em&gt;. Gli oggetti &lt;code class=&quot;highlighter-rouge&quot;&gt;+~&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;-~&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;*~&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;/~&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;osc~&lt;/code&gt; e &lt;code class=&quot;highlighter-rouge&quot;&gt;phasor~&lt;/code&gt; possono
essere configurati per accettare entrambi i tipi di segnale.&lt;/p&gt;

&lt;h4 id=&quot;selettori-e-blocchi&quot;&gt;Selettori e blocchi&lt;/h4&gt;

&lt;p&gt;Gli oggetti &lt;code class=&quot;highlighter-rouge&quot;&gt;switch~&lt;/code&gt; e &lt;code class=&quot;highlighter-rouge&quot;&gt;block~&lt;/code&gt; sono utilizzati per attivare o
disattivare parti della computazione audio e per controllare la
dimensione dei blocchi di calcolo; deve essere presente uno solo dei due
oggetti per ogni finestra della patch e il suo effetto verrà esteso a
tutte le subpatch. Entrambi accettano due argomenti per la loro
costruzione: il primo è la dimensione del blocco e il secondo un fattore
di sovrapposizione.&lt;/p&gt;

&lt;p&gt;L’oggetto &lt;code class=&quot;highlighter-rouge&quot;&gt;switch~&lt;/code&gt; può essere usato per ridurre il carico
computazionale scegliendo, ad esempio, uno tra diversi algoritmi di
sintesi da utilizzare: per farlo è sufficiente che ogni algoritmo sia
implementato in una subpatch diversa.&lt;/p&gt;

&lt;h4 id=&quot;connessioni-esterne&quot;&gt;Connessioni esterne&lt;/h4&gt;

&lt;p&gt;I segnali possono essere inviati non solo tra blocchi di una stessa
finestra, ma anche tra finestre diverse oppure possono essere dati in
input all’algoritmo che li ha generati in una configurazione in
retroazione. Questo può essere implementato attraverso tre coppie di
oggetti:&lt;/p&gt;

&lt;dl&gt;
  &lt;dt&gt;throw&lt;script type=&quot;math/tex&quot;&gt;\sim&lt;/script&gt;/catch&lt;script type=&quot;math/tex&quot;&gt;\sim&lt;/script&gt; –&lt;/dt&gt;
  &lt;dd&gt;
    &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;throw~&lt;/code&gt; accumula dati in un bus, mentre &lt;code class=&quot;highlighter-rouge&quot;&gt;catch~&lt;/code&gt; legge i dati
accumulati e riazzera il bus per il ciclo successivo;&lt;/p&gt;
  &lt;/dd&gt;
  &lt;dt&gt;send&lt;script type=&quot;math/tex&quot;&gt;\sim&lt;/script&gt;/receive&lt;script type=&quot;math/tex&quot;&gt;\sim&lt;/script&gt; –&lt;/dt&gt;
  &lt;dd&gt;
    &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;send~&lt;/code&gt; salva un segnale che può essere ricevuto più volte da un
blocco &lt;code class=&quot;highlighter-rouge&quot;&gt;receive~&lt;/code&gt;, il quale però può leggere un solo &lt;code class=&quot;highlighter-rouge&quot;&gt;send~&lt;/code&gt; alla
volta;&lt;/p&gt;
  &lt;/dd&gt;
  &lt;dt&gt;delread&lt;script type=&quot;math/tex&quot;&gt;\sim&lt;/script&gt;/delwrite&lt;script type=&quot;math/tex&quot;&gt;\sim&lt;/script&gt; –&lt;/dt&gt;
  &lt;dd&gt;
    &lt;p&gt;se viene inviato un segnale ad un punto precedente nella rete audio,
esso viene ricevuto solo al ciclo successivo, con un ritardo quindi
di 1.45 millisecondi (con le impostazioni di default). Gli oggetti
&lt;code class=&quot;highlighter-rouge&quot;&gt;delread~&lt;/code&gt; e &lt;code class=&quot;highlighter-rouge&quot;&gt;delwrite~&lt;/code&gt; permettono di ridurre al minimo tale
ritardo.&lt;/p&gt;
  &lt;/dd&gt;
&lt;/dl&gt;

&lt;h4 id=&quot;scheduling&quot;&gt;Scheduling&lt;/h4&gt;

&lt;p&gt;Lo scheduler di Pure Data cerca di mantenere un certo &lt;em&gt;vantaggio&lt;/em&gt; sul
calcolo in modo da poter assorbire eventuali forti incrementi nel carico
computazionale; tale comportamento può essere impostato tramite le flag
“audiobuffer” o “frags”.&lt;/p&gt;

&lt;p&gt;Se durante l’elaborazione dell’audio si accumulano dei ritardi, possono
verificarsi delle interruzioni nei flussi di input e output; tuttavia lo
streaming su disco non viene influenzato.&lt;/p&gt;

&lt;p&gt;Le operazioni di PD sono deterministiche, nel senso che le computazioni
vengono eseguite nel momento in cui vengono schedulate senza subire
cambiamenti di ordine in real–time. Se un’operazione viene attivata da
un evento esterno, viene associata ad un tempo; questo serve a garantire
che le esecuzioni siano consistenti con le scadenze temporali imposte
dallo scheduler (il tempo non deve mai decrescere).&lt;/p&gt;

&lt;h3 id=&quot;scrivere-external-per-pure-data&quot;&gt;Scrivere &lt;em&gt;external&lt;/em&gt; per Pure Data&lt;/h3&gt;

&lt;p&gt;Con il termine &lt;em&gt;external&lt;/em&gt; si indica un oggetto che non è compreso in
Pure Data ma che può essere caricato dinamicamente durante l’esecuzione
di PD; si differenziano dagli &lt;em&gt;internal&lt;/em&gt; in quanto questi ultimi sono le
primitive già incluse in PD. Una volta che un external viene caricato in
memoria, non è più distinguibile dagli internal. Una libreria è una
collezione di external compilati all’interno di un unico file binario;
il nome di una libreria varia a seconda del sistema operativo per la
quale viene implementata: ad esempio, se viene creata la libreria
&lt;code class=&quot;highlighter-rouge&quot;&gt;my_lib&lt;/code&gt;, essa dovrà essere chiamata &lt;code class=&quot;highlighter-rouge&quot;&gt;my_lib.pd_linux&lt;/code&gt; nei sistemi
Linux, &lt;code class=&quot;highlighter-rouge&quot;&gt;my_lib.pd_irix&lt;/code&gt; e &lt;code class=&quot;highlighter-rouge&quot;&gt;my_lib.dll&lt;/code&gt; nei sistemi Win32. Una libreria
elementare include esattamente un external avente lo stesso nome della
libreria.&lt;/p&gt;

&lt;p&gt;A differenza degli external, una libreria può essere importata in due
modi:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;tramite opzione da riga di comando: &lt;code class=&quot;highlighter-rouge&quot;&gt;-lib my_lib&lt;/code&gt; (così la libreria e tutti gli external in essa contenuti vengono caricati all’avvio di PD);&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;creando un oggetto &lt;code class=&quot;highlighter-rouge&quot;&gt;my_lib&lt;/code&gt; (consigliabile quando la libreria contiene un solo oggetto con il nome della libreria stessa).&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In entrambi i casi PD prima controlla se una libreria &lt;code class=&quot;highlighter-rouge&quot;&gt;my_lib&lt;/code&gt; è già
stata caricata; se così non è, viene cercato il file corrispondente e,
se trovato, tutti gli external inclusi vengono caricati.&lt;/p&gt;

&lt;p&gt;Pure Data è scritto in C, quindi anche gli external vanno scritti in
questo linguaggio di programmazione; il codice per un semplice external
che stampa il messaggio “hello world!” è riportato di seguito
 &lt;a class=&quot;citation&quot; href=&quot;#pdexternal&quot;&gt;(Zmölnig, J. M., n.d.)&lt;/a&gt;:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    #include &quot;m_pd.h&quot;

    static t_class *helloworld_class;

    typedef struct _helloworld {
    t_object x_obj;
    } t_helloworld;

    void helloworld_bang(t_helloworld *x)
    {
        post(&quot;Hello world !!&quot;);
    }

    void *helloworld_new(void)
    {
        t_helloworld *x = (t_helloworld *)pd_new(helloworld_class);
        return (void *)x;
    }

    void helloworld_setup(void)
    {
        helloworld_class = class_new(gensym(&quot;helloworld&quot;),
                                                (t_newmethod)helloworld_new,
                                                0, sizeof(t_helloworld),
                                                CLASS_DEFAULT, 0);
        class_addbang(helloworld_class, helloworld_bang);
    }
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Inizialmente viene definita la nuova &lt;em&gt;classe&lt;/em&gt; (qui il termine “classe”
viene usato con un significato diverso da quello usuale della
programmazione ad oggetti), dove &lt;code class=&quot;highlighter-rouge&quot;&gt;hello_worldclass&lt;/code&gt; è un puntatore alla
nuova classe e la struttura &lt;code class=&quot;highlighter-rouge&quot;&gt;t_helloworld&lt;/code&gt; costituisce il &lt;em&gt;dataspace&lt;/em&gt;
della classe; la variabile &lt;code class=&quot;highlighter-rouge&quot;&gt;t_object&lt;/code&gt; è assolutamente necessaria e serve
a memorizzare le proprietà dell’oggetto, come la sua rappresentazione
grafica e le informazioni su inlet e outlet.&lt;/p&gt;

&lt;p&gt;Vengono poi definite le funzioni (&lt;em&gt;methods&lt;/em&gt;) per manipolare i dati;
quando l’istanza della classe riceve un dato, viene richiamato un
metodo; ogni metodo è associato ad un inlet. La funzione implementata
viene eseguita solo quando un nuovo dato arriva a tale inlet.&lt;/p&gt;

&lt;p&gt;Al caricamento della libreria, PD richiama la funzione
&lt;code class=&quot;highlighter-rouge&quot;&gt;helloworld_setup()&lt;/code&gt;: tale funzione dichiara la nuova classe e le sue
proprietà. L’istruzione &lt;code class=&quot;highlighter-rouge&quot;&gt;class_new&lt;/code&gt; crea una nuova classe e ritorna un
puntatore ad essa: il primo argomento è il nome simbolico della classe;
il secondo e il terzo definiscono il costruttore e il distruttore; il
quarto definisce la dimensione della struttura dati; il quinto determina
l’aspetto grafico dell’oggetto; i rimanenti sono gli argomenti
dell’oggetto. L’istruzione successiva serve per aggiungere i metodi alla
classe (il primo argomento è la classe, il secondo è il metodo).&lt;/p&gt;

&lt;p&gt;L’inizializzazione dell’oggetto avviene tramite la funzione
&lt;code class=&quot;highlighter-rouge&quot;&gt;helloworld_new()&lt;/code&gt;, i cui argomenti dipendono dalla definizione data con
&lt;code class=&quot;highlighter-rouge&quot;&gt;class_new&lt;/code&gt;.&lt;/p&gt;

&lt;h3 id=&quot;sec:librerie_pd&quot;&gt;Librerie utili per Pure Data&lt;/h3&gt;

&lt;h4 id=&quot;gem&quot;&gt;GEM&lt;/h4&gt;

&lt;p&gt;GEM (acronimo per &lt;em&gt;Graphical Environment for Multimedia&lt;/em&gt;,
&lt;a href=&quot;http://gem.iem.at&quot;&gt;http://gem.iem.at&lt;/a&gt;) è una collezione di external che permettono di
integrare elaborazioni grafiche OpenGL in una patch; sono disponibili
diversi tipi di forme geometriche, di luci e di texture; è possibile
implementare il movimento della visuale e processare l’immagine.&lt;/p&gt;

&lt;p&gt;Le elaborazioni della parte audio e della grafica vengono svolte
contemporaneamente: in tal modo si può creare un vero e proprio scenario
virtuale semplicemente utilizzando una rete di blocchi creati e gestiti
come i blocchi nativi di PD.&lt;/p&gt;

&lt;h4 id=&quot;flext&quot;&gt;Flext&lt;/h4&gt;

&lt;p&gt;Si è visto che Pure Data è un software scritto in C, e gli external
devono essere scritti in tale linguaggio; l’utente però potrebbe avere
la necessità di usare le meno complesse strutture del C++, nonché il
pieno supporto alla programmazione ad oggetti che questo offre. Per
soddisfare questa esigenza nasce &lt;em&gt;flext&lt;/em&gt;
(&lt;a href=&quot;http://grrrr.org/ext/flext/&quot;&gt;http://grrrr.org/ext/flext/&lt;/a&gt;), una libreria per lo sviluppo di
external in C++. Con flext è possibile creare librerie di external che
possono essere compilate per Pure Data, per Max/MSP e per differenti
piattaforme (Windows, Linux, OSX) e compilatori.&lt;/p&gt;

&lt;p&gt;Un semplice esempio di external basato su flext è il seguente.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    // inclusione del header file
    #include &amp;lt;flext.h&amp;gt;

    // controllo sulla versione
    #if !defined(FLEXT_VERSION) || (FLEXT_VERSION &amp;lt; 400)
    #error You need at least flext version 0.4.0
    #endif

    // definizione della classe
    // Attenzione: il nome della classe deve essere lo stesso
    // nome dell'oggetto (senza l'eventuale ~)
    class simple1:

    public flext_base
    {
        FLEXT_HEADER(simple1,flext_base)

        public:
        // costruttore
        simple1()
        {
            // definizione degli inlets:
            // il primo deve essere sempre di tipo anything
            // (oppure signal per gli oggetti dsp)
            AddInAnything(); 
    
            // definizione degli outlets:
            AddOutFloat(); // aggiunta di un outlet float (indice 0)
    
            // registrazione dei metodi:
            // registra il metodo &quot;m_float&quot; per l'inlet 0
            FLEXT_ADDMETHOD(0,m_float); 
        }
    
        protected:
        // definizione del metodo
        void m_float(float input)
        {
            float result;
            if(input == 0) {
            post(&quot;%s - zero can't be inverted!&quot;,thisName());
            result = 0;
            }
            else
                result = 1/input;
                
            // manda il valore in output all'outlet
            ToOutFloat(0,result); // (0 è l'indice dell'outlet)
        }

        private:
        // callback per il metodo &quot;m_float&quot;
        FLEXT_CALLBACK_1(m_float,float) 
    };

    // creazione della classe
    FLEXT_NEW(&quot;simple1&quot;,simple1)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;figure class=&quot;image&quot;&gt;
  &lt;img src=&quot;images/flext_bn.jpg&quot; alt=&quot;Utilizzo del modulo simple1 basato su flext: accetta un numero in input e invia in output il suo inverso.&quot; /&gt;
  &lt;figcaption&gt;Utilizzo del modulo simple1 basato su flext: accetta un numero in input e invia in output il suo inverso.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Come si può notare, è stata utilizzata la programmazione ad oggetti, a
partire dalla creazione di una classe derivata dalla classe base
&lt;em&gt;flext_base&lt;/em&gt;, la quale contiene tutte le funzioni essenziali. Il
costruttore viene richiamato nel momento in cui l’oggetto è incluso
nella patch; più precisamente è chiamato quando si crea un’istanza della
classe, contiene tutte le inizializzazioni necessarie e ha lo stesso
nome della classe. Tra le inizializzazioni devono essere presenti le
dichiarazioni di inlet e outlet e le associazioni tra metodi e
rispettivi inlet.&lt;/p&gt;

&lt;p&gt;Un &lt;em&gt;callback wrapper&lt;/em&gt; è necessario per stabilire un collegamento con PD
per ogni metodo che deve essere lanciato ogni volta che un dato viene
ricevuto: ciò avviene tramite l’istruzione
&lt;code class=&quot;highlighter-rouge&quot;&gt;FLEXT_CALLBACK_1(m_float, float)&lt;/code&gt;. Con l’ultimo comando si informa il
sistema riguardo al nome della classe e ai suoi argomenti di creazione.
In &lt;a href=&quot;#fig:flext&quot;&gt;1.9&lt;/a&gt;{reference-type=”ref” reference=”fig:flext”} è
riportato un esempio di utilizzo di questo external.&lt;/p&gt;

&lt;h3 id=&quot;implementazione-della-patch-generatrice-di-rumore-frattale&quot;&gt;Implementazione della patch generatrice di rumore frattale&lt;/h3&gt;

&lt;p&gt;Nella realizzazione della patch per Pure Data che implementa l’algoritmo
di generazione di rumore frattale, per convenienze implementative, i
filtri sono stati riscritti come cascata di biquadri: perciò la cascata
è formata da &lt;script type=&quot;math/tex&quot;&gt;N/2&lt;/script&gt; filtri del secondo ordine, ognuno con le seguenti
funzioni di trasferimento:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
H^{(i)}(z) = H^{j}H^{j-1}(z) &amp; = &amp; \frac{(1+b_jz^{-1})(1+b_{j-1}z^{-1})}{(1+a_jz^{-1})(1+a_{j-1}z^{-1})} \label{eq:surface7} \\
                             &amp; = &amp;\mbox{} \frac{1+(b_j+b_{j-1})z^{-1}+(b_jb_{j-1})z^{-2}}{1+(a_j+a_{j-1})z^{-1}+(a_ja_{j-1})z^{-2}}\\
                                             &amp;   &amp; con~ j=2\cdot i, i=1...N/2.\end{aligned} %]]&gt;&lt;/script&gt;

&lt;figure class=&quot;image&quot;&gt;
  &lt;img src=&quot;images/surface_modeler1.jpg&quot; alt=&quot;La patch surface_modeler che implementa la generazione di rumore frattale.&quot; /&gt;
  &lt;figcaption&gt;La patch surface_modeler che implementa la generazione di rumore frattale.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Il parametro di controllo più importante impostabile dall’utente è
&lt;script type=&quot;math/tex&quot;&gt;\beta&lt;/script&gt;, che definisce lo spettro &lt;script type=&quot;math/tex&quot;&gt;1/f^{\beta}&lt;/script&gt;; deve essere impostato
anche il numero di poli della cascata di filtri assieme alla frequenza
del primo polo: con questi parametri viene controllata l’accuratezza
dell’approssimazione &lt;script type=&quot;math/tex&quot;&gt;1/f^{\beta}&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;Nella figura precedente è riportata la patch &lt;em&gt;surface_modeler&lt;/em&gt;
che implementa la generazione di rumore frattale. Per avviare la
computazione è sufficiente cliccare sul blocco (in modalità &lt;em&gt;running&lt;/em&gt;):&lt;/p&gt;

&lt;figure class=&quot;image&quot;&gt;
  &lt;img src=&quot;images/surfacemodeler2.jpg&quot; alt=&quot;&quot; /&gt;
  &lt;figcaption&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;subito dopo si seleziona il numero di poli desiderato (due, quattro o
sei). Il controllo sulla patch avviene variando il parametro &lt;script type=&quot;math/tex&quot;&gt;\beta&lt;/script&gt;
tramite lo slider:&lt;/p&gt;

&lt;figure class=&quot;image&quot;&gt;
  &lt;img src=&quot;images/surfacemodeler3.jpg&quot; alt=&quot;&quot; /&gt;
  &lt;figcaption&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;figure class=&quot;image&quot;&gt;
  &lt;img src=&quot;images/surfacemodeler4.jpg&quot; alt=&quot;Il modulo (subpatch) _initialize_fractal_noise&quot; /&gt;
  &lt;figcaption&gt;Il modulo (subpatch) _initialize_fractal_noise&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Il modulo &lt;em&gt;_cascade&lt;/em&gt; invece è una subpatch nella quale viene
implementata una cascata di tre filtri, come è possibile vedere in
&lt;a href=&quot;#fig:cascade&quot;&gt;1.12&lt;/a&gt;{reference-type=”ref” reference=”fig:cascade”}; ogni
oggetto &lt;code class=&quot;highlighter-rouge&quot;&gt;biquad~&lt;/code&gt; è un filtro biquadro a due poli e due zeri; ognuno di
questi filtri calcola le seguenti equazioni differenziali:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{array}{ll}
y(n) = ff1 \cdot w(n) + ff2 \cdot w(n-1) + ff3 \cdot w(n-2) \\
w(n) = x(n) + fb1 \cdot w(n-1) + fb2 \cdot w(n-2)
\end{array}&lt;/script&gt;

&lt;p&gt;I valori &lt;script type=&quot;math/tex&quot;&gt;fb1, fb2, ff1, ff2, ff3&lt;/script&gt; vengono dati, in
quest’ordine, come argomenti di creazione dell’oggetto.&lt;/p&gt;

&lt;figure class=&quot;image&quot;&gt;
  &lt;img src=&quot;images/cascade_bn.jpg&quot; alt=&quot;La subpatch _cascade&quot; /&gt;
  &lt;figcaption&gt;La subpatch _cascade&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h3 id=&quot;sec:patch_sliding&quot;&gt;Implementazione della patch generatrice di rumore di sfregamento&lt;/h3&gt;

&lt;figure class=&quot;image&quot;&gt;
  &lt;img src=&quot;images/patchsliding1.jpg&quot; alt=&quot;&quot; /&gt;
  &lt;figcaption&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;figure class=&quot;image&quot;&gt;
  &lt;img src=&quot;images/patchsliding2.jpg&quot; alt=&quot;&quot; /&gt;
  &lt;figcaption&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Come abbiamo visto è stato sviluppato un modello che descrive il suono
per un corpo che rotola sopra una superficie. Se il corpo, invece di
rotolare, striscia sulla superficie, provoca sempre la generazione di
micro–contatti, che tuttavia avvengono con modalità diverse rispetto al
rotolamento.&lt;/p&gt;

&lt;p&gt;Una prima distinzione si ha nella velocità con cui avviene il contatto:
se per il modello di rotolamento deve essere presa in considerazione la
velocità angolare dell’oggetto che rotola (che nel caso di una sfera si
calcola come &lt;script type=&quot;math/tex&quot;&gt;\omega = v/r&lt;/script&gt;, con &lt;script type=&quot;math/tex&quot;&gt;r&lt;/script&gt; raggio della sfera), per un corpo
che striscia si deve considerare la velocità tangenziale, cioè la
velocità lungo il piano sul quale giace la superficie. In secondo luogo
il suono di un moto di rotolamento è spesso caratterizzato da
irregolarità periodiche dovute alle caratteristiche particolari del
corpo che rotola. Se questo non è perfettamente sferico e perfettamente
liscio, i micro–contatti non saranno tutti uguali ma varieranno; in
particolare per un corpo che rotola i micro–contatti si presentano con
le stesse caratteristiche a scadenze periodiche (variabili con la
velocità di movimento), e ciò si riflette nel suono prodotto, il quale
sarà caratterizzato da variazioni periodiche. Per un corpo che striscia
invece le irregolarità della sua superficie non comportano
caratteristiche periodiche nel suono. Tutto ciò è valido se la
dimensione del corpo è sufficientemente grande rispetto alla tessitura
della superficie sulla quale si muove.&lt;/p&gt;

&lt;p&gt;La patch che implementa la generazione di rumore di sfregamento,
mostrata in
&lt;a href=&quot;#fig:patchsliding1&quot;&gt;[fig:patchsliding1]&lt;/a&gt;{reference-type=”ref”
reference=”fig:patchsliding1”}, è stata quindi elaborata a partire dalla
patch che implementa il modello di rotolamento.&lt;/p&gt;

&lt;h4 id=&quot;holy-rollersim&quot;&gt;holy-roller&lt;script type=&quot;math/tex&quot;&gt;\sim&lt;/script&gt;&lt;/h4&gt;

&lt;p&gt;Il cuore della computazione viene svolto dalla subpatch &lt;code class=&quot;highlighter-rouge&quot;&gt;holy_roller~&lt;/code&gt;
(&lt;a href=&quot;#fig:patchsliding2&quot;&gt;[fig:patchsliding2]&lt;/a&gt;{reference-type=”ref”
reference=”fig:patchsliding2”}); l’oggetto &lt;code class=&quot;highlighter-rouge&quot;&gt;holy_roller~&lt;/code&gt; possiede 13
inlet:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;inlet 0 – accetta un oggetto di tipo messaggio contente il nome di un file &lt;code class=&quot;highlighter-rouge&quot;&gt;.wav&lt;/code&gt;; tale file è stato precedentemente ottenuto registrando per circa 10 secondi l’output della patch generatrice di rumore frattale (e pertanto contiene a sua volta un rumore frattale);&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;inlet 1 e 2 – non utilizzati in questa implementazione; accettano entrambi un segnale utilizzato poi come forza aggiuntiva da applicare all’oggetto rotolante;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;inlet 3 – accetta un numero in virgola mobile proporzionale all’amplificazione in ampiezza che deve subire il rumore frattale;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;inlet 4 – assumendo che l’oggetto che si muove sulla superficie sia una sfera, questo inlet riceve un numero in virgola mobile corrispondente al diametro della sfera in centimetri; sarà poi utilizzato per calcolarne la massa;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;inlet 5 – riceve un numero in virgola mobile indicante la velocità (in &lt;script type=&quot;math/tex&quot;&gt;m/s&lt;/script&gt;) dell’oggetto che si muove;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;inlet 6, 7, 8 e 9 – ricevono tutti dei segnali di controllo per l’impostazione delle frequenze e i tempi di decadimento degli oggetti modali usati nel modello di impatto (il tempo di decadimento è definito come il tempo richiesto affinché l’ampiezza decresca di un fattore &lt;script type=&quot;math/tex&quot;&gt;1/e&lt;/script&gt; rispetto al suo valore iniziale);&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;inlet 10 – riceve un numero in virgola mobile (&lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt;) proporzionale alla rigidità dell’oggetto;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;inlet 11 e 12 – ricevono due numeri in virgola mobile (&lt;script type=&quot;math/tex&quot;&gt;\alpha&lt;/script&gt; e &lt;script type=&quot;math/tex&quot;&gt;\lambda&lt;/script&gt;) utilizzati nel modulo &lt;code class=&quot;highlighter-rouge&quot;&gt;impact_modalb~&lt;/code&gt; per il calcolo della forza di impatto.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;elaborazione-di-diametro-e-velocità&quot;&gt;Elaborazione di diametro e velocità&lt;/h4&gt;

&lt;p&gt;Il valore del diametro viene elaborato da alcuni blocchi allo scopo di
calcolare dei valori indicanti il volume e la massa dell’oggetto
rotolante.&lt;/p&gt;

&lt;p&gt;Il modulo &lt;code class=&quot;highlighter-rouge&quot;&gt;_smoother&lt;/code&gt; ha lo scopo di trasformare una variazione
istantanea del valore del diametro secondo una rampa lineare della
durata di un millisecondo.&lt;/p&gt;

&lt;p&gt;Il valore di velocità invece viene inviato in input all’oggetto
&lt;code class=&quot;highlighter-rouge&quot;&gt;_clip_velo+fade&lt;/code&gt;; tale oggetto invia al primo outlet il valore di input
se questo è maggiore del valore dato come primo argomento di costruzione
(che nell’implementazione è &lt;script type=&quot;math/tex&quot;&gt;0.01&lt;/script&gt;), altrimenti viene inviato
quest’ultimo. Il ruolo di questo outlet è impedire che alla patch
successiva &lt;code class=&quot;highlighter-rouge&quot;&gt;clip_exp~&lt;/code&gt; venga inviato costantemente un segnale di
controllo nullo; se ciò si verificasse infatti si creerebbe un ciclo
infinito che porterebbe ad un funzionamento non corretto della patch. Al
secondo outlet viene inviato il valore ricevuto all’inlet opportunamente
scalato nell’intervallo delimitato dai due argomenti.&lt;/p&gt;

&lt;h4 id=&quot;clip_expsim&quot;&gt;clip_exp&lt;script type=&quot;math/tex&quot;&gt;\sim&lt;/script&gt;&lt;/h4&gt;

&lt;p&gt;I valori di raggio (in metri) e di velocità (in metri al secondo)
vengono inviati ai due oggetti &lt;code class=&quot;highlighter-rouge&quot;&gt;clip_exp~&lt;/code&gt;, la cui funzione è quella di
limitare la variazione logaritmica dei segnali in input. Più
precisamente, in ognuno dei due oggetti viene per prima cosa calcolato
il rapporto tra due campioni in input a distanza di un millisecondo
l’uno dall’altro; se tale rapporto è maggiore di &lt;code class=&quot;highlighter-rouge&quot;&gt;maxfact&lt;/code&gt; o minore di
&lt;code class=&quot;highlighter-rouge&quot;&gt;minfact&lt;/code&gt; (calcolati a partire dal parametro di costruzione), il valore
del campione corrente viene limitato e inviato all’outlet. Se il
rapporto è minore del valore predeterminato, il campione viene inviato
in output invariato. Il codice seguente mostra come questo algoritmo sia
implementato (per ragioni di efficienza) in C.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    static t_int *clip_exp_tilde_perform(t_int *w)
    {
      t_float *in = (t_float *)(w[1]);
      t_float *out = (t_float *)(w[2]);
    
    t_clip_exp_tilde_ctl *c = (t_clip_exp_tilde_ctl *)(w[3]);
    t_int buffersize = (t_int)(w[4]);

    t_float input, ratio;

        // esamina tutto il buffer
    while (buffersize--)
        {
        input = *in++;

            // se last è diverso da zero
        if (c-&amp;gt;last != 0.)
            {
                // se ratio è compreso tra maxfact e minfact
                // in ouput viene mandato input
                // altrimenti l'output è impostato a maxfact o minfact
            ratio = input / c-&amp;gt;last;
        
            if (ratio &amp;gt; c-&amp;gt;maxfact)
                c-&amp;gt;last *= c-&amp;gt;maxfact;
            
            else if (ratio &amp;lt; c-&amp;gt;minfact)
                c-&amp;gt;last *= c-&amp;gt;minfact;
            
            else
                c-&amp;gt;last = input;
            }

      *out++ = c-&amp;gt;last;
    }

    return (w+5);
    }
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I valori &lt;code class=&quot;highlighter-rouge&quot;&gt;maxfact&lt;/code&gt; e &lt;code class=&quot;highlighter-rouge&quot;&gt;minfact&lt;/code&gt; vengono calcolati nel seguente modo:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    static void set_expmax(t_clip_exp_tilde *x, t_floatarg expmax)
    {
      t_clip_exp_tilde_ctl *c = x-&amp;gt;x_ctl;

        // se l'argomento del modulo è &amp;lt; 1
        // pongo maxfact e minfact = 1
      if (expmax &amp;lt;= 1.)
    {
        c-&amp;gt;maxfact = c-&amp;gt;minfact = 1.;
      post(&quot;clip_exp: expmax &amp;lt;= 1?! Is set to 1.&quot;);
    }
    // atrimenti:
    // maxfact = epmax^(1000/samprate)
    else
    {
        c-&amp;gt;maxfact = pow(expmax, 1000. / x-&amp;gt;samprate);
      c-&amp;gt;minfact = 1. / c-&amp;gt;maxfact;
    }
    }
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;circ_max_filtersim&quot;&gt;circ_max_filter&lt;script type=&quot;math/tex&quot;&gt;\sim&lt;/script&gt;&lt;/h4&gt;

&lt;p&gt;Gli outlet dei due moduli &lt;code class=&quot;highlighter-rouge&quot;&gt;clip_exp~&lt;/code&gt; sono collegati al secondo e terzo
inlet dell’oggetto &lt;code class=&quot;highlighter-rouge&quot;&gt;circ_max_filter~&lt;/code&gt;:&lt;/p&gt;

&lt;p&gt;la sua funzione è quella di tracciare il profilo della superficie sulla
quale l’oggetto rotola e calcolare i punti di contatto tra i due. Nel
primo inlet entra il controllo di segnale ottenuto dalla moltiplicazione
del rumore frattale per il fattore di amplificazione &lt;code class=&quot;highlighter-rouge&quot;&gt;surface_depth&lt;/code&gt;. Il
ciclo principale svolto dal modulo è il seguente:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;static t_int *circ_max_filter_perform(t_int *w)
{
    t_float *in1 = (t_float *)(w[1]);
    t_float *in2 = (t_float *)(w[2]);
    t_float *in3 = (t_float *)(w[3]);
    t_float *out = (t_float *)(w[4]);

    t_circ_max_filter_ctl *c = (t_circ_max_filter_ctl *)(w[5]);
    t_float *p_samprate = (t_float *)(w[6]);
    t_int buffersize = (t_int)(w[7]);

    t_float input, radius, velocity;
    t_int range;

    while (buffersize--)
    {
        input = *in1++;
        radius = *in2++;
        velocity = *in3++;

        inc_bottom_ivalue1_circ_buff_1float2int(c-&amp;gt;p_peaks, -1);

        while ((range = bottom_ivalue1_circ_buff_1float2int(c-&amp;gt;p_peaks))
                &amp;lt; bottom_ivalue2_circ_buff_1float2int(c-&amp;gt;p_peaks))
        {
            delete_bottom_circ_buff_1float2int(c-&amp;gt;p_peaks);
            inc_bottom_ivalue1_circ_buff_1float2int(c-&amp;gt;p_peaks, range);
        }
    
        *out++ = up_circle(velocity * range / *p_samprate, radius)
                + bottom_fvalue_circ_buff_1float2int(c-&amp;gt;p_peaks);

        to_buffer(c-&amp;gt;p_peaks, *p_samprate, input, radius, velocity, 1);
    }

    return (w+8);
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In particolare si può notare come l’istruzione&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    *out++ = up_circle(velocity * range / *p_samprate, radius)
            + bottom_fvalue_circ_buff_1float2int(c-&amp;gt;p_peaks);
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;calcola i punti di contatto svolgendo la computazione della funzione &lt;script type=&quot;math/tex&quot;&gt;f_x(q)&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;La funzione &lt;code class=&quot;highlighter-rouge&quot;&gt;up_circle&lt;/code&gt; richiede due argomenti; dopo aver calcolato i
quadrati di questi, ritorna la radice quadrata della differenza dei due:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    static INLINE t_float up_circle(t_float x, t_float radius)
    {
        t_float x_2 = x*x, radius_2 = radius*radius;

        if (x_2 &amp;gt;= radius_2)
        {
          return(0.);
        }
        else
            return(sqrt(radius_2 - x_2));
    }
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;La funzione &lt;code class=&quot;highlighter-rouge&quot;&gt;to_buffer&lt;/code&gt; si occupa di aggiornare il profilo della
superficie in base al segnale ricevuto al primo inlet.&lt;/p&gt;

&lt;h4 id=&quot;_surface_tracersim&quot;&gt;_surface_tracer&lt;script type=&quot;math/tex&quot;&gt;\sim&lt;/script&gt;&lt;/h4&gt;

&lt;p&gt;Il file con estensione &lt;code class=&quot;highlighter-rouge&quot;&gt;.wav&lt;/code&gt; contenente il rumore frattale viene letto
dalla subpatch &lt;code class=&quot;highlighter-rouge&quot;&gt;_surface_tracer~&lt;/code&gt;
(&lt;a href=&quot;#fig:surfacetracer&quot;&gt;[fig:surfacetracer]&lt;/a&gt;{reference-type=”ref”
reference=”fig:surfacetracer”}). Questa subpatch, assieme alle subpatch
in essa contenute quali &lt;code class=&quot;highlighter-rouge&quot;&gt;pd tracer+calculation~&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;soundfiler_tracer~&lt;/code&gt; e
&lt;code class=&quot;highlighter-rouge&quot;&gt;table_tracer~&lt;/code&gt;, legge il file audio e lo scrive in un array;
successivamente, per inviare i campioni in output, esegue una ricerca di
tipo &lt;em&gt;table look–up&lt;/em&gt; con frequenza dipendente dalla velocità di
movimento dell’oggetto sulla superficie.&lt;/p&gt;

&lt;figure class=&quot;image&quot;&gt;
  &lt;img src=&quot;images/surfacetracer.jpg&quot; alt=&quot;&quot; /&gt;
  &lt;figcaption&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h4 id=&quot;impact_modalbsim&quot;&gt;impact_modalb&lt;script type=&quot;math/tex&quot;&gt;\sim&lt;/script&gt;&lt;/h4&gt;

&lt;p&gt;Le successive elaborazioni dei segnali finora calcolati sono svolte dal
modulo &lt;code class=&quot;highlighter-rouge&quot;&gt;impact_modalb~&lt;/code&gt;, un oggetto che implementa il modello descritto
in  &lt;a class=&quot;citation&quot; href=&quot;#art:soundobj&quot;&gt;(Avanzini, F., Rath, M., &amp;amp; Rocchesso, D., 2003)&lt;/a&gt; per i suoni di impatto.&lt;/p&gt;

&lt;figure class=&quot;image&quot;&gt;
  &lt;img src=&quot;images/impactmodalb.jpg&quot; alt=&quot;L'oggetto impact_modalb&quot; /&gt;
  &lt;figcaption&gt;L'oggetto impact_modalb&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;In questa implementazione vengono utilizzati due modi e tre punti di
interazione.&lt;/p&gt;

&lt;figure class=&quot;image&quot;&gt;
  &lt;img src=&quot;images/partolist.jpg&quot; alt=&quot;&quot; /&gt;
  &lt;figcaption&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;I parametri della forza di contatto e della massa vengono ricevuti dalla
subpatch &lt;code class=&quot;highlighter-rouge&quot;&gt;interaction+mass&lt;/code&gt; e la subpatch in essa contenuta
&lt;code class=&quot;highlighter-rouge&quot;&gt;_par_to_list4&lt;/code&gt;; in output (secondo outlet) viene mandata
una lista contenente queste informazioni. Nei quattro inlet vengono
ricevuti, in ordine: &lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;\alpha&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;\lambda&lt;/script&gt; e la massa del percussore.&lt;/p&gt;

&lt;p&gt;La subpatch &lt;code class=&quot;highlighter-rouge&quot;&gt;_modal_object_parameters3_2&lt;/code&gt;, dove 3 è il numero di modi e
2 il numero di punti di interazione, raccoglie i parametri del
risonatore. Nei primi tre inlet entrano i fattori moltiplicativi per
frequenza, tempo di decadimento e guadagno; ai successivi tre inlet sono
collegati i controlli delle frequenze di tutti i modi, poi i tempi di
decadimento di tutti i modi. Gli ultimi inlet ricevono i livelli di
ciascun modo per ogni punto di interazione con l’eventuale possibilità
di invertire la fase (&lt;em&gt;phase–reverse&lt;/em&gt;). In uscita sono presenti cinque
outlet: il primo per la lista dei fattori, il secondo per la lista delle
frequenze, il terzo per la lista dei tempi di decadimento e un outlet
per ogni punto di interazione con l’indice del punto di interazione
seguito dalla lista dei livelli (con l’eventuale fattore di phase
reverse). Infine l’oggetto &lt;code class=&quot;highlighter-rouge&quot;&gt;_modal_object_parameters3_2&lt;/code&gt; deve essere
inizializzato con la seguente lista di argomenti: lista dei valori delle
frequenze, lista dei valori dei tempi di decadimento, valori dei punti
di interazione e del phase–reverse; un valore 1 per il livello
corrisponde ad una impostazione del relativo slider a 100, dato che
quest’ultimo viene convertito in dB RMS.&lt;/p&gt;

&lt;p&gt;L’oggetto &lt;code class=&quot;highlighter-rouge&quot;&gt;impact_modalb~&lt;/code&gt; possiede i seguenti argomenti di costruzione:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;valori di default di &lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;\alpha&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;\lambda&lt;/script&gt; e massa del
percussore;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;numero di modi e numero di punti di interazione;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;maschera dei punti di interazione;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;valori di default dei tre fattori di guadagno;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;valori di default delle frequenze;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;valori di default dei tempi di decadimento;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;per ogni punto di interazione il suo indice (partendo da 0) seguito
dai valori dei livelli.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;image&quot;&gt;
  &lt;img src=&quot;images/impactmodalb2.jpg&quot; alt=&quot;L'oggetto impact_modalb e la divisione tra i gruppi di argomenti di costruzione&quot; /&gt;
  &lt;figcaption&gt;L'oggetto impact_modalb e la divisione tra i gruppi di argomenti di costruzione&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h4 id=&quot;sec:vandendoel&quot;&gt;Dipendenza dell’ampiezza del suono dalla forza normale&lt;/h4&gt;

&lt;p&gt;Secondo studi svolti da Van Den Doel, Kry e Pai [@art:vandendoel], nei
contatti che avvengono tra due corpi e che coinvolgono forze di
frizione, queste ultime sono calcolabili come:
&lt;script type=&quot;math/tex&quot;&gt;F_{frizione} = \mu F_{normale}&lt;/script&gt; e il volume del suono prodotto da
ogni contatto è proporzionale a &lt;script type=&quot;math/tex&quot;&gt;\sqrt{v \cdot F_{normale}}&lt;/script&gt;, dove &lt;script type=&quot;math/tex&quot;&gt;v&lt;/script&gt; è
la velocità alla quale avviene il contatto; in questo calcolo si assume
che l’energia acustica sia proporzionale alla perdita di capacità da
parte della superficie di opporre una resistenza (di frizione) al moto.
Nella subpatch &lt;code class=&quot;highlighter-rouge&quot;&gt;holy_roller~&lt;/code&gt; tale caratteristica è implementata tramite
gli oggetti in figura:&lt;/p&gt;

&lt;figure class=&quot;image&quot;&gt;
  &lt;img src=&quot;images/vandendoel.jpg&quot; alt=&quot;&quot; /&gt;
  &lt;figcaption&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h2 id=&quot;riferimenti&quot;&gt;Riferimenti&lt;/h2&gt;

&lt;ol class=&quot;bibliography&quot;&gt;&lt;li&gt;&lt;span id=&quot;art:soundobj&quot;&gt;Avanzini, F., Rath, M., &amp;amp; Rocchesso, D. (2003). Low-level sound models: resonators, interactions, surface textures. &lt;i&gt;The Sounding Object&lt;/i&gt;, 119–148.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;book:fractal&quot;&gt;Hastings, H. M., &amp;amp; Sugihara, G. (1993). &lt;i&gt;Fractals: A User’s Guide for the Natural Sciences.&lt;/i&gt; Oxford University Press.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;book:brownian&quot;&gt;Resnick, S. (1992). &lt;i&gt;Adventures in Stochastic Processes&lt;/i&gt;. Birkhäuser Boston.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;book:wornell&quot;&gt;Wornell, G. W. (1998). &lt;i&gt;The Digital Signal Processing Handbook&lt;/i&gt;. CRC Press and IEEE Press.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;art:pentland&quot;&gt;Pentland, A. P. (1988). Fractal-Based Description Of Surfaces. &lt;i&gt;Natural Computation&lt;/i&gt;, 279–298.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;art:saletti&quot;&gt;Saletti, R. (Novembre 1986). A comparison between two methods to generate 1/f^γnoise. &lt;i&gt;Proc. IEEE&lt;/i&gt;, &lt;i&gt;74&lt;/i&gt;, 1595–1596.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;art:corsini&quot;&gt;Corsini, G., &amp;amp; Saletti, R. (Dicembre 1988). A 1/f^γpower spectrum noise sequence generator. &lt;i&gt;IEEE Trans. on Instrumentation and Measurement&lt;/i&gt;, &lt;i&gt;37&lt;/i&gt;(4), 615–619.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;book:mitra&quot;&gt;Mitra, S. K. (1998). &lt;i&gt;Digital Signal Processing: A Computer Based Approach&lt;/i&gt;. McGraw-Hill.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;pdexternal&quot;&gt;Zmölnig, J. M. &lt;i&gt;HOWTO write an external for puredata&lt;/i&gt;. Institut for electronic music and acoustics.&lt;/span&gt;&lt;/li&gt;&lt;/ol&gt;</content><author><name>Andrea Maglie</name></author><category term="haptic feedback" /><category term="pure data" /><summary type="html">Il modello di rotolamento</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="/patchsliding1.jpg" /></entry><entry><title type="html">La percezione uditiva</title><link href="/percezione-uditiva.html" rel="alternate" type="text/html" title="La percezione uditiva" /><published>2019-01-22T00:00:00+01:00</published><updated>2019-01-22T00:00:00+01:00</updated><id>/percezione-uditiva</id><content type="html" xml:base="/percezione-uditiva.html">&lt;p&gt;I suoni che percepiamo ogni giorno nascono da interazioni tra oggetti
(un martello che colpisce un metallo, una moneta che cade) o da
cambiamenti nelle proprietà di un singolo oggetto (come un palloncino
che scoppia). Ma siamo in grado di riconoscere tali eventi fisici e le
loro proprietà solo sulla base del suono prodotto? Per rispondere a
questa domanda sono stati svolti numerosi studi, utilizzando diversi
approcci. Uno di questi consiste nel dividere l’oggetto di studio in tre
livelli:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;livello fisico;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;livello acustico;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;livello percettivo.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;L’analisi delle relazioni tra il livello percettivo e il livello fisico
permette di capire se le caratteristiche di un evento vengono
riconosciute propriamente o vengono scalate; l’analisi delle relazioni
tra livello fisico e livello acustico ci dice come variano le proprietà
del segnale sonoro in base alle proprietà fisiche dell’evento; infine
dall’analisi delle relazioni tra livello acustico e percettivo si
capisce se e come i segnali acustici influenzano il riconoscimento e la
classificazione delle proprietà dell’evento in esame.&lt;/p&gt;

&lt;p&gt;Le caratteristiche del segnale audio possono essere raggruppate in due
categorie:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;caratteristiche studiate dalla ricerca classica sulla percezione
sonora, come ampiezza, durata, tonalità, timbro e attacco;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;caratteristiche della sorgente sonora.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Durante l’ascolto la nostra attenzione viene rivolta a l’una o l’altra
classe di proprietà. In base a ciò è possibile dire che le persone
assumono due tipi di comportamenti durante l’ascolto dei suoni
&lt;a class=&quot;citation&quot; href=&quot;#art:gaver1&quot;&gt;(Gaver, W. W., 1993)&lt;/a&gt;:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;l’ascolto di tipo &lt;em&gt;musicale&lt;/em&gt; porta a riconoscere i suoni assieme
alle loro proprietà;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;l’ascolto &lt;em&gt;di tutti i giorni&lt;/em&gt; porta a riconoscere gli eventi e le
sorgenti sonore piuttosto che le proprietà del suono.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;La maggior parte della nostra esperienza nell’ascolto degli eventi può
essere classificata come ascolto di tutti i giorni: ascoltiamo ciò che
avviene attorno a noi, imparando cosa è importante evitare e cosa invece
può offrirci una possibilità di interazione. Le dimensioni percettive e
gli attributi considerati sono quelli dell’evento che produce il suono,
e non quelli del suono di per sé; tale esperienza è molto diversa
dall’ascolto di tipo musicale, e non può essere studiata pienamente
utilizzando gli approcci tradizionali all’acustica. E’ anche vero che i
suoni musicali non sono rappresentativi della classe di suoni che
regolarmente sentiamo:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;i suoni musicali sono armonici, hanno un’evoluzione temporale
semplice, non rivelano molte informazioni riguardo alla loro
sorgente e variano lungo dimensioni come tonalità e ampiezza;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;i suoni di tutti i giorni sono inarmonici o rumorosi, hanno
un’evoluzione temporale complessa, spesso rivelano molte
informazioni riguardo alla loro sorgente e variano lungo molte
dimensioni.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Per studiare quest’ultima tipologia di suoni è necessario espandere la
psicoacustica in due modi: considerando le dimensioni del suono e della
sua sorgente e trattando alcune variabili complesse come elementari.
Tali assunzioni guidano lo sviluppo dell’&lt;em&gt;approccio ecologico&lt;/em&gt; alla
percezione uditiva.&lt;/p&gt;

&lt;p&gt;In questo tipo di approccio gli stimoli elementari non necessariamente
corrispondono a dimensioni fisiche altrettanto elementari, ma in alcuni
casi sono costituite da eventi complessi; per questo, secondo
l’approccio ecologico, lo studio della percezione deve essere rivolto a
scoprire le dimensioni rilevanti per la percezione e le informazioni
relative a queste.&lt;/p&gt;

&lt;h2 id=&quot;dallevento-allesperienza&quot;&gt;Dall’evento all’esperienza&lt;/h2&gt;

&lt;figure class=&quot;image&quot;&gt;
  &lt;img src=&quot;images/gaver.jpg&quot; alt=&quot;L'esempio dell'automobile come sorgente di onde sonore; alcune onde
raggiungono l'orecchio umano immutate, altre invece vengono modificate
dall'ambiente.&quot; /&gt;
  &lt;figcaption&gt;L'esempio dell'automobile come sorgente di onde sonore; alcune onde
raggiungono l'orecchio umano immutate, altre invece vengono modificate
dall'ambiente.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Immaginiamo di sentire l’avvicinarsi di un’automobile. Possiamo
considerare tale evento come una propagazione continua di energia dalla
sorgente al soggetto; lungo il percorso che questo flusso di energia
compie si trovano vari ostacoli, ognuno con le sue caratteristiche ed
ognuno influenzante la propagazione.&lt;/p&gt;

&lt;p&gt;Nel caso in esame, la prima fonte di informazione è l’automobile (la
sorgente): il suono dipende da svariati fattori, come il movimento dei
cilindri del motore, lo sfregamento degli ingranaggi e le vibrazioni
della carrozzeria. Vengono così determinate le onde di pressione che si
propagano radialmente dalla sorgente (contrariamente alla luce radiante,
la propagazione radiale del suono ha una struttura ricca e fornisce
molte informazioni riguardo la sua sorgente).&lt;/p&gt;

&lt;p&gt;Successivamente il suono viene modificato dagli ostacoli che incontra
nell’ambiente circostante; in particolare il suono perde energia man
mano che si allontana dalla sorgente, specialmente alle alte frequenze
(anche se non sono presenti ostacoli), e ciò fornisce un’informazione
riguardo alla localizzazione della sorgente; se la sorgente si muove si
avverte un cambio di frequenze (effetto Doppler) e un cambiamento
nell’ampiezza del suono indica un’allontanamento o avvicinamento della
sorgente stessa. Dato che il sistema uditivo umano è mobile, possiamo
girare la testa al fine di cogliere i cambiamenti nei pattern e
migliorare la localizzazione. Vediamo così come un suono dia
informazioni circa un’&lt;em&gt;interazione tra materiali&lt;/em&gt; in un certo &lt;em&gt;luogo&lt;/em&gt; e
in un determinato &lt;em&gt;ambiente&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Ulteriori studi &lt;a class=&quot;citation&quot; href=&quot;#art:gaver1&quot;&gt;(Gaver, W. W., 1993)&lt;/a&gt; hanno portato alla distinzione dei suoni
di tutti i giorni in tre grandi categorie: solidi, liquidi e
aerodinamici, in quanto raramente queste classi vengono confuse tra di
loro. Ogni classe viene poi suddivisa in base al tipo di interazione tra
i materiali: ad esempio i suoni generati da solidi vibranti sono divisi
in suoni di rotolamento, di sfregamento, di impatto e di deformazione.
Queste classi costituiscono gli eventi base che producono dei suoni.&lt;/p&gt;

&lt;h3 id=&quot;vibrazioni-dei-solidi&quot;&gt;Vibrazioni dei solidi&lt;/h3&gt;

&lt;p&gt;Questa classe comprende eventi come la rottura di un vetro, una porta
che sbatte o un’automobile in moto. Gli oggetti vibrano quando su di
essi una forza viene impressa e rilasciata, portando il sistema fuori
dal suo stato di equilibrio; tale forza deforma l’oggetto dalla sua
configurazione originale, mentre la forza che l’oggetto oppone alla
deformazione viene trasformata in energia potenziale nella nuova
configurazione. Quando la forza smette di agire, l’oggetto cerca di
tornare nella posizione di riposo, l’energia potenziale si trasforma in
energia cinetica e l’oggetto vibra. Le vibrazioni continuano fino a
quando tutta l’energia accumulata viene persa e l’oggetto torna nella
posizione iniziale o trova un nuovo equilibrio. Il tipo di interazione
(un urto, uno sfregamento o un rotolamento) determina sia la variazione
nel tempo dell’ampiezza che lo spettro della vibrazione; la forza invece
determina l’ampiezza complessiva della vibrazione. Il pattern di
vibrazione è determinato anche dal tipo di materiale di cui è costituito
l’oggetto, quindi dalla sua rigidità. La dimensione determina la più
bassa frequenza di vibrazione, mentre la forma determina frequenza e
pattern spettrale prodotto.&lt;/p&gt;

&lt;p&gt;Tutti i parametri considerati possono essere raggruppati in due domini:
il dominio della frequenza e il dominio temporale. Dal momento che la
frequenza è il reciproco del tempo, è difficile separare questi due
domini dal punto di vista fisico; tuttavia sono separabili dal punto di
vista psicologico: i parametri nel dominio della frequenza influenzano
le vibrazioni dell’oggetto, mentre i parametri nel dominio del tempo
provocano effetti che diventano evidenti solo dopo alcuni cicli di
vibrazioni. Gli attributi dell’oggetto (densità, dimensioni) hanno
effetti sul suono nel dominio della frequenza, mentre gli attributi
dell’interazione (tipo e forza) influenzano i parametri nel dominio
temporale.&lt;/p&gt;

&lt;h3 id=&quot;eventi-aerodinamici&quot;&gt;Eventi aerodinamici&lt;/h3&gt;

&lt;p&gt;I suoni aerodinamici vengono prodotti quando una sorgente modifica
l’attuale pressione atmosferica circostante (come quando esplode un
palloncino). La variazione di pressione si propaga come un’onda, la
quale, se raggiunge l’orecchio e possiede particolari proprietà, può
essere avvertita come suono. La maggior parte delle informazioni viene
data dalla banda di frequenza del suono, dipendente dalla forza e dalla
quantità della variazione di pressione. Le componenti in alta frequenza
indicano la velocità nel cambiamento di pressione, mentre le componenti
in bassa frequenza dipendono dal gas coinvolto.&lt;/p&gt;

&lt;p&gt;Un altro tipo di evento aerodinamico si verifica quando un cambiamento
nella pressione imprime energia ad un oggetto provocando una sua
vibrazione.&lt;/p&gt;

&lt;h3 id=&quot;liquidi&quot;&gt;Liquidi&lt;/h3&gt;

&lt;p&gt;Gli eventi che coinvolgono liquidi dipendono da una deformazione
iniziale come nella vibrazione dei solidi, ma la vibrazione non
influisce sull’aria circostante in modo da provocare un suono; il suono
invece è il risultato della formazione e variazione di cavità risonanti
nella superficie del liquido. Per rendersene conto, basta pensare ad un
piccolo oggetto che cade in un bicchiere d’acqua: al momento del
contatto il liquido viene spostato dall’oggetto, formando una cavità che
risuona ad una frequenza caratteristica, amplificando e modificando
l’onda di pressione creata dall’impatto. Successivamente la pressione
del liquido lo porta a chiudere la cavità, immergendo completamente
l’oggetto. Un tale suono è quindi caratterizzato da un breve impulso
seguito da altri brevi impulsi di frequenza più alta. I dettagli del
suono sono determinati da massa, dimensione, velocità dell’oggetto e
dalla viscosità del liquido.&lt;/p&gt;

&lt;h3 id=&quot;eventi-che-producono-suoni-complessi&quot;&gt;Eventi che producono suoni complessi&lt;/h3&gt;

&lt;p&gt;Molti suoni dipendono da pattern complessi degli eventi descritti, o una
combinazione di questi. Anche se la fisica non descrive tali eventi
complessi, esistono attributi di alto livello che producono importanti
effetti sul loro suono. Un esempio di attributo è l’intervallo tra
eventi successivi: una sequenza di passi possono essere avvertiti come
una camminata se gli intervalli tra un passo e l’altro cadono
all’interno di un certo intervallo; altro esempio è la presenza di
vincoli tra gli oggetti coinvolti nell’evento (sarebbe strano sentire il
cigolio di una porta che si chiude lentamente accompagnato dal forte
suono di una porta che viene chiusa con forza).&lt;/p&gt;

&lt;p&gt;In tutti questi casi le sorgenti sonore possono essere considerate
annidate (si pensi al suono di un’automobile, del suo motore e dei
cilindri); in tal caso, un evento base può essere definito come un
evento composto da una singola interazione e un singolo oggetto che
produce il suono. Gli eventi complessi possono essere considerati allora
combinazioni di eventi base, nelle quali la struttura della combinazione
aggiunge informazioni importanti a quelle già fornite dagli eventi base.
Infine, è proprio questa struttura più complessa che permette di
estrarre più facilmente informazioni.&lt;/p&gt;

&lt;p&gt;In base alla struttura delle combinazioni possiamo distinguere tre tipi
di eventi complessi:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;gli eventi definiti da un &lt;em&gt;pattern temporale&lt;/em&gt; di eventi base (come
il rimbalzo di una palla è composto da un pattern di impatti);&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;gli eventi &lt;em&gt;residui&lt;/em&gt;, dati dalla sovrapposizione di diversi eventi
base;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;gli eventi &lt;em&gt;ibridi&lt;/em&gt;, dati dall’interazione tra diversi tipi di
materiale.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Ognuno di questi eventi complessi potenzialmente produce lo stesso
profilo di sorgente sonora, più eventualmente altre proprietà
specifiche: ad esempio una serie di impatti spaziati opportunamente nel
tempo può portare ad individuare il rimbalzo di una palla, dando anche
informazioni sul materiale e sulla simmetria della palla stessa.&lt;/p&gt;

&lt;p&gt;Il suono prodotto da questi eventi varia in base a molte dimensioni
fisiche di base; tuttavia spesso non percepiamo la variazione del suono
in ogni singola dimensione, bensì avvertiamo una variazione lungo una
nuova dimensione (fittizia) che incorpora tutte le altre.&lt;/p&gt;

&lt;h2 id=&quot;descrivere-gli-eventi&quot;&gt;Descrivere gli eventi&lt;/h2&gt;

&lt;figure class=&quot;image&quot;&gt;
  &lt;img src=&quot;images/gerarchia_eventi.jpg&quot; alt=&quot;Gerarchia degli eventi&quot; /&gt;
  &lt;figcaption&gt;Gerarchia degli eventi&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Mentre i suoni vengono descritti in base ad attributi come frequenza,
ampiezza e durata, più difficile è trovare degli attributi che
permettano di descrivere gli eventi di tutti i giorni.&lt;/p&gt;

&lt;p&gt;Una possibilità è quella di classificare questi eventi in base al
contesto nel quale si verificano. Ciò può essere utile se si vuole
trovare un suono all’interno di una classificazione di tale tipo, ma non
permette una descrizione efficiente di ciò che sentiamo, in quanto le
classi non sono mutuamente esclusive (uno stesso evento può verificarsi
in contesti diversi). Più interessante sembra essere una descrizione di
tipo gerarchico: in questo modo gli eventi che si trovano ad un livello
superiore nella gerarchia danno informazioni utili sul tipo di eventi di
livello subordinato, mentre dimensioni e caratteristiche servono a
descrivere le differenze tra i membri di una stessa categoria.&lt;/p&gt;

&lt;p&gt;In base a quanto detto possiamo quindi classificare i suoni di tutti i
giorni come riportato in figura: al più alto livello tutti gli eventi
sono visti come interazione di materiali; al livello successivo invece
vengono divisi in vibrazioni di solidi, aerodinamici e liquidi; al terzo
livello la distinzione è tra eventi base.&lt;/p&gt;

&lt;h2 id=&quot;percezione-e-psicofisica&quot;&gt;Percezione e psicofisica&lt;/h2&gt;

&lt;p&gt;Come per la percezione aptica, anche la percezione sonora è un processo
che si articola in tre livelli: livello fisico, neurale e mentale. In
più, nel riconoscimento dei suoni le persone cercano di trarre vantaggio
dall’esperienza acquisita nel passato (anche se non è possibile
effettuare una stima di tale esperienza).&lt;/p&gt;

&lt;p&gt;Esiste una differenza, nella nostra percezione uditiva, tra &lt;em&gt;ciò che è&lt;/em&gt;
e &lt;em&gt;ciò che sentiamo&lt;/em&gt;:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;siamo in grado di sentire suoni che non esistono (la fondamentale
mancante);&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;non riusciamo a sentire suoni che esistono (mascheramento);&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;sentiamo due diversi eventi con lo stesso insieme di stimoli (ritmi
reversibili);&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;possiamo sentire suoni che sono prodotti da sorgenti inesistenti
nell’ambiente (musica elettronica o qualsiasi manipolazione
spettrale di suoni reali).&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;L’uomo agisce in base a ciò che sente, non in base a ciò che esiste.&lt;/p&gt;

&lt;h2 id=&quot;ascoltare-e-riconoscere-la-sorgente&quot;&gt;Ascoltare e riconoscere la sorgente&lt;/h2&gt;

&lt;p&gt;Dire che l’uomo “sente” la sorgente del suono può essere corretto se si
considera un ambiente naturale: se ad esempio ci troviamo vicino ad un
violinista che suona, possiamo dire che sentiamo il violino suonare. Ma
ascoltando il suono di un violino proveniente da un impianto hi–fi non
ci verrebbe mai in mente di dire che stiamo sentendo il cono
dell’altoparlante (anche se ciò è vero); piuttosto continuiamo a
sostenere che stiamo sentendo un violino. E’ opportuno quindi dire che
l’uomo non sente la sorgente del suono, bensì l’uomo “rappresenta” la
sorgente.&lt;/p&gt;

&lt;p&gt;Il riconoscimento del suono e della sua sorgente è però un concetto
diverso rispetto alla percezione:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;si può avere percezione senza riconoscimento;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;il riconoscimento può avvenire a vari livelli: è possibile
riconoscere un rumore ma non una tonalità, il rombo di un’auto ma
non quello di una motocicletta;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;si possono verificare riconoscimenti fasulli: più precisamente un
riconoscimento è sempre “vero” all’inizio, ma può diventare “falso”
dopo aver percepito ulteriori informazioni sull’evento (come quando
si pensa di sentire la pioggia e invece si tratta del rumore delle
foglie mosse dal vento);&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;ci sono riconoscimenti ambigui (ad esempio quando non siamo in grado
di identificare una voce come maschile o femminile);&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;lo stesso stimolo acustico può dare vita a diversi riconoscimenti a
seconda che si verifichi da solo o che si ripeti (un singolo colpo
di pistola viene identificato come tale, mentre una sequenza veloce
di tali colpi fa pensare ad una mitragliatrice);&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;a volte la sorgente che viene riconosciuta è immateriale (si pensi
all’accelerazione o decelerazione di un ritmo);&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;alcuni riconoscimenti avvengono in tempo reale, mentre altri possono
avvenire “in ritardo”, riguardando suoni già sentiti e memorizzati
nella nostra mente.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;i-modelli-fisici&quot;&gt;I modelli fisici&lt;/h2&gt;

&lt;p&gt;I modelli fisici hanno lo scopo di sintetizzare, tramite un modello
matematico, le proprietà degli oggetti reali; in particolare tramite i
modelli audio si cerca di riprodurre i suoni associati a determinati
eventi.&lt;/p&gt;

&lt;p&gt;I modelli sono però sempre delle idealizzazioni, e quindi non possono
rappresentare fedelmente i fenomeni che si verificano in natura. Inoltre
gli stimoli provenienti da sorgenti reali variano lungo molte
dimensioni, ma non tutte possono essere sintetizzate: è possibile ad
esempio riprodurre le variazioni di ampiezza, timbro, durata, e
dinamica; ma diventa molto difficile sintetizzare proprietà come
presenza, brillantezza e contenuto espressivo. Inoltre, nella fisica
reale sono presenti molti oggetti che vibrano simultaneamente, mentre
l’orecchio riceve una unica onda sonora; è necessario così estrarre da
quest’ultima i segnali o le caratteristiche che possano ricreare la
molteplicità di oggetti, la loro disposizione, il percorso dell’onda
sonora ed eventuali ostacoli lungo il suo cammino.&lt;/p&gt;

&lt;h2 id=&quot;riferimenti&quot;&gt;Riferimenti&lt;/h2&gt;

&lt;ol class=&quot;bibliography&quot;&gt;&lt;li&gt;&lt;span id=&quot;art:gaver1&quot;&gt;Gaver, W. W. (1993). What in the world do we hear? An ecological approach to auditory event perception. &lt;i&gt;Ecological Psychology&lt;/i&gt;, &lt;i&gt;5&lt;/i&gt;(1), 1–29.&lt;/span&gt;&lt;/li&gt;&lt;/ol&gt;</content><author><name>Andrea Maglie</name></author><category term="haptic feedback" /><category term="pure data" /><summary type="html">I suoni che percepiamo ogni giorno nascono da interazioni tra oggetti (un martello che colpisce un metallo, una moneta che cade) o da cambiamenti nelle proprietà di un singolo oggetto (come un palloncino che scoppia). Ma siamo in grado di riconoscere tali eventi fisici e le loro proprietà solo sulla base del suono prodotto? Per rispondere a questa domanda sono stati svolti numerosi studi, utilizzando diversi approcci. Uno di questi consiste nel dividere l’oggetto di studio in tre livelli:</summary></entry><entry><title type="html">La percezione aptica</title><link href="/percezione-aptica.html" rel="alternate" type="text/html" title="La percezione aptica" /><published>2019-01-15T00:00:00+01:00</published><updated>2019-01-15T00:00:00+01:00</updated><id>/percezione-aptica</id><content type="html" xml:base="/percezione-aptica.html">&lt;h2 id=&quot;il-tatto&quot;&gt;Il tatto&lt;/h2&gt;

&lt;p&gt;Il senso del tatto può essere visto come un sistema multisensoriale
attivo. Il termine multisensoriale indica che le modalità del tatto
riguardano vari sistemi sensoriali, più precisamente:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;sistema cutaneo;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;sistema cinestetico;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;sistema aptico.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Il sistema cutaneo riceve degli input sensoriali dai meccanorecettori
(le terminazioni nervose che rispondono a stimolazioni meccaniche)
situati nella pelle. Il sistema cinestetico (cioè quella parte del
sistema nervoso che si occupa della percezione della posizione e del
movimento degli arti) riceve informazioni dai meccanorecettori posti nei
muscoli, nei tendini e nelle giunture. Infine, il sistema aptico
utilizza le informazioni che provengono da questi due sistemi. Il
termine &lt;em&gt;aptico&lt;/em&gt; è associato al tocco attivo, e nella vita di tutti i
giorni il tocco è proprio di questo tipo: muovendo gli arti e la pelle
su superfici ed oggetti, i sensori tattili vengono stimolati, rivelando
moltissime importanti proprietà del mondo che ci circonda.&lt;/p&gt;

&lt;p&gt;Nel corso degli anni sono state formulate diverse definizioni di tocco
attivo o passivo. Secondo Gibson &lt;a class=&quot;citation&quot; href=&quot;#art:gibson&quot;&gt;(Gibson, J. J., 1962)&lt;/a&gt; il tocco è passivo quando
è assente il movimento volontario dei muscoli, mentre il tocco attivo è
costituito dall’esplorazione degli oggetti tramite comandi inviati dal
cervello ai muscoli. Loomis e Lederman hanno rielaborato il pensiero di
Gibson per ottenere una classificazione del sistema sensoriale in base
agli input usati; secondo tale classificazione esistono cinque
differenti modalità di tocco &lt;a class=&quot;citation&quot; href=&quot;#art:loomis&quot;&gt;(Loomis, J. M. &amp;amp; Lederman, S. J., 1986)&lt;/a&gt;:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;percezione tattile (cutanea);&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;percezione cinestetica passiva (risposta cinestetica senza movimento
volontario);&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;percezione apitca passiva (risposte cinestetica e cutanea senza
movimento volontario);&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;percezione cinestetica attiva;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;percezione aptica attiva.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Solo negli ultimi due casi l’osservatore ha un controllo motorio sul
processo di esplorazione tattile.&lt;/p&gt;

&lt;p&gt;Un’ulteriore classificazione può essere fatta tra le sensazioni
meccaniche del tocco (come pressione e posizione) e le sensazioni legate
alla temperatura e al dolore; in tal caso non è diverso solo il tipo di
sensazione, ma è diversa anche la tipologia di percezione neurale.&lt;/p&gt;

&lt;h2 id=&quot;neurofisiologia-del-tatto&quot;&gt;Neurofisiologia del tatto&lt;/h2&gt;

&lt;figure class=&quot;image&quot;&gt;
  &lt;img src=&quot;images/meccanorecettori.jpg&quot; alt=&quot;Sezione verticale della pelle della mano, con la locazione dei quattro
tipi dimeccanorecettori.&quot; /&gt;
  &lt;figcaption&gt;Sezione verticale della pelle della mano, con la locazione dei quattro
tipi dimeccanorecettori.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;La pelle è l’organo sensoriale più grande del nostro corpo; nell’adulto
medio si estende per circa 2 metri e pesa dai 3 ai 5 chilogrammi. E’
composta di due strati: l’&lt;em&gt;epidermide&lt;/em&gt; (la parte più esterna) e il
&lt;em&gt;derma&lt;/em&gt; (la parte interna); in entrambi gli strati si trovano i
meccanorecettori, i responsabili della traduzione
degli stimoli meccanici in stimoli neurali. Si può considerare un
ulteriore strato compreso tra il derma e i muscoli: l’&lt;em&gt;ipoderma&lt;/em&gt;; esso
contiene tessuti connettivi e grasso sottocutaneo, oltre alle
terminazioni dei meccanorecettori.&lt;/p&gt;

&lt;p&gt;La pelle della mano contiene quattro diversi tipi di meccanorecettori,
distinguibili in base all’area percettiva e alla risposta agli stimoli.&lt;/p&gt;

&lt;dl&gt;
  &lt;dt&gt;Unità ad adattamento veloce –&lt;/dt&gt;
  &lt;dd&gt;
    &lt;p&gt;Le unità ad adattamento veloce (FA - fast adapting) mostrano una
rapida risposta alle deformazioni della pelle. Se la zona di
ricezione degli stimoli è piccola e ben definita si parla di unità
FAI, mentre le unità FAII sono costituite da zone ricettive più
grandi e con confini poco definiti.&lt;/p&gt;
  &lt;/dd&gt;
  &lt;dt&gt;Unità ad adattamento lento –&lt;/dt&gt;
  &lt;dd&gt;
    &lt;p&gt;Le unità ad adattamento lento (SA - slow adapting) esibiscono una
risposta continua alle deformazioni sostenute della pelle; le unità
SAI hanno una forte sensibilità dinamica e mostrano una risposta
irregolare alle stimolazioni sostenute, mentre le unità SAII
esibiscono maggiore regolarità nella risposta pur essendo meno
sensibili.&lt;/p&gt;
  &lt;/dd&gt;
&lt;/dl&gt;

&lt;p&gt;Altri tipi di recettori rispondono alle stimolazioni termiche; sono
presenti unità di recettori che rispondono al caldo e altre che
rispondono al freddo, e tutte sono localizzate nella parte più esterna
della pelle. Assieme a questi si trovano anche i recettori del dolore.&lt;/p&gt;

&lt;p&gt;I meccanorecettori presenti nei muscoli, nei tendini, nelle giunture e
nella pelle delle mani contribuiscono al senso cinestetico di movimento
degli arti; le terminazioni di diametro più largo codificano il tasso di
cambiamento della lunghezza delle fibre muscolari e le vibrazioni; le
terminazioni più piccole sono più sensibili nella fase statica
dell’attività muscolare.&lt;/p&gt;

&lt;h2 id=&quot;aspetti-sensoriali-del-tatto&quot;&gt;Aspetti sensoriali del tatto&lt;/h2&gt;

&lt;h3 id=&quot;sensibilità-e-risoluzione&quot;&gt;Sensibilità e risoluzione&lt;/h3&gt;

&lt;p&gt;Molti esperimenti sono stati fatti per capire quali sono le soglie di
reazione umane alle deformazioni meccaniche della pelle, per poi scalare
l’ampiezza delle sensazioni in corrispondenza all’ampiezza degli stimoli
e trovare le relazioni tra le reazioni recettive e le caratteristiche
degli stimoli alle diverse soglie:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;La capacità di risoluzione spaziale della pelle è stata misurata
come la minima distanza tra due punti tale che questi vengano
percepiti come distinti; utilizzando una griglia di punti, la
capacità di risoluzione valutata è di circa 1 millimetro.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Gli esperimenti sulla capacità di risoluzione temporale, valutata in
termini di sensibilità alle vibrazioni, hanno mostrato che gli
adulti sono in grado di cogliere vibrazioni fino a 700 Hz, cioè
possono distinguere intervalli temporali di circa 1.4
millisecondi. Analizzando invece la capacità di individuare come
successivi due impulsi, ciascuno di un millisecondo, si è trovato
che i due devono essere separati almeno di 5.5 millisecondi.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Tutto ciò dimostra che la mano ha una capacità risolutiva spaziale
migliore dell’orecchio ma minore dell’occhio, mentre per la risoluzione
temporale la situazione si inverte, avendosi che la mano è migliore
dell’occhio ma peggiore dell’udito.&lt;/p&gt;

&lt;h3 id=&quot;effetti-della-posizione-del-corpo-e-delletà&quot;&gt;Effetti della posizione del corpo e dell’età&lt;/h3&gt;

&lt;p&gt;La sensibilità della pelle varia in base alla parte del corpo che viene
stimolata: ad esempio il viso rileva forze di bassa entità, mentre le
dita sono più efficienti nell’elaborare informazioni spaziali. Gli
effetti dell’età sono stati studiati esaminando le soglie di rilevamento
delle vibrazioni: con l’avanzare dell’età queste soglie aumentano,
principalmente a causa della perdita di recettori. Lo stesso si verifica
per le soglie di rilevamento della densità spaziale; dai 20 agli 80 anni
si verifica un aumento di circa 1% all’anno delle soglie di
discriminazione della distanza tra due punti e nel rilevamento del loro
orientamento rispetto alle dita.&lt;/p&gt;

&lt;h3 id=&quot;manipolazioni-basate-sulla-sensibilità&quot;&gt;Manipolazioni basate sulla sensibilità&lt;/h3&gt;

&lt;p&gt;Le informazioni ricevute dalla cute giocano un ruolo fondamentale
nell’interazione con gli oggetti. Basti pensare al fatto che le persone
dotate di alte soglie di discriminazione tendono ad afferrare gli
oggetti con più forza per poterli manipolare; per soggetti con gravi
problemi la manipolazione può diventare impossibile.&lt;/p&gt;

&lt;p&gt;Afferrare e manipolare un oggetto richiede che la presa e le forze
vengano coordinate lungo una sequenza di stadi; i recettori forniscono
le informazioni necessarie a dosare e coordinare le forze per compiere
queste azioni. Ma le persone non usano solo le informazioni che vengono
ricevute istantaneamente dal cervello: vengono sfruttate anche le
conoscenze acquisite nelle esperienze passate circa il peso e le altre
proprietà degli oggetti. Ciò conduce all’uso di movimenti muscolari già
programmati e nell’adattamento di questi alle nuove proprietà degli
oggetti manipolati.&lt;/p&gt;

&lt;h2 id=&quot;percezione-aptica-delle-proprietà-degli-oggetti-e-delle-superfici&quot;&gt;Percezione aptica delle proprietà degli oggetti e delle superfici&lt;/h2&gt;

&lt;p&gt;Secondo Klatzky e Lederman &lt;a class=&quot;citation&quot; href=&quot;#art:lederman5&quot;&gt;(Lederman, S. J. &amp;amp; Klatzky, R. L., 1999)&lt;/a&gt;, il sistema aptico inizia
l’estrazione di informazioni già a partire dalle unità periferiche (i
recettori). Ciò contrasta con il metodo di funzionamento degli organi
visivi: la percezione visiva di un oggetto diventa ben definita solo in
seguito a varie elaborazioni di alto livello.&lt;/p&gt;

&lt;p&gt;Sono state fatte varie distinzioni tra le tipologie di informazioni che
possono essere estratte dalla manipolazione di un oggetto o una
superficie. Una prima discriminazione è tra proprietà &lt;em&gt;geometriche&lt;/em&gt; e
proprietà del &lt;em&gt;materiale&lt;/em&gt;:&lt;/p&gt;

&lt;dl&gt;
  &lt;dt&gt;Proprietà geometriche –&lt;/dt&gt;
  &lt;dd&gt;
    &lt;p&gt;Le proprietà geometriche sono specifiche di un oggetto e possono
essere divise in &lt;em&gt;dimensione&lt;/em&gt; e &lt;em&gt;forma&lt;/em&gt;; inoltre possiamo
considerare geometrie a livello microscopico e a livello
macroscopico. A livello microscopico un oggetto è abbastanza piccolo
da ricoprire solo una limitata regione della pelle, come la punta di
un dito; ciò produce una deformazione sulla pelle che viene
codificata dai meccanorecettori (in particolare dai SAI), ricavando
una mappa della disposizione dell’oggetto e le sue profondità. Nel
caso macroscopico l’oggetto viene avvolto dalle mani (o dagli arti
in generale), raccogliendo informazioni dai sensori cinestetici e da
zone della pelle che non sono continue (come le dita); la
determinazione della geometria avviene integrando tra loro tutte
queste informazioni.&lt;/p&gt;
  &lt;/dd&gt;
  &lt;dt&gt;Proprietà del materiale –&lt;/dt&gt;
  &lt;dd&gt;
    &lt;p&gt;Le proprietà del materiale possono essere distinte, secondo Klatzky
e Lederman, in &lt;em&gt;tessitura&lt;/em&gt;, &lt;em&gt;rigidità&lt;/em&gt;, &lt;em&gt;temperatura apparente&lt;/em&gt; e
&lt;em&gt;peso&lt;/em&gt;. Le tessiture comprendono proprietà quali ruvidità, densità
spaziale e viscosità. La rigidità non sempre corrisponde alla
resistenza che oppone un oggetto (si pensi al tasto di un
pianoforte: il tasto è rigido, ma nel momento in cui viene premuto
non oppone una forte resistenza fino a quando non viene premuto fino
in fondo); ciò significa che in questo caso entrano in gioco sia le
informazioni cutanee che quelle cinestetiche.&lt;/p&gt;
  &lt;/dd&gt;
&lt;/dl&gt;

&lt;h3 id=&quot;ruvidità&quot;&gt;Ruvidità&lt;/h3&gt;

&lt;p&gt;Una superficie ruvida è costituita da asperità poste sopra un substrato
relativamente omogeneo. La tessitura può essere microscopica o
macroscopica: si parla di micro–tessitura se le asperità sono spaziate
ad intervalli dell’ordine del millesimo di millimetro; si parla di
macro–tessitura se gli intervalli sono di uno o due ordini di grandezza
più ampi. Con intervalli oltre i 3–4 millimetri, la superficie non
appare più come dotata di tessitura ma come una superficie liscia con
delle irregolarità puntuali.&lt;/p&gt;

&lt;p&gt;Secondo vari studi, la ruvidità percepita aumenta all’aumentare dello
spazio tra le asperità; l’aumento della grandezza delle asperità invece
contribuisce a far avvertire la superficie come meno ruvida. La
percezione è influenzata anche dalle modalità con le quali questa
avviene: l’applicazione di una pressione sulla superficie con le dita
porta ad aumentare la ruvidità percepita; anche la diversa velocità di
esplorazione porta a considerare diversi livelli di ruvidità. Ciò che
invece non influisce è se il controllo è attivo o passivo; quindi le
informazioni cinestetiche giocano un ruolo marginale. Secondo il modello
formulato da Taylor e Lederman, la percezione della ruvidità è basata
sulla complessiva deformazione che lo stimolo provoca sulla pelle;
inoltre la deformazione può essere assunta come unidimensionale.&lt;/p&gt;

&lt;p&gt;Quando gli stimoli vengono presentati sulla pelle nuda, l’uomo tende a
non considerare le vibrazioni nella valutazione delle macro–tessiture
(quindi lo spazio tra le asperità e la velocità di esplorazione
influiscono minimamente sul giudizio); il contrario avviene nella
percezione delle micro–tessiture, per le quali le vibrazioni permettono
di discriminare tessiture con asperità comprese tra 0.6 e 1.6 micron
spaziate di circa 100 micron.&lt;/p&gt;

&lt;h3 id=&quot;peso&quot;&gt;Peso&lt;/h3&gt;

&lt;p&gt;Weber nel 1834 notò che un oggetto viene percepito come più pesante
quando viene impugnato rispetto a quando viene semplicemente appoggiato
sulla pelle, suggerendo che il peso percepito non dipende solamente dal
suo valore oggettivo. Successivamente Chapentier e Dresslar
&lt;a class=&quot;citation&quot; href=&quot;#book:chapentier&quot;&gt;(Chapentier, A., 1891)&lt;/a&gt; capirono che altri fattori intervengono nella
percezione del peso, dato che, tra due oggetti dello stesso peso e
diverse dimensioni, quello più piccolo sembra più pesante. Tali
osservazioni sono state confermate dai risultati di alcuni studi,
secondo i quali la percezione del peso dipende dalla resistenza che
questo oppone alla rotazione (in particolare dipende dagli autovalori
della matrice di rotazione).&lt;/p&gt;

&lt;p&gt;Un altro fattore che influenza questo tipo di percezione è il materiale
di cui è costituito l’oggetto: oggetti composti da materiali più densi
vengono avvertiti come più pesanti; oggetti più scivolosi necessitano di
una presa più forte per essere manipolati, e ciò può condurre ad
avvertirli come più pesanti. L’influenza del materiale non è presente
tuttavia quando si considerano oggetti di grande massa o quando oggetti
di piccola massa vengono impugnati con forza.&lt;/p&gt;

&lt;p&gt;In tutte queste percezioni è sempre presente una componente cognitiva,
che porta l’uomo ad utilizzare informazioni già acquisite in passato per
questo tipo di valutazioni.&lt;/p&gt;

&lt;h3 id=&quot;curvatura&quot;&gt;Curvatura&lt;/h3&gt;

&lt;p&gt;Con il termine &lt;em&gt;curvatura&lt;/em&gt; si intende il tasso di cambiamento
dell’angolo della tangente ad una curva al variare del punto per il
quale passa la tangente. La percezione della curvatura viene influenzata
da eventuali altre superfici toccate in precedenza, oppure dal fatto che
la curvatura sia orientata lungo le dita, che tocchi il palmo o il dorso
della mano.&lt;/p&gt;

&lt;p&gt;Durante l’esplorazione di una superficie curva, l’informazione più
importante (quella che viene valutata di più nella determinazione del
livello di curvatura o nella distinzione tra oggetti curvi e oggetti
piani) è data dalla differenza nelle posizioni relative delle dita.&lt;/p&gt;

&lt;h3 id=&quot;lesplorazione-manuale-nella-percezione-delle-proprietà-degli-oggetti&quot;&gt;L’esplorazione manuale nella percezione delle proprietà degli oggetti&lt;/h3&gt;

&lt;figure class=&quot;image&quot;&gt;
  &lt;img src=&quot;images/esplorazioni1.jpg&quot; alt=&quot;Procedure di esplorazione aptica delle proprietà degli
oggetti.&quot; /&gt;
  &lt;figcaption&gt;Procedure di esplorazione aptica delle proprietà degli
oggetti.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Lederman e Klatzky &lt;a class=&quot;citation&quot; href=&quot;#art:lederman7&quot;&gt;(Lederman, S. J. &amp;amp; Klatzky, R. L., 1987)&lt;/a&gt; hanno individuato l’esistenza di
movimenti particolari che vengono eseguiti dalle persone
nell’esplorazione delle proprietà di un oggetto, e ogni tipo di
proprietà è associata ad una diversa &lt;em&gt;procedura di esplorazione&lt;/em&gt;:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;il &lt;em&gt;movimento laterale&lt;/em&gt; è associato all’esplorazione delle
tessiture;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;con il &lt;em&gt;contatto statico&lt;/em&gt; si cerca di massimizzare la zona di
contatto con la superficie per individuarne la temperatura;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;impugnare&lt;/em&gt; un oggetto serve per capire a grandi linee la sua forma
e il suo volume;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;esercitare una &lt;em&gt;pressione&lt;/em&gt; fornisce informazioni sulla resistenza di
un oggetto;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;tenere in mano l’oggetto&lt;/em&gt; dà informazioni circa il suo peso;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;l’&lt;em&gt;esplorazione dei contorni&lt;/em&gt; serve a individuare i contorni precisi
(e quindi la forma precisa) dell’oggetto in esame.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;percezione-aptica-dello-spazio&quot;&gt;Percezione aptica dello spazio&lt;/h2&gt;

&lt;p&gt;Non esiste ancora una definizione universalmente accettata di &lt;em&gt;spazio
aptico&lt;/em&gt;; Lederman, Klatzky, Collins e Wardell &lt;a class=&quot;citation&quot; href=&quot;#art:lederman6&quot;&gt;(Lederman, S. J., Klatzky, R. L., Collins, A., &amp;amp; Wardell, J., 1987)&lt;/a&gt; hanno
introdotto una distinzione tra lo spazio che viene raggiunto ed
esplorato dalle mani e lo spazio che viene esplorato tramite movimenti
del corpo. Il primo tipo di spazio si può definire &lt;em&gt;spazio
manipolatorio&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;La percezione aptica dello spazio è anisotropica, in quanto non è
possibile applicare una metrica alle percezioni di questo tipo: esse
risultano distorte rispetto alla realtà e non sono uniformi lungo lo
spazio esplorato. Un primo tipo di distorsione è costituito
dall’illusione verticale–orizzontale: la lunghezza di linee verticali
viene sovrastimata rispetto alle stesse poste in orizzontale. Tale
illusione è stata riscontrata sia in soggetti ciechi che in soggetti
senza problemi di vista (quindi non è dovuta a illusioni visive) ed è
fortemente influenzata dal movimento delle braccia usato durante
l’esplorazione (dipende quindi dalla distanza relativa tra soggetto e
oggetto esaminato).&lt;/p&gt;

&lt;p&gt;Un altro tipo di illusione riguarda il movimento radiale o tangenziale:
i movimenti radiali (verso il corpo e lontano dal corpo) tendono a dare
un giudizio sovrastimato rispetto ai movimenti tangenziali di uguale
estensione. Anche in questo caso è forte l’influenza della posizione
degli arti; ad esempio una distanza percepita è più grande se la mano si
trova vicina al corpo.&lt;/p&gt;

&lt;p&gt;Il terzo tipo di illusione riguarda l’orientamento obliquo: riprodurre
l’orientamento di un’asta è più difficile quando questa è obliqua (a 45
gradi ad esempio). Si è visto inoltre che l’effetto è maggiore quando è
presente la forza gravitazionale rispetto a quando questa viene
annullata con dei pesi che la controbilanciano; infatti, se l’asta è
posta sul piano orizzontale, l’effetto non si presenta.&lt;/p&gt;

&lt;h3 id=&quot;percezione-di-pattern-bidimensionali-e-tridimensionali&quot;&gt;Percezione di pattern bidimensionali e tridimensionali&lt;/h3&gt;

&lt;p&gt;Di seguito sono riportati i risultati di vari studi eseguiti su diversi
tipi di pattern aptici.&lt;/p&gt;

&lt;dl&gt;
  &lt;dt&gt;Pattern vibrotattili –&lt;/dt&gt;
  &lt;dd&gt;
    &lt;p&gt;Un pattern vibrotattile viene generato stimolando una parte del
corpo (usualmente le dita) tramite un insieme di punti di contatto.
I punti di contatto sono costituiti da una matrice di spilli che
vibrano circa 230 volte al secondo; le righe sono distanti tra loro
circa un millimetro mentre le colonne circa $2.5$ millimetri. Due
pattern che vengono presentati ad una certa distanza temporale tra
loro possono sommarsi formando un nuovo pattern o produrre due
distinte risposte; tali effetti si verificano anche se i pattern
vengono presentati allo stesso istante su dita diverse della mano.
Le interazioni spaziali si verificano se il pattern stimola aree
comuni della pelle o quando provocano uno stesso tipo di reazione su
zone diverse della pelle; l’abilità di discriminare i pattern è
inversamente proporzionale all’area di pelle che viene stimolata da
tutti i pattern.&lt;/p&gt;
  &lt;/dd&gt;
  &lt;dt&gt;Pattern bidimensionali e forme libere –&lt;/dt&gt;
  &lt;dd&gt;
    &lt;p&gt;I pattern bidimensionali sono composti da un insieme di sporgenze
lineari o puntuali, come ad esempio il Braille. I meccanorecettori
coinvolti principalmente nella percezione di queste trame sono i
SAI, i quali agiscono su piccole aree e quindi sono più sensibili
alle discontinuità di una superficie, e insieme producono una
risposta che preserva la forma della superficie stessa. Se i pattern
bidimensionali rappresentano oggetti reali, il riconoscimento di
questi è guidato in gran parte dalle conoscenze visuali che si
posseggono dell’oggetto in esame.&lt;/p&gt;
  &lt;/dd&gt;
  &lt;dt&gt;Oggetti tridimensionali –&lt;/dt&gt;
  &lt;dd&gt;
    &lt;p&gt;Contrariamente alle riproduzioni bidimensionali, gli oggetti reali
vengono riconosciuti molto bene al tatto. Una causa di ciò è che
nella riproduzione bidimensionale viene eliminata una dimensione, la
profondità, la quale può essere ricostruita tramite la vista ma non
tramite il tocco. L’esplorazione solitamente inizia con
l’individuazione delle caratteristiche locali, per poi passare
all’estrazione delle caratteristiche globali dell’oggetto. Oggetti
simili nella forma ma con diverse caratteristiche locali vengono
tendenzialmente giudicati come diversi se l’esplorazione è aptica,
mentre con l’esplorazione visiva vengono giudicati simili; il
risultato dell’esplorazione aptica si avvicina a quello
dell’esplorazione visuale man mano che il tempo di analisi
dell’oggetto aumenta. Inoltre sembra che le persone tendano ad
esaminare la parte frontale di un oggetto se viene usa la vista,
mentre in un’analisi aptica si considera maggiormente la parte
posteriore (quella che più spesso viene esplorata con le dita).&lt;/p&gt;
  &lt;/dd&gt;
&lt;/dl&gt;

&lt;p&gt;Sono stati eseguiti degli studi anche sull’interazione tra percezione
aptica e percezione visuale, allo scopo di verificare se l’attenzione
del soggetto viene rivolta automaticamente verso una certa zona dello
spazio o piuttosto viene attirata da degli stimoli. Se vengono stimolati
entrambi i sensi (tatto e vista) concordemente, allora l’attenzione può
essere rivolta spontaneamente sia nella modalità visiva che nella
modalità aptica; al contrario, una stimolazione visiva incongruente con
quella aptica può condurre ad una direzione errata dell’attenzione
aptica. Tale effetto è dovuto alla dominazione del senso della vista sul
senso del tatto e dipende dall’età del soggetto (all’aumentare dell’età
aumenta il ruolo del tatto) e dalla sua esperienza.&lt;/p&gt;

&lt;h2 id=&quot;memoria-aptica&quot;&gt;Memoria aptica&lt;/h2&gt;

&lt;p&gt;Nello studio della memoria umana è sempre stata data molta importanza
agli effetti degli stimoli visivi e uditivi; d’altro canto lo studio
della memoria aptica viene reso più difficile dal fatto che gli stimoli
aptici possono essere facilmente modulati da quelli visivi (oppure da
una memoria visiva).&lt;/p&gt;

&lt;p&gt;Secondo Millar &lt;a class=&quot;citation&quot; href=&quot;#art:millar&quot;&gt;(Millar, S., 1999)&lt;/a&gt;, esiste nell’uomo una memoria aptica che è
a breve termine e si limita a ricordare due o tre oggetti. Quando
vengono raccolte informazioni tramite il tatto, la loro rappresentazione
in memoria può essere intrinseca a questa modalità o più generica. Ad
esempio nell’esplorazione di piccoli pattern come il Braille o altre
tessiture, la rappresentazione avviene in termini di proprietà aptiche;
se invece i pattern possono essere organizzati secondo strutture
spaziali, le informazioni su queste strutture vengono memorizzate
assieme alle informazioni puramente aptiche. La rappresentazione aptica
è intramodale anche nel senso che tali informazioni possono essere usate
non solo dal tatto ma anche dalla vista; infatti spesso i pattern
esplorati apticamente vengono poi riconosciuti alla vista.&lt;/p&gt;

&lt;p&gt;I bambini riescono a discriminare quasi sempre gli oggetti che vengono
loro presentati sia utilizzando sempre una stessa modalità di analisi
(solo aptica o solo visiva), sia utilizzando modalità diverse (a volte
aptica, a volte visiva). L’accuratezza di questi riconoscimenti tuttavia
decrementa se vengono analizzati oggetti non conosciuti; si pensa che
questo dipenda dal fatto che i soggetti effettuano una &lt;em&gt;categorizzazione
aptica&lt;/em&gt; degli oggetti in base alle loro proprietà e alla disponibilità o
meno di una percezione visiva degli stessi.&lt;/p&gt;

&lt;p&gt;La memoria umana può essere &lt;em&gt;esplicita&lt;/em&gt; o &lt;em&gt;implicita&lt;/em&gt;: la differenza tra
le due è che nell’uso della memoria esplicita il soggetto
volontariamente ricerca informazioni tra i suoi ricordi. La memoria
aptica può essere sia implicita che esplicita, ma con modalità diverse
rispetto alla memoria visiva.&lt;/p&gt;

&lt;h2 id=&quot;feedback-aptico&quot;&gt;Feedback aptico&lt;/h2&gt;

&lt;p&gt;Se si pensa al principio di azione–reazione di Newton, si capisce come
in natura non esiste il concetto di &lt;em&gt;feedback aptico&lt;/em&gt;. Le variabili
intensive (come la forza) descrivono in modo astratto le interazioni,
mentre le variabili estensive descrivono lo stato del fenomeno
osservato, e non è possibile eseguire una separazione di questi due tipi
di variabili; le forze sono individuabili attraverso i loro effetti
sullo stato del fenomeno e tali effetti non sono necessariamente gli
stessi per fenomeni sotto la stessa influenza.&lt;/p&gt;

&lt;p&gt;Non si può parlare quindi di feedback aptico nell’interazione tra
oggetti fisici, ma solo nell’interazione uomo–uomo o uomo–macchina.
Sia l’uomo che le macchine infatti possono essere visti come formati da
sensori e attuatori (gli oggetti invece costituiscono entità non
separabili da questo punto di vista). Come abbiamo visto, nell’uomo
esistono vari tipi di recettori che raccolgono le informazioni e le
trasmettono al cervello; per quanto riguarda le macchine invece è
evidente che, per poter ricevere e trasmettere una forza, devono essere
equipaggiate con sensori e attuatori artificiali. Come vedremo nel
capitolo dedicato ai dispositivi aptici, i dispositivi e le interfacce
aptiche posseggono queste caratteristiche.&lt;/p&gt;

&lt;h2 id=&quot;riferimenti&quot;&gt;Riferimenti&lt;/h2&gt;

&lt;ol class=&quot;bibliography&quot;&gt;&lt;li&gt;&lt;span id=&quot;art:gibson&quot;&gt;Gibson, J. J. (1962). Observation On Active Touch. &lt;i&gt;Psychological Review&lt;/i&gt;, &lt;i&gt;69&lt;/i&gt;, 477–490.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;art:loomis&quot;&gt;Loomis, J. M., &amp;amp; Lederman, S. J. (1986). Tactual perception. &lt;i&gt;Handbook of Perception and Human Performances&lt;/i&gt;, &lt;i&gt;2&lt;/i&gt;, 31–41.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;art:lederman5&quot;&gt;Lederman, S. J., &amp;amp; Klatzky, R. L. (1999). The haptic glance: A route to rapid object identification and manipulation. &lt;i&gt;Attention and Performance&lt;/i&gt;, &lt;i&gt;17: Cognitive regulation of performance: Interaction of theory and application&lt;/i&gt;, 165–196.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;book:chapentier&quot;&gt;Chapentier, A. (1891). &lt;i&gt;Experimental study of some aspects of weight perception&lt;/i&gt; (Vol. 3, pp. 122–135). Archives de Physiologie Normales et Pathologiques.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;art:lederman7&quot;&gt;Lederman, S. J., &amp;amp; Klatzky, R. L. (1987). Hand movements: A window into haptic object recognition. &lt;i&gt;Cognitive Psychology&lt;/i&gt;, &lt;i&gt;19&lt;/i&gt;, 342–368.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;art:lederman6&quot;&gt;Lederman, S. J., Klatzky, R. L., Collins, A., &amp;amp; Wardell, J. (1987). Exploring environments by hand or foot: Time-based heuristics for encoding distance in movement space. &lt;i&gt;Journal of Experimental Psychology: Learning, Memory and Cognition&lt;/i&gt;, &lt;i&gt;13&lt;/i&gt;, 606–614.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;art:millar&quot;&gt;Millar, S. (1999). Memory in Touch. &lt;i&gt;Psicothema&lt;/i&gt;, &lt;i&gt;11&lt;/i&gt;, 747–767.&lt;/span&gt;&lt;/li&gt;&lt;/ol&gt;</content><author><name>Andrea Maglie</name></author><category term="haptic feedback" /><category term="pure data" /><summary type="html">Il tatto</summary></entry><entry><title type="html">Una piattaforma per il rendering audio aptico di interazioni continue</title><link href="/piattaforma-rendering-audio-aptico-intro.html" rel="alternate" type="text/html" title="Una piattaforma per il rendering audio aptico di interazioni continue" /><published>2019-01-10T00:00:00+01:00</published><updated>2019-01-10T00:00:00+01:00</updated><id>/piattaforma-rendering-audio-aptico-intro</id><content type="html" xml:base="/piattaforma-rendering-audio-aptico-intro.html">&lt;p&gt;&lt;em&gt;10 anni fa pubblicavo questa tesi per la mia laurea in Ingegneria Informatica all’Università di Padova. 9 mesi passati a lavorare su quello che sicuramente è il progetto più complesso e interessante sul quale abbia mai lavorato.&lt;/em&gt;&lt;/p&gt;

&lt;h1 id=&quot;indice-dei-capitoli&quot;&gt;Indice dei capitoli&lt;/h1&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;/percezione-aptica.html&quot;&gt;La percezione aptica&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/percezione-uditiva.html&quot;&gt;La percezione uditiva&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/pure-data-modelli-audio.html&quot;&gt;Pure Data e i modelli audio&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Dispositivi aptici&lt;/li&gt;
  &lt;li&gt;Il Phantom Omni&lt;/li&gt;
  &lt;li&gt;L’applicazione Phantom Friction&lt;/li&gt;
  &lt;li&gt;Esperimenti sulla percezione audio-aptica&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&quot;introduzione&quot;&gt;Introduzione&lt;/h1&gt;
&lt;p&gt;Se nel campo della computer graphics le tessiture (texture) sono state studiate estensivamente allo scopo di rendere sempre più reali gli ambienti virtuali, altrettanto non si può dire per quanto riguarda la computer haptics, dove con tale termine si intende la scienza che studia la simulazione e lo scambio tra utente e computer di informazioni legate al senso del tatto. Tuttavia, per creare un ambiente virtuale realistico, è indispensabile fornire all’utente un ritorno di suono in corrispondenza al verificarsi di un evento (come un contatto tra oggetti).&lt;/p&gt;

&lt;p&gt;Quando tocchiamo la superficie di un oggetto reale ad occhi chiusi, raccogliamo principalmente due tipi di informazione: il primo tipo è costituito dalle informazioni percepite tramite le dita e le mani, ovvero tramite il senso del tatto; il secondo tipo è invece costituito dalle informazioni uditive, catturate dai suoni che vengono generati nel momento in cui muoviamo le nostre dita sulla superficie. La piattaforma descritta è stata realizzata proprio basandosi su questi concetti, estendendoli all’interazione con oggetti simulati.&lt;/p&gt;

&lt;p&gt;L’obiettivo a cui mira questa tesi è la realizzazione di un sistema che permetta l’interazione visiva, aptica e sonora con un ambiente virtuale (dove la parola aptica è legata al senso di tocco attivo):
visiva in quanto l’utente può visualizzare su uno schermo gli oggetti virtuali con cui interagisce;
aptica perché gli oggetti virtuali possono essere sentiti al tatto;
audio perché, come in un ambiente reale, sia possibile ascoltare i suoni provocati dalle interazioni.&lt;/p&gt;

&lt;p&gt;In particolare l’attenzione viene concentrata sulla realizzazione di tessiture sulle superfici, le quali, attraverso il controllo di alcuni parametri di significato fisico, possano far sentire (sia tramite il tatto che l’udito) all’utente super ci di diverso livello di ruvidità (non ci preoccupiamo invece delle tessiture grafiche). La sintesi dell’audio non ha più come elemento centrale il segnale che viene prodotto e percepito, ma la sorgente del suono: attraverso modelli fisici si cerca quindi di descrivere la sorgente (o le sorgenti) sonore unitamente agli eventi che producono il suono stesso. Variando un insieme ristretto di parametri è così possibile simulare diverse sorgenti; gli stessi parametri possono poi essere utilizzati per controllare la simulazione aptica.&lt;/p&gt;

&lt;p&gt;La rappresentazione grafica dell’ambiente tridimensionale virtuale è stata fatta sfruttando le API OpenGL; si è scelto di mantenere questa componente molto semplice, infatti l’ambiente è costituito da un solo oggetto che si presenta liscio e monocromatico alla vista. Per rendere possibile la rappresentazione aptica invece è necessario eseguire un interfacciamento tra l’applicazione grafica e un dispositivo di force feedback (il Phantom® Omni di SensAble Technologies in questo caso); utilizzando tale dispositivo sarà possibile toccare ogni oggetto dell’ambiente virtuale. Infine la sintesi dell’audio è stata realizzata separatamente tramite l’ambiente di programmazione visuale Pure Data, implementando dei modelli fisici la cui validità è già consolidata. Le tre componenti possono essere eseguite contemporaneamente, realizzando una completa simulazione in tempo reale che può prestarsi ad una molteplicità di usi diversi.&lt;/p&gt;

&lt;p&gt;Nel primo capitolo si descrive brevemente il senso del tatto, in particolare come il corpo umano raccoglie ed elabora le informazioni tattili; nel secondo invece viene analizzato il modo di percepire da parte dell’uomo i suoni nell’ambiente circostante.&lt;/p&gt;

&lt;p&gt;Nel terzo capitolo si illustrano per prima cosa i modelli fisici su cui viene basata la sintesi dell’audio. I modelli comprendono la realizzazione di tessiture stocastiche a partire da un rumore filtrato e il rendering di suoni di contatto tramite una sintesi modale. Successivamente viene descritto il funzionamento dell’ambiente di programmazione Pure Data e dei moduli esterni utilizzati; segue una spiegazione dettagliata su come sono stati implementati i modelli fisici in patch eseguibili in tale ambiente.&lt;/p&gt;

&lt;p&gt;Il capitolo 4 riporta una panoramica sui dispositivi aptici, mentre il capitolo successivo illustra nel dettaglio il dispositivo Phantom® Omni , unitamente al toolkit OpenHaptics utilizzato per la programmazione di tale dispositivo.&lt;/p&gt;

&lt;p&gt;Nel capitolo 6 si tratta la programmazione dell’interfaccia grafica e dell’interfaccia aptica; viene spiegato come avviene il rendering delle forze e come sono realizzate le tessiture aptiche. Infine si spiega come queste componenti siano state integrate tra loro e come avviene la comunicazione tra l’interfaccia grafica/aptica e l’algoritmo di sintesi audio creato ed eseguito in Pure Data.&lt;/p&gt;

&lt;p&gt;La parte finale (capitolo 7) è dedicata agli esperimenti di percezione bimodale audio aptica: dopo aver presentato una panoramica sugli studi che sono già stati condotti in questo campo, vengono illustrate le differenze rispetto a questi e gli aspetti innovativi degli studi svolti in questa tesi. Sono stati svolti tre esperimenti: il primo in condizione di variazione concorde degli stimoli audio e aptico, gli altri in condizioni di variazione discorde dei due stimoli. In conclusione vengono riportati i risultati e le discussioni sulle percezioni avute dai partecipanti.&lt;/p&gt;</content><author><name>Andrea Maglie</name></author><category term="haptic feedback" /><category term="pure data" /><summary type="html">10 anni fa pubblicavo questa tesi per la mia laurea in Ingegneria Informatica all’Università di Padova. 9 mesi passati a lavorare su quello che sicuramente è il progetto più complesso e interessante sul quale abbia mai lavorato.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="/hero-phantomomni.jpg" /></entry><entry><title type="html">DevFest Veneto 2018</title><link href="/devfest-veneto-2018-recap.html" rel="alternate" type="text/html" title="DevFest Veneto 2018" /><published>2018-12-01T00:00:00+01:00</published><updated>2018-12-01T00:00:00+01:00</updated><id>/devfest-veneto-2018-recap</id><content type="html" xml:base="/devfest-veneto-2018-recap.html">&lt;p&gt;L’edizione 2018 della DevFest Veneto, organizzata da noi del GDG Venezia, si è tenuta il 24 Novembre. 
Si tratta della nostra 2° DevFest e, come la prima edizione tenutasi un anno fa, abbiamo registrato il tutto esaurito, grazie sia ad uno schedule fatto da speaker di alto livello, sia alla location e organizzazione fornita da &lt;a href=&quot;http://www.fabrica.it/?lang=it_it&quot;&gt;Fabrica&lt;/a&gt;.
Vi siete persi qualcosa? Buone notizie: tutti gli speech sono stati registrati e sono disponibili su YouTube!
Ecco qui di seguito l’elenco completo.&lt;/p&gt;

&lt;h2 id=&quot;serverless-event-driven-architecture-with-kotlin-the-pixartprinting-use-case&quot;&gt;Serverless Event-driven Architecture with Kotlin: the Pixartprinting use case&lt;/h2&gt;
&lt;p&gt;Speaker: &lt;em&gt;Marco Falcier&lt;/em&gt;, Software Engineer, Pixartprinting S.P.A.&lt;/p&gt;
&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/4vsduSesG6U&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;h2 id=&quot;do-it-yourselfie-an-android-things-photo-booth&quot;&gt;Do-it-yourselfie, an Android Things photo booth&lt;/h2&gt;
&lt;p&gt;Speaker: &lt;em&gt;Daniele Bonaldo&lt;/em&gt;, Android Developer, Novoda&lt;/p&gt;
&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/FoegZ2PnshM&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;h2 id=&quot;la-morte-nera-e-se-fosse-prototipizzata-con-firebase-e-react&quot;&gt;La Morte Nera, e se fosse prototipizzata con Firebase e React?&lt;/h2&gt;
&lt;p&gt;Speaker: &lt;em&gt;Michel Murabito&lt;/em&gt;, Web Developer, Spindox&lt;/p&gt;
&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/D-8S9c2t1FY&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;h2 id=&quot;everydays---why-you-need-to-create-daily&quot;&gt;EVERYDAYS - why you need to create daily&lt;/h2&gt;
&lt;p&gt;Speaker: &lt;em&gt;Riccardo Giorato&lt;/em&gt;, Student&lt;/p&gt;
&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/oTxLNxppMUM&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;h2 id=&quot;android-jet-navigation&quot;&gt;Android JET Navigation&lt;/h2&gt;
&lt;p&gt;Speaker: &lt;em&gt;Matteo Bonifazi&lt;/em&gt;, Android Developer, Sisal&lt;/p&gt;
&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/UUDNKjHXQMg&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;h2 id=&quot;frameworkless-frontend-development&quot;&gt;Frameworkless Frontend Development&lt;/h2&gt;
&lt;p&gt;Speaker: &lt;em&gt;Francesco Strazzullo&lt;/em&gt;, Web Developer, ideato&lt;/p&gt;
&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/q_WGYGsRHQg&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;h2 id=&quot;the-build-time-of-my-life&quot;&gt;The (build) time of my life&lt;/h2&gt;
&lt;p&gt;Speaker: &lt;em&gt;Lorenzo Quiroli&lt;/em&gt;, Senior Android Developer, busuu&lt;/p&gt;
&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/Oila7PK27LE&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;h2 id=&quot;blockchain-la-non-rivoluzione-ci-aspetta&quot;&gt;BlockChain, la (non) rivoluzione ci aspetta!&lt;/h2&gt;
&lt;p&gt;Speaker: &lt;em&gt;Thomas Rossetto&lt;/em&gt;, Fullstack Dev, Seesaw&lt;/p&gt;
&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/QPJnaIBWHSA&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;h2 id=&quot;introduzione-a-machine-learning&quot;&gt;Introduzione a Machine Learning&lt;/h2&gt;
&lt;p&gt;Speaker: &lt;em&gt;Cesare Montresor&lt;/em&gt;, Software Developer, Tarallucci, Vino e Machine Learning&lt;/p&gt;
&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/SFdC80gC06A&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;</content><author><name>Andrea Maglie</name></author><category term="devfest" /><category term="gdg" /><summary type="html">L’edizione 2018 della DevFest Veneto, organizzata da noi del GDG Venezia, si è tenuta il 24 Novembre. Si tratta della nostra 2° DevFest e, come la prima edizione tenutasi un anno fa, abbiamo registrato il tutto esaurito, grazie sia ad uno schedule fatto da speaker di alto livello, sia alla location e organizzazione fornita da Fabrica. Vi siete persi qualcosa? Buone notizie: tutti gli speech sono stati registrati e sono disponibili su YouTube! Ecco qui di seguito l’elenco completo.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="/hero-devfest-veneto-2018.png" /></entry><entry><title type="html">Speed up your builds… without upgrading your hardware</title><link href="/speed-up-your-builds-without-upgrading-your-hardware.html" rel="alternate" type="text/html" title="Speed up your builds… without upgrading your hardware" /><published>2018-04-27T00:00:00+02:00</published><updated>2018-04-27T00:00:00+02:00</updated><id>/speed-up-your-builds-without-upgrading-your-hardware</id><content type="html" xml:base="/speed-up-your-builds-without-upgrading-your-hardware.html">&lt;p&gt;&lt;em&gt;I wish every build could look like this :)&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Often the only way to (really) speed up slow builds is to add more RAM, replace the old HDD with an SSD or buy a complete new PC (or Laptop). All those solutions requires investing (a lot of) money. What if we can just keep current hardware and delegate the build to another, more powerful, machine?&lt;/p&gt;

&lt;p&gt;I usually develop for Android, and everyone knows that Android builds are slow. Waiting for a build to complete is one of the things that make me feel like I’m wasting my day. Edit, launch the build, wait 4 to 5 minutes to have the app compiled and installed, run the app, test, edit, launch another build, wait again… no, I really hate it! So what can I do?&lt;/p&gt;

&lt;p&gt;&lt;em&gt;(I won’t write here about all the fine tunings that can be done inside your gradle configurations files, they’re not in the scope of this article)&lt;/em&gt;&lt;/p&gt;

&lt;h2 id=&quot;test-driven-development&quot;&gt;Test Driven Development&lt;/h2&gt;

&lt;p&gt;I’m a fan of &lt;em&gt;TDD&lt;/em&gt;, so I try to skip many of those cycle of builds + manual testing by writing unit tests. The cycle becomes: write test, implement your code, build and run tests. You’ll skip the time-consuming manual testing (and you’ll gain all the well-known benefits of TDD).&lt;/p&gt;

&lt;p&gt;But sometimes it’s not enough, for example when you’re fine-tuning the layout of the app, or when you need to debug a specific use-case. If the project is quite big, also running unit tests can become time consuming.&lt;/p&gt;

&lt;h2 id=&quot;hardware-upgrade-or&quot;&gt;Hardware upgrade or…?&lt;/h2&gt;

&lt;p&gt;So, should I upgrade my hardware? But my hardware is quite enough for everything that I do every day, except for code compiling! I’d like to have a machine just for compiling. Maybe a &lt;em&gt;Virtual Machine&lt;/em&gt;, that can be launched only when I need it, and than I’ll pay just for the time I use it…&lt;/p&gt;

&lt;h2 id=&quot;welcome-to-mainframer&quot;&gt;…welcome to Mainframer&lt;/h2&gt;

&lt;p&gt;Then I’ve discovered a great tool: &lt;a href=&quot;https://github.com/gojuno/mainframer&quot;&gt;Mainframer&lt;/a&gt;. It’s a command line tool that lets you easily launch your build on a remote machine! No need to buy a new laptop anymore!(*)&lt;/p&gt;

&lt;p&gt;When you launch a build through Mainframer, it will:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Establish a &lt;em&gt;ssh&lt;/em&gt; connection with the remote machine.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Use &lt;em&gt;rsync&lt;/em&gt; to sync current directory with the remote machine.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Lunch the build command on the remote machine.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Sync back the directory when build finishes.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Here are the steps I’ve taken for my configuration:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Create a Linux Virtual Machine on AWS&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Launch the Virtual Machine, ssh into it and install all the build tools that you need. For Android I’ve installed just the JDK and Android SDK (more details on configurations of the remote machine &lt;a href=&quot;https://github.com/gojuno/mainframer/blob/development/docs/SETUP_REMOTE.md&quot;&gt;here&lt;/a&gt;). Configure the Virtual Machine with the hardware features that fits your needs.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Configure ssh on your local machine (more info &lt;a href=&quot;https://github.com/gojuno/mainframer/blob/development/docs/SETUP_LOCAL.md&quot;&gt;here&lt;/a&gt;).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Download a copy of mainframer.sh in the root of your project.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Test that everything is configured correctly with the following command:&lt;/p&gt;

    &lt;p&gt;./mainframer echo “It works!” &amp;gt; success.txt&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Yeah, just this simple one-time setup! Now you can launch your build like this:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;./mainframer ./gradle assembleDebug
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;end enjoy a faster compilation!&lt;/p&gt;

&lt;p&gt;Another great advantage of this approach is that, if the build still requires “long time”, your local machine won’t be overloaded by the build process and you can use it for other stuff.&lt;/p&gt;

&lt;p&gt;Mainframer supports the following build systems:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/gojuno/mainframer/blob/development/samples/gradle&quot;&gt;Gradle&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/gojuno/mainframer/blob/development/samples/gradle-android&quot;&gt;Gradle Android&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/gojuno/mainframer/blob/development/samples/rust&quot;&gt;Rust&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/gojuno/mainframer/blob/development/samples/clang&quot;&gt;Clang&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/gojuno/mainframer/blob/development/samples/gcc&quot;&gt;GCC&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/gojuno/mainframer/blob/development/samples/mvn&quot;&gt;Maven&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/gojuno/mainframer/blob/development/samples/buck&quot;&gt;Buck&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://github.com/gojuno/mainframer/blob/development/samples/go&quot;&gt;Go&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;And there’s also an &lt;a href=&quot;https://github.com/elpassion/mainframer-intellij-plugin&quot;&gt;IntelliJ Plugin&lt;/a&gt;!&lt;/p&gt;

&lt;h2 id=&quot;bonus&quot;&gt;Bonus&lt;/h2&gt;

&lt;p&gt;You can get the maximum boost to your development process by applying both TDD and Mainframer!&lt;/p&gt;

&lt;p&gt;(*)Yes, I know, you don’t have to pay for a new laptop but you have to pay for VM usage. But it’s much cheaper, and you can upgrade or throw away the VM according to your budget and your needs.&lt;/p&gt;</content><author><name>Andrea Maglie</name></author><category term="android" /><category term="gradle" /><summary type="html">I wish every build could look like this :)</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="/hero-speed-up-builds.png" /></entry><entry><title type="html">Il prezzo del context-switching</title><link href="/prezzo-context-switching.html" rel="alternate" type="text/html" title="Il prezzo del context-switching" /><published>2017-06-22T00:00:00+02:00</published><updated>2017-06-22T00:00:00+02:00</updated><id>/prezzo-context-switching</id><content type="html" xml:base="/prezzo-context-switching.html">&lt;blockquote&gt;
  &lt;p&gt;&lt;em&gt;Il prezzo del cambio di contesto (context-switching) si paga sia in termini di tempo che di fatica mentale.&lt;/em&gt;&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Vi è mai capitato di rivolgervi ad una persona mentre questa sta lavorando, magari semplicemente per chiedere un favore, e vedere che questa persona vi risponde in modo seccato apparentemente senza nessun motivo? A me sì. E ora vi spiego perché accade.&lt;/p&gt;

&lt;p&gt;Per un lavoro che richiede concentrazione, le distrazioni e il cambio di contesto rappresentano un costo elevato in termini di tempo e fatica mentale.
Prendiamo ad esempio il lavoro dello sviluppatore software: progettare e scrivere software è uno di quei lavori che richiede di immagazzinare molte informazioni nella testa tutte allo stesso momento (le specifiche del task a cui stai lavorando, i nomi delle variabili, le strutture dati da utilizzare, le API da invocare, i nomi delle funzioni che hai già scritto e che ti possono tornare utili…). Sono tutte informazioni che vengono tenute in memoria e vengono usate solo fin tanto che lavoriamo su quel task; una volta terminato il task, vengono “dimenticate” per lasciare lo spazio a quello che dovremo immagazzinare per il task successivo.
Ora, supponiamo di distrarre un programmatore mentre sta lavorando ad particolare un task, portando la sua attenzione altrove, ad esempio perché gli chiediamo informazioni su un altro task su cui ha lavorato il giorno prima: lui sarà costretto a svuotare la mente per passare al nuovo contesto (la nostra richiesta di informazioni) e ragionare su questo. Appena finito, potrà tornare al suo lavoro, ma non immediatamente: prima dovrà raccogliere di nuovo le idee e le informazioni necessarie.&lt;/p&gt;

&lt;p&gt;A seconda della tipologia e difficoltà del task, questa operazione può richiedere più o meno tempo e fatica. Se ripetuto più volte al giorno può causare una forte fatica mentale e causare nella “vittima” la sensazione frustante di non essere riuscito a portare a termine quello che doveva.&lt;/p&gt;

&lt;p&gt;Quindi il prezzo del cambio di contesto si paga sia in termini di tempo che di fatica mentale (dalla quale deriva lo stress e lo stato di alterazione).&lt;/p&gt;

&lt;p&gt;La soluzione? Cercare di limitare il più possibile queste situazioni. Se siete il lavoratore in questione, spiegate alle persone vicino a voi (i vostri colleghi ad esempio) che state lavorando e avete bisogno di concentrarvi, e di rimandare di qualche ora tutto quello che non è strettamente urgente (meglio: condividete con loro questo post!). L’uso della messaggistica asincrona (e-mail, WhatsApp, Telegram, Slack…) è utile in questi casi: se quello che va comunicato è urgente (ma veramente urgente!), lo si dice a voce, per tutto il resto si può mandare un messaggio o una mail e sarà il destinatario a decidere quando è il momento più opportuno per “distrarsi” e leggere i messaggi.&lt;/p&gt;

&lt;p&gt;Lascio a voi decidere chi ricade nella categoria del “lavoratore” e chi nella categoria dei “rompiscatole”… E ricordatevi che, a volte, la stessa persona può ricoprire entrambi i ruoli! 😜&lt;/p&gt;

&lt;p&gt;Fonte: l’esperienza personale, e questo articolo: https://www.petrikainulainen.net/software-development/processes/the-cost-of-context-switching/&lt;/p&gt;</content><author><name>Andrea Maglie</name></author><category term="italiano" /><category term="other" /><summary type="html">Il prezzo del cambio di contesto (context-switching) si paga sia in termini di tempo che di fatica mentale.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="/hero-context-switch.png" /></entry><entry><title type="html">Migrate to Android Studio 2.3</title><link href="/android-studio-2.3-migration.html" rel="alternate" type="text/html" title="Migrate to Android Studio 2.3" /><published>2017-03-08T00:00:00+01:00</published><updated>2017-03-08T00:00:00+01:00</updated><id>/android-studio-2.3-migration</id><content type="html" xml:base="/android-studio-2.3-migration.html">&lt;p&gt;&lt;img src=&quot;/images/android_studio_logo.png&quot; alt=&quot;Android Studio Logo&quot; /&gt;&lt;/p&gt;

&lt;h3 id=&quot;new-features&quot;&gt;New features&lt;/h3&gt;

&lt;p&gt;Android Studio 2.3 has finally been released in the &lt;a href=&quot;http://tools.android.com/download/studio/builds/2-3-0&quot;&gt;Stable Channel&lt;/a&gt;. There are many improvements compared to version 2.2, including:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;Instant run&lt;/em&gt;: now there are two different icons, one for restart the app and another one for apply changes without restart.
&lt;img src=&quot;/images/instant_run_2.3.png&quot; alt=&quot;Screen Shot 2017-03-02 at 9.12.10 AM.png&quot; /&gt;&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Build cache&lt;/em&gt;: it’s used to have faster clean builds by caching exploded AARs and pre-dexed external libraries.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;App Links Assistant&lt;/em&gt; (Tools → App Link Assistant): it allows you to create new intent filters for your URLs, declare your app’s website association through a Digital Asset Links file, and test your Android App Links support.&lt;/li&gt;
  &lt;li&gt;Support to &lt;em&gt;WebP lossless image format&lt;/em&gt;: the WebP format is . Android Studio 2.3 has a new wizard that converts any non-launcher PNG file to WebP (up to 25% smaller than a PNG) and WebP back to PNG.&lt;/li&gt;
  &lt;li&gt;&lt;em&gt;Constraint Layout&lt;/em&gt; with Chains and Ratios: this new version of Android Studio includes the stable release of ConstraintLayout. You can now chain two or more Android views bi-directionally together to form a group on one dimension (helpful when you want to place two views close together but want to spread them across empty space).&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;problems-and-issues&quot;&gt;Problems and issues&lt;/h3&gt;

&lt;p&gt;Migration from Android Studio 2.2 to Android Studio 2.3 may not be completely painless.&lt;/p&gt;

&lt;p&gt;This is a list of things that you may need to check to have everything working correctly.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;In your root build.gradle file the &lt;strong&gt;gradle version should be updated to 2.3.0&lt;/strong&gt;:&lt;/p&gt;

    &lt;p&gt;classpath ‘com.android.tools.build:gradle:2.3.0’&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The &lt;strong&gt;gradle distribution&lt;/strong&gt; should be updated to &lt;strong&gt;version 3.3&lt;/strong&gt;, so check for the following line in file gradle/wrapper/gradle-wrapper.properties:&lt;/p&gt;

    &lt;p&gt;distributionUrl=https://services.gradle.org/distributions/gradle-3.3-all.zip&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;If you are using &lt;strong&gt;Kotlin&lt;/strong&gt;, update both the Android Studio plugin and the dependency in your build.gradle to &lt;strong&gt;version 1.1.0&lt;/strong&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;If you use &lt;strong&gt;annotation processors&lt;/strong&gt; (like &lt;em&gt;ButterKnife&lt;/em&gt; for example):&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;remove the following line:&lt;/p&gt;

        &lt;p&gt;apply plugin: ‘android-apt’&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;change all occurrences of “apt” to “annotationProcessor”:&lt;/p&gt;

        &lt;p&gt;compile ‘com.jakewharton:butterknife:8.4.0’&lt;/p&gt;

        &lt;p&gt;annotationProcessor ‘com.jakewharton:butterknife-compiler:8.4.0’&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;You can find a complete list of improvements in the original blog post: &lt;a href=&quot;https://android-developers.googleblog.com/2017/03/android-studio-2-3.html&quot;&gt;https://android-developers.googleblog.com/2017/03/android-studio-2-3.html&lt;/a&gt;&lt;/p&gt;</content><author><name>Andrea Maglie</name></author><category term="android" /><category term="Android Studio" /><category term="gradle" /><category term="annotation processor" /><summary type="html"></summary></entry><entry><title type="html">Android Auto Fit TextView</title><link href="/android-auto-fit-textview.html" rel="alternate" type="text/html" title="Android Auto Fit TextView" /><published>2016-09-09T00:00:00+02:00</published><updated>2016-09-09T00:00:00+02:00</updated><id>/android-auto-fit-textview</id><content type="html" xml:base="/android-auto-fit-textview.html">&lt;p&gt;Android framework provides no support for creating a TextView that can fit its content to its size.&lt;/p&gt;

&lt;p&gt;There are some libraries out there that try to solve this problem, like:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;github.com/AndroidDeveloperLB/AutoFitTextView&quot;&gt;AutoFitTextView&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;github.com/grantland/android-autofittextview&quot;&gt;android-autofittextview&lt;/a&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;But no one of them seems to work in every situation.
Looking into &lt;em&gt;stackoverflow&lt;/em&gt; I’ve come to this &lt;a href=&quot;http://stackoverflow.com/questions/16017165/auto-fit-textview-for-android/21851239&quot;&gt;post&lt;/a&gt; which seems to hold the best working answer that I’ve tried.&lt;/p&gt;

&lt;p&gt;Except when I’ve tried to use it when &lt;em&gt;textAllCaps=”true”&lt;/em&gt;.
The solution to this issue has already been reported in one of the comments.
So I put up all togheter, and the result is the following &lt;em&gt;AutoResizeTextView&lt;/em&gt; implementation:&lt;/p&gt;

&lt;script src=&quot;https://gist.github.com/TechIsFun/df5270aea46968ea165fb52d41bde803.js&quot;&gt;&lt;/script&gt;</content><author><name>Andrea Maglie</name></author><category term="android" /><summary type="html">Android framework provides no support for creating a TextView that can fit its content to its size.</summary></entry><entry><title type="html">Workshop su Google, Android e IoT</title><link href="/workshop-google-android-iot.html" rel="alternate" type="text/html" title="Workshop su Google, Android e IoT" /><published>2016-06-18T00:00:00+02:00</published><updated>2016-06-18T00:00:00+02:00</updated><id>/workshop-google-android-iot</id><content type="html" xml:base="/workshop-google-android-iot.html">&lt;figure&gt;
	&lt;img src=&quot;/images/google_android_iot.png&quot; alt=&quot;&quot; /&gt;
&lt;/figure&gt;

&lt;p&gt;Oggi si è tenuto al FabLab di Padova un &lt;a href=&quot;http://www.officinedigitalizip.it/iot-intel-fablab-officine-digitali&quot;&gt;Workshop sull’&lt;em&gt;Internet of Things&lt;/em&gt;&lt;/a&gt;, durante il quale ho avuto il piacere di anticipare l’intervento di &lt;em&gt;Intel&lt;/em&gt; con un intervento dal titolo &lt;em&gt;#Google, #Android, #IoT&lt;/em&gt;.&lt;/p&gt;

&lt;figure&gt;
	&lt;img src=&quot;/images/google_android_iot_2.png&quot; alt=&quot;&quot; /&gt;
&lt;/figure&gt;

&lt;p&gt;Durante questo intervento ho fornito agli attendenti una panoramica sui servizi che Google mette a disposizione per l’universo dell’Internet of Things e mostrato come è possibile far comunicare una &lt;em&gt;app Android&lt;/em&gt; con una scheda &lt;em&gt;Arduino&lt;/em&gt;, sia a corto raggio tramite Bluetooth che da remoto appoggiandosi su un server &lt;em&gt;Firebase&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Le slide sono liberamente consultabili a &lt;a href=&quot;https://goo.gl/photos/CjAZ3zQDkeedCnAx9&quot;&gt;questo indirizzo&lt;/a&gt;.&lt;/p&gt;

&lt;iframe src=&quot;https://www.facebook.com/plugins/post.php?href=https%3A%2F%2Fwww.facebook.com%2Fofficinedigitalizip%2Fphotos%2Fa.1656856607881766.1073741829.1649972415236852%2F1781290402105052%2F%3Ftype%3D3&amp;amp;width=500&quot; width=&quot;500&quot; height=&quot;380&quot; style=&quot;border:none;overflow:hidden&quot; scrolling=&quot;no&quot; frameborder=&quot;0&quot; allowtransparency=&quot;true&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;Di seguito i riferimenti ai servizi, prodotti e librerie citati nell’intervento:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Firebase: &lt;a href=&quot;firebase.google.com&quot;&gt;firebase.google.com&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Brillo: &lt;a href=&quot;developers.google.com/brillo/&quot;&gt;developers.google.com/brillo/&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Weave: &lt;a href=&quot;developers.google.com/weave/&quot;&gt;developers.google.com/weave/&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Retrofit: &lt;a href=&quot;square.github.io/retrofit/&quot;&gt;square.github.io/retrofit/&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Node.js: &lt;a href=&quot;http://nodejs.org&quot;&gt;nodejs.org&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Johnny-five: &lt;a href=&quot;github.com/rwaldron/johnny-five/&quot;&gt;github.com/rwaldron/johnny-five/&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Udoo: &lt;a href=&quot;www.udoo.org&quot;&gt;www.udoo.org&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Raspberry: &lt;a href=&quot;www.raspberrypi.org&quot;&gt;www.raspberrypi.org&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;IFTTT: &lt;a href=&quot;ifttt.com&quot;&gt;ifttt.com&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Zapier: &lt;a href=&quot;zapier.com&quot;&gt;zapier.com&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Grazie a &lt;a href=&quot;http://www.officinedigitalizip.it/&quot;&gt;OfficineDigitali ZIP&lt;/a&gt; per aver ospitato il mio intervento!&lt;/p&gt;</content><author><name>Andrea Maglie</name></author><category term="android" /><category term="google" /><category term="iot" /><summary type="html"></summary></entry></feed>