<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.6.3">Jekyll</generator><link href="/feed.xml" rel="self" type="application/atom+xml" /><link href="/" rel="alternate" type="text/html" /><updated>2020-10-01T14:38:32+02:00</updated><id>/</id><title type="html">ing. Andrea Maglie</title><subtitle></subtitle><author><name>Andrea Maglie</name></author><entry><title type="html">DevFest 2020</title><link href="/devfest-2020.html" rel="alternate" type="text/html" title="DevFest 2020" /><published>2020-10-01T00:00:00+02:00</published><updated>2020-10-01T00:00:00+02:00</updated><id>/devfest-2020</id><content type="html" xml:base="/devfest-2020.html">&lt;iframe width=&quot;100%&quot; height=&quot;400&quot; src=&quot;https://www.youtube.com/embed/z-Wdaw0VRak&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;

&lt;p&gt;Quest‚Äôanno la Dev Fest Veneto lascia lo spazio ad un evento tutto nuovo: la &lt;strong&gt;DevFest 2020&lt;/strong&gt;!&lt;/p&gt;

&lt;p&gt;Non sai ancora cos‚Äô√® una DevFest?&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Le DevFest sono degli eventi organizzati una volta l‚Äôanno dalle diverse community GDG in tutto il mondo, e si differenziano dagli altri eventi in quanto hanno una durata di uno o pi√π giorni, un bacino di utenza molto pi√π elevato e la possibilit√† di avere interventi di altissima qualit√† sostenuti da aziende specializzate, professionisti o GDE (Google Developer Expert) provenienti da diverse parti d‚ÄôItalia.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Questa DevFest si distingue dalle precedenti perch√© nasce dalla collaborazione di diversi GDG Italiani (tra cui il &lt;strong&gt;GDG Venezia&lt;/strong&gt; naturalmente) e sar√† &lt;strong&gt;completamente on-line&lt;/strong&gt;. Sar√† quindi un unico evento nazionale che va a sostituire (per quest‚Äôanno) i singoli eventi regionali, in modo da rendere pi√π semplice la partecipazione di tutti nel pieno rispetto delle norme anti COVID-19.&lt;/p&gt;

&lt;h2 id=&quot;quando&quot;&gt;Quando?&lt;/h2&gt;

&lt;p&gt;üóìÔ∏è La DevFest 2020 si terr√† il 17 il 18 ottobre.&lt;/p&gt;

&lt;h2 id=&quot;di-cosa-si-parler√†&quot;&gt;Di cosa si parler√†?&lt;/h2&gt;

&lt;p&gt;Si pronunciano dei talk di ottima qualit√† su 7 aree tematiche:&lt;/p&gt;

&lt;p&gt;‚òÅ Cloud Computing&lt;/p&gt;

&lt;p&gt;ü§ñ Machine Learning&lt;/p&gt;

&lt;p&gt;üíª Front-end&lt;/p&gt;

&lt;p&gt;üì± Mobile&lt;/p&gt;

&lt;p&gt;üìä Marketing&lt;/p&gt;

&lt;p&gt;üåç Diversity &amp;amp; Inclusion&lt;/p&gt;

&lt;p&gt;üòé Soft Skill&lt;/p&gt;

&lt;p&gt;Il programma completo lo trovi nel sito web ufficiale:  &lt;a href=&quot;http://devfest.it/&quot;&gt;devfest.it&lt;/a&gt;&lt;/p&gt;

&lt;h1 id=&quot;come-mi-posso-iscrivere&quot;&gt;Come mi posso iscrivere?&lt;/h1&gt;

&lt;p&gt;L‚Äôiscrizione √® completamente gratuita. Per prenotare il tuo posto virtuale vai a &lt;a href=&quot;http://gdg.community.dev/e/mjmb5j/&quot;&gt;questo link&lt;/a&gt; e registrati!&lt;/p&gt;

&lt;h1 id=&quot;aiutaci-a-diffondere-la-devfest&quot;&gt;Aiutaci a diffondere la DevFest!&lt;/h1&gt;

&lt;p&gt;Per diffondere la DevFest condividi i link presenti in questo articolo e ricordati di usare gli hashtag ufficiali: #devfestitalia #devfestita20 #devfest2020 #gdgfun&lt;/p&gt;

&lt;p&gt;Se vuoi scoprire invece quali altri DevFest sono organizzate nel resto dell‚ÄôEuropa  ‚û° &lt;a href=&quot;devfesteurope.com&quot;&gt;devfesteurope.com&lt;/a&gt;&lt;/p&gt;</content><author><name>Andrea Maglie</name></author><category term="devfest" /><summary type="html"></summary></entry><entry><title type="html">Get Started with Flutter Mobile Development: All You Need to Know</title><link href="/flutter-how-to-start.html" rel="alternate" type="text/html" title="Get Started with Flutter Mobile Development: All You Need to Know" /><published>2020-08-22T00:00:00+02:00</published><updated>2020-08-22T00:00:00+02:00</updated><id>/flutter-how-to-start</id><content type="html" xml:base="/flutter-how-to-start.html">&lt;p&gt;&lt;em&gt;Flutter&lt;/em&gt; is an open-source mobile UI framework by Google. It was released in May 2017 and the purpose behind creating Flutter was to enable users to create native mobile applications with only one codebase. In summary, it means to use a single programming language to create two different apps, one for iOS and one for Android.&lt;/p&gt;

&lt;p&gt;Flutter is a framework that uses the programming language called &lt;em&gt;Dart&lt;/em&gt;. The language is a lot older than the framework itself and it was first created by Google in October 2011. It has improved a lot in the past few years and a major reason behind that is the continuous development by Google.&lt;/p&gt;

&lt;h2 id=&quot;why-learn-flutter&quot;&gt;Why Learn Flutter?&lt;/h2&gt;

&lt;p&gt;The reason behind learning Flutter is because of the following reasons:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;It is simple to learn. Since it is based on a framework, you don‚Äôt have to start everything from scratch. The best part about Flutter is that it is a comprehensive documentation available and a great community around it.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;You can publish to iOS and Android with the same course. No need to develop two different code bases for publishing your app.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Google Code labs offer guided tutorials for getting started with Flutter and launching your first project without any external help.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Flutter can increase user productivity exponentially through performance-focused code and a powerful UI experience made possiblebecause of Flutter widgets.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;The popularity of Flutter can be easily gauged through this diagram.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;./images/flutter-interest-over-time.png&quot; alt=&quot;&quot; /&gt;&lt;/p&gt;

&lt;p&gt;As you can see in the past few years many people have started using Flutter because of its ease of use. Google, GROUPON, eBay, and Alibaba Group are also using Flutter to improve the robustness of their apps in a short time.&lt;/p&gt;

&lt;h2 id=&quot;install-and-configure-flutter-sdk&quot;&gt;Install and configure Flutter SDK&lt;/h2&gt;

&lt;p&gt;Now that we are familiar with Flutter, it is time to get started with this programming framework.&lt;/p&gt;

&lt;p&gt;To get started, download Flutter SDK for your platform (Windows, macOS, Linux or Chrome OS) from this link: https://flutter.dev/docs/get-started/install. We will be using Mac to go through the whole installation process.&lt;/p&gt;

&lt;p&gt;1 - Extract the files to your desired location (for example: &lt;code class=&quot;highlighter-rouge&quot;&gt;~/development&lt;/code&gt;)&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;cd&lt;/span&gt; ~/development
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;unzip ~/Downloads/flutter_macos_1.20.2-stable.zip
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;2 - Add Flutter tool to your path.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;export &lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;PATH&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$PATH&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;:&lt;/span&gt;&lt;span class=&quot;sb&quot;&gt;`&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;pwd&lt;/span&gt;&lt;span class=&quot;sb&quot;&gt;`&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;/flutter/bin&quot;&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;3 - Download Flutter pre-development binaries by running the following code:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;flutter precache
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;4 - Check if you have missed anything during the installation of Flutter SDK with the following command:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;flutter doctor
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This command tells how and what dependencies need to be added to Flutter to make it work without any error. For example, you can find out if there‚Äôs a configuration error, like missing command-line tools in Android SDK:&lt;/p&gt;

&lt;div class=&quot;language-xml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;[!] Android toolchain - develop for Android devices
    ‚Ä¢ Android SDK at /usr/local/Caskroom/android-platform-tools/27.0.1
    ‚Ä¢ Android NDK location not configured (optional; useful for native profiling support)
    ‚Ä¢ ANDROID_HOME = /usr/local/Caskroom/android-sdk/3859397
    ‚úó Android SDK is missing command line tools;
    ‚Ä¢ Try re-installing or updating your Android SDK,
      visit https://flutter.io/setup/#android-setup for detailed instructions.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This error means that you won‚Äôt be able to build any Flutter project until you fix it. The following output contains some warning that can be ignored:&lt;/p&gt;

&lt;div class=&quot;language-xml highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;Doctor summary (to see all details, run flutter doctor -v):
[‚úì] Flutter (Channel stable, 1.20.2, on Mac OS X 10.15.5 19F101, locale it)

[‚úì] Android toolchain - develop for Android devices (Android SDK version 29.0.3)
[‚úì] Xcode - develop for iOS and macOS (Xcode 11.6)
[‚úì] Android Studio (version 4.0)
[!] IntelliJ IDEA Ultimate Edition (version 2020.2)
    ‚úó Flutter plugin not installed; this adds Flutter specific functionality.
    ‚úó Dart plugin not installed; this adds Dart specific functionality.
[!] VS Code (version 1.44.2)
    ‚úó Flutter extension not installed; install from
      https://marketplace.visualstudio.com/items?itemName=Dart-Code.flutter
[!] Connected device
    ! No devices available

! Doctor found issues in 3 categories.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In this case the sdk is correctly configured supposing that we work with Android Studio and not with IntelliJ or Visual Studio code.&lt;/p&gt;

&lt;p&gt;The message &lt;code class=&quot;highlighter-rouge&quot;&gt;! No devices available&lt;/code&gt; just means that no physical device is actually connected and no emulator is running. We can continue to develop and build our application without any attached device; we‚Äôll need to attach a device (or start emulator) only when we want to launch the app.&lt;/p&gt;

&lt;h3 id=&quot;assign-the-same-path-to-flutter--dart&quot;&gt;Assign the Same Path to Flutter &amp;amp; Dart&lt;/h3&gt;

&lt;p&gt;Starting from release 1.19.0, the Flutter SDK already contains the &lt;code class=&quot;highlighter-rouge&quot;&gt;dart&lt;/code&gt; command alongside the &lt;code class=&quot;highlighter-rouge&quot;&gt;flutter&lt;/code&gt; command. But if you installed Flutter and Dart at different times, you need to double check paths. Xcode won‚Äôt work if the assigned path of Flutter and Dart programming language is different. So you need to assign the same path to both of them. Here is an example of the wrong assigned path.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;which flutter dart

/path-to-flutter-sdk/bin/flutter
/usr/local/bin/dart
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;As you can see, files are placed in different &lt;code class=&quot;highlighter-rouge&quot;&gt;bin&lt;/code&gt; directories.&lt;/p&gt;

&lt;p&gt;Fix your path env variable until you get an output like this:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;which flutter dart

/path-to-flutter-sdk/bin/flutter
/path-to-flutter-sdk/bin/dart
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;install-xcode-on-mac&quot;&gt;Install Xcode on Mac&lt;/h3&gt;

&lt;p&gt;Mac needs to install &lt;a href=&quot;https://developer.apple.com/xcode/&quot;&gt;Xcode&lt;/a&gt; to run the Flutter framework. So, download and install Xcode on your Mac system.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;xcode-select &lt;span class=&quot;se&quot;&gt;\-&lt;/span&gt;&lt;span class=&quot;nt&quot;&gt;-switch&lt;/span&gt; /Applications/Xcode.app/Contents/Developer
&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;xcodebuild &lt;span class=&quot;nt&quot;&gt;-runFirstLaunch&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Make sure that Xcode license agreement is signed, or else your mobile application won‚Äôt reach the build stage. To do so, type:&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;sudo &lt;/span&gt;xcodebuild &lt;span class=&quot;nt&quot;&gt;-license&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;xcode-emulator&quot;&gt;Xcode Emulator&lt;/h3&gt;

&lt;p&gt;Xcode offers a mobile application emulator so that you can test the apps that you have created in a virtual environment. Type the following command to run the Xcode emulator.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;open &lt;span class=&quot;nt&quot;&gt;-a&lt;/span&gt; Simulator
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Once you successfully run your Xcode simulator, you can start creating your first Flutter app.&lt;/p&gt;

&lt;h2 id=&quot;creating-your-first-flutter-app&quot;&gt;Creating Your First Flutter App&lt;/h2&gt;

&lt;p&gt;Type the following command to start creating your app.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;flutter create my_flutter_app
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;You will see a directory created. Enter this directory by typing the folder name in the console.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;cd &lt;/span&gt;my_flutter_app
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Once you have created your first app, you can launch it on the iOS emulator using the following command.&lt;/p&gt;

&lt;div class=&quot;language-bash highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;nv&quot;&gt;$ &lt;/span&gt;flutter run
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h2 id=&quot;more-resources-to-learn-flutter&quot;&gt;More Resources to Learn Flutter&lt;/h2&gt;

&lt;p&gt;If you are interested in developing your web project using Flutter, you will need to learn everything about how Flutter works. Here are some of the best resources that can help you do that.&lt;/p&gt;

&lt;h3 id=&quot;dartdev&quot;&gt;&lt;a href=&quot;https://dart.dev/&quot;&gt;Dart.dev&lt;/a&gt;&lt;/h3&gt;

&lt;p&gt;This is the official documentation for learning everything about the Dart programming language. The documentation covers every aspect of Dart language, from &lt;a href=&quot;https://dart.dev/samples&quot;&gt;Hello World&lt;/a&gt; to more advanced concepts like &lt;a href=&quot;https://dart.dev/guides/language/effective-dart&quot;&gt;Effective Dart&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;flutter-gallery&quot;&gt;Flutter Gallery&lt;/h3&gt;

&lt;p&gt;Want to create an app but don‚Äôt want to start from scratch? That‚Äôs where the Flutter gallery comes in. It offers bits and pieces that you can join together to create what you want. Just like Lego.&lt;/p&gt;

&lt;h3 id=&quot;flutters-for-beginners&quot;&gt;Flutters for Beginners&lt;/h3&gt;

&lt;p&gt;So you want to start learning Flutter? The best way to learn a new framework is through a proper book. Books offer comprehensive experience and they cover the whole subject from top to bottom making it easier for users to easily grasp new concepts without worry. &lt;a href=&quot;https://amzn.to/3kDTk99&quot;&gt;Flutter for Beginners&lt;/a&gt; is a great book to learn Flutter.&lt;/p&gt;

&lt;h3 id=&quot;flutter-development-free-course&quot;&gt;Flutter Development Free Course&lt;/h3&gt;

&lt;p&gt;I personally love free courses on anything that can create value in my life. As a developer, I love all courses related to Flutter that can enhance my knowledge. The &lt;a href=&quot;https://resocoder.com/2019/08/27/flutter-tdd-clean-architecture-course-1-explanation-project-structure/&quot;&gt;Flutter development course&lt;/a&gt; by Reso Coder is a great resource for beginners.&lt;/p&gt;

&lt;h3 id=&quot;flutter-newsletter&quot;&gt;Flutter Newsletter&lt;/h3&gt;

&lt;p&gt;Finally, another great resource for learning all about the Flutter framework and staying updated on the latest developments is the &lt;a href=&quot;https://flutterweekly.net/&quot;&gt;Flutter newsletter&lt;/a&gt;. Simply subscribe to the newsletter if you already know Flutter but want to stay up to date on the latest happenings about the programming framework in the digital arena.&lt;/p&gt;

&lt;h2 id=&quot;ready-to-start-flutter-development&quot;&gt;Ready to Start Flutter Development?&lt;/h2&gt;

&lt;p&gt;Feeling excited? So am I!&lt;/p&gt;

&lt;p&gt;Flutter development opens many new opportunities for users who would like to stay at the edge of their game. With Flutter, people can cut their costs of developing applications. Where they were building multiple applications, they can now use a single application that will work cross-platform without a problem.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;If you still have any questions left, please feel free to ask us in the comment box below.&lt;/em&gt;&lt;/p&gt;</content><author><name>Andrea Maglie</name></author><category term="flutter" /><category term="mobile" /><summary type="html">Flutter is an open-source mobile UI framework by Google. It was released in May 2017 and the purpose behind creating Flutter was to enable users to create native mobile applications with only one codebase. In summary, it means to use a single programming language to create two different apps, one for iOS and one for Android.</summary></entry><entry><title type="html">Esperimenti sulla percezione audio‚Äìaptica</title><link href="/esperimenti-percezione-audio-aptica.html" rel="alternate" type="text/html" title="Esperimenti sulla percezione audio--aptica" /><published>2019-02-26T00:00:00+01:00</published><updated>2019-02-26T00:00:00+01:00</updated><id>/esperimenti-percezione-audio-aptica</id><content type="html" xml:base="/esperimenti-percezione-audio-aptica.html">&lt;hr /&gt;

&lt;p&gt;&lt;i&gt;Disclaimer: Il contenuto di questo post √® un estratto della mia tesi di laurea, realizzata nel 2006. Alcuni esempi potrebbero risultare obsoleti, tuttavia la componente teorica resta un buon riferimento per chi deve approcciarsi alla materia.&lt;/i&gt;&lt;/p&gt;
&lt;hr /&gt;

&lt;h2 id=&quot;introduzione&quot;&gt;Introduzione&lt;/h2&gt;

&lt;p&gt;L‚Äôapplicazione, completa nelle sue componenti di simulazione aptica e
audio, √® stata utilizzata per effettuare degli esperimenti di percezione
audio‚Äìaptica. Questo ha due obiettivi: il primo √® verificare la
correttezza dell‚Äôapplicazione, cio√® serve a capire se effettivamente
riesce ad infondere nei soggetti le sensazioni che noi vogliamo
simulare, in particolare quella di controllare un corpo che striscia su
una superficie; il secondo obiettivo √® capire come avviene nei singoli
soggetti la percezione bimodale audio‚Äìaptica, quale senso prevale tra
tatto, vista e udito, quali caratteristiche della scena virtuale vengono
prese in considerazione e quali invece vengono trascurate.&lt;/p&gt;

&lt;p&gt;La percezione visiva non rientra nell‚Äôoggetto del nostro studio, infatti
per questo motivo la scena presentata √® graficamente molto semplice,
l‚Äôessenziale per permettere una corretta interazione tramite l‚Äôuso del
Phantom¬† Omni¬†; tuttavia in alcuni casi, come vedremo nel seguito,
alcuni soggetti sono stati influenzati lo stesso nelle loro percezioni
dalla componente grafica.&lt;/p&gt;

&lt;p&gt;Il tipo di percezione uditiva che si vuole studiare riguarda l‚Äôascolto
dei suoni di tutti i giorni; in particolare non si vuole che l‚Äôutente
riconosca la sorgente del suono, bens√¨ le caratteristiche della sorgente
(la sua ruvidit√†) e dell‚Äôevento che genera il suono (un rotolamento o
uno sfregamento).&lt;/p&gt;

&lt;h2 id=&quot;studi-precedenti&quot;&gt;Studi precedenti&lt;/h2&gt;

&lt;p&gt;Sebbene gli esperimenti di percezione vengano svolti da pi√π di
trent‚Äôanni, solo negli ultimi tempi la tecnologia ha reso possibile
l‚Äôutilizzo di strumenti che permettano l‚Äôinterazione con un ambiente
virtuale. Quando tali dispositivi non erano disponibili, gli esperimenti
di percezione aptica venivano condotti facendo interagire i soggetti con
oggetti reali usando direttamente le mani; i suoni erano reali e non
simulati; se non si volevano fornire suggerimenti visivi, l‚Äôoggetto
veniva nascosto.&lt;/p&gt;

&lt;h3 id=&quot;lederman--percezione-uditiva-delle-tessiture-1978&quot;&gt;Lederman ‚Äì Percezione uditiva delle tessiture (1978)&lt;/h3&gt;

&lt;p&gt;Susan Lederman &lt;a class=&quot;citation&quot; href=&quot;#art:lederman2&quot;&gt;(Lederman, S. J., 1979)&lt;/a&gt; nel 1978 ha svolto un insieme di tre
studi mirati a investigare il ruolo del suono prodotto dal tocco nella
percezione della tessitura di una superficie, nei quali ai soggetti era
data la possibilit√† di giudicare la ruvidit√† della superficie solamente
basandosi sul suono. Questi giudizi si sono dimostrati simili, ma non
uguali a quelli dati solo in base alle sensazioni aptiche. Negli
esperimenti in cui venivano forniti entrambi i tipi di sensazioni, i
soggetti tendevano ad usare maggiormente le informazioni aptiche.&lt;/p&gt;

&lt;h4 id=&quot;esperimento-1&quot;&gt;Esperimento 1&lt;/h4&gt;

&lt;p&gt;Nel primo esperimento i soggetti devono stimare la ruvidit√† di un
insieme di piatti di diversa dimensione basandosi solamente sul suono
prodotto dal tocco di questi; il suono viene generato dallo
sperimentatore. Dai risultati √® emerso che la ruvidit√† percepita
decresce leggermente all‚Äôaumentare della grandezza del piatto, mentre √®
proporzionale alla pressione esercitata con le dita, e tale effetto √®
pi√π evidente quanto pi√π grande √® il piatto.&lt;/p&gt;

&lt;h4 id=&quot;esperimento-2&quot;&gt;Esperimento 2&lt;/h4&gt;

&lt;p&gt;Nel secondo esperimento viene chiesto ai soggetti di giudicare la
ruvidit√† degli stessi piatti, ma in due condizioni diverse. Nella prima
la stima viene basata solo sulla percezione aptica (il suono viene
mascherato); nella seconda invece ogni utente ha a disposizione sia gli
stimoli aptici che gli stimoli uditivi, entrambi prodotti dall‚Äôutente
stesso nella sua esplorazione dei piatti.&lt;/p&gt;

&lt;p&gt;Non si sono verificate differenze significative tra i risultati nelle
due modalit√†, e gli effetti riscontrati sono simili a quelli del primo
esperimento, a differenza del fatto che la dimensione influisce ancora
di pi√π sulla ruvidit√† percepita, mentre la pressione esercitata ha un
effetto minore. Rispetto al primo esperimento invece si √® notato che la
diminuzione della ruvidit√† percepita √® pi√π marcata quando vengono
avvertite solo le sensazioni aptiche rispetto a quando vengono avvertite
solo quelle uditive. Dato che i risultati delle due modalit√† (solo tatto
e tatto pi√π udito) non sono molto diversi, si deduce che in questo
secondo esperimento gli utenti non hanno usato molto le informazioni sul
suono prodotto per discriminare le diverse ruvidit√†.&lt;/p&gt;

&lt;h4 id=&quot;esperimento-3&quot;&gt;Esperimento 3&lt;/h4&gt;

&lt;p&gt;In questo terzo esperimento sono stati usati piatti di ruvidit√†
variabile, e i soggetti dovevano giudicare in tre diverse condizioni:
solamente ascoltando il suono prodotto dallo sperimentatore, solamente
esplorando con le dita la superficie dell‚Äôoggetto e infine avendo a
disposizione entrambi i tipi di sensazioni.&lt;/p&gt;

&lt;p&gt;La ruvidit√† percepita tende a crescere monotonicamente con l‚Äôaumentare
dell‚Äôampiezza delle scanalature sulla superficie dei piatti; quando sono
presenti entrambi gli stimoli, i soggetti tendono a dare giudizi simili
a quelli dati utilizzando solo il tatto, mentre la superficie viene
giudicata pi√π ruvida quando √® presente solo lo stimolo uditivo. Sono
stati analizzati anche gli effetti della forza applicata dalle dita e la
velocit√† di movimento delle stesse sulla superficie durante
l‚Äôesplorazione: la ruvidit√† stimata √® tanto maggiore quanto maggiore √®
la forza applicata, e ci√≤ indipendentemente dalle informazioni
sensoriali coinvolte. Se si analizzano le differenze nelle stime
effettuate applicando una maggiore o minore forza con le dita, si nota
come queste siano pi√π evidenti quando √® presente solo lo stimolo uditivo
e le dita vengono mosse lentamente; se il movimento √® veloce, le
differenze nelle stime sono pi√π o meno le stesse in tutte le tre
modalit√†.&lt;/p&gt;

&lt;h4 id=&quot;osservazioni-generali&quot;&gt;Osservazioni generali&lt;/h4&gt;

&lt;p&gt;Considerando i risultati di tutti e tre gli esperimenti, i giudizi dei
soggetti riflettono la loro abilit√† nel riconoscere i cambiamenti dovuti
alle alterazioni di dimensioni, scanalature, pressione e velocit√† di
movimento. Molto probabilmente i due aspetti degli stimoli acustici che
hanno influito di pi√π sulle stime sono l‚Äôaltezza (frequenza
fondamentale) e l‚Äôampiezza; √® stato notato infatti che l‚Äôaltezza del
suono tende a decrescere all‚Äôaumentare della profondit√† delle
scanalature e della forza esercitata con le dita. Infine l‚Äôaumento della
velocit√† di esplorazione provoca un aumento dell‚Äôaltezza e
dell‚Äôampiezza.&lt;/p&gt;

&lt;h3 id=&quot;guest-catmur-e-lloyd--interazioni-audioaptiche-nella-percezione-della-ruvidit√†-2002&quot;&gt;Guest, Catmur e Lloyd ‚Äì Interazioni audio‚Äìaptiche nella percezione della ruvidit√† (2002)&lt;/h3&gt;

&lt;p&gt;Negli studi di Guest, Catmur e Lloyd &lt;a class=&quot;citation&quot; href=&quot;#art:guest&quot;&gt;(Guest, S., Catmur, C., Lloyd, D., &amp;amp; Spence, C., 2002)&lt;/a&gt; si √® posta
l‚Äôattenzione sulla cosiddetta &lt;em&gt;illusione pelle‚Äìpergamena&lt;/em&gt;, cercando di
dimostrarla attraverso tre esperimenti.&lt;/p&gt;

&lt;p&gt;L‚Äôattrezzatura sperimentale prevede l‚Äôapplicazione di fogli di carta
abrasiva su un disco di plastica, il quale ruota grazie ad un motore.
Tutto ci√≤ √® contenuto all‚Äôinterno di una scatola di legno sulla quale √®
prevista un‚Äôapertura per poter inserirvi la mano, in modo da nascondere
alla vista ci√≤ che si tocca. Premendo un apposito pulsante posto sulla
scatola, l‚Äôutente pu√≤ avanzare di esperimento in esperimento. Il suono
viene catturato da un microfono, processato in modo diverso a seconda
del tipo di esperimento e inviato al soggetto tramite un paio di cuffie.&lt;/p&gt;

&lt;h4 id=&quot;esperimento-1-1&quot;&gt;Esperimento 1&lt;/h4&gt;

&lt;p&gt;Ai soggetti √® stato chiesto di riconoscere ripetutamente quale tra due
campioni √® quello ruvido e quale quello liscio, trascurando il suono e
basandosi esclusivamente sulle sensazioni date dal tocco della carta;
tuttavia il suono prodotto viene modificato effettuando
un‚Äôamplificazione o un‚Äôattenuazione in frequenza. Sono state proprio
queste modifiche a cambiare le stime dei soggetti. In particolare, per
il campione di carta liscia, amplificare le alte frequenze ha prodotto
un forte incremento di stime errate rispetto all‚Äôattenuazione delle
stesse frequenze; per il campione ruvido invece accade il contrario:
amplificare le alte frequenze porta a meno errori nelle stime rispetto a
quando le frequenze vengono attenuate.&lt;/p&gt;

&lt;h4 id=&quot;esperimento-2-1&quot;&gt;Esperimento 2&lt;/h4&gt;

&lt;p&gt;In questo caso il microfono cattura il suono prodotto mentre il soggetto
si sfrega le mani; questo dovr√† poi giudicare, oltre al livello di
ruvidit√†, se il suono d√† una sensazione di mani asciutte o umide.&lt;/p&gt;

&lt;p&gt;Per quanto riguarda l‚Äôanalisi della ruvidit√†, la manipolazione
dell‚Äôampiezza del suono o delle frequenze non porta a cambiamenti
significativi nelle stime. Quest‚Äôultime invece cambiano quando entrambi
gli effetti vengono applicati simultaneamente; in particolare, quando il
livello di attenuazione √® di -40dB, viene percepita una ruvidit√† via via
maggiore al crescere delle amplificazioni in frequenza. Per l‚Äôanalisi
della ruvidit√† invece si ha che un suono pi√π forte porta a percepire le
mani come pi√π asciutte, e lo stesso effetto si ha con un‚Äôamplificazione
in alta frequenza.&lt;/p&gt;

&lt;h4 id=&quot;esperimento-3-1&quot;&gt;Esperimento 3&lt;/h4&gt;

&lt;p&gt;Rispetto al secondo esperimento, si √® introdotta l‚Äôanalisi degli effetti
provocati da un ritardo applicato al suono percepito. E‚Äô risultato che
con un ritardo di 150 e 300 millisecondi viene percepita una ruvidit√†
maggiore rispetto a quando l‚Äôaudio risulta perfettamente sincronizzato,
mentre si ha la sensazione di mani meno asciutte. In entrambi i casi
per√≤ l‚Äôeffetto √® pi√π marcato quando il ritardo corrisponde a 150
millisecondi; il fatto che le percezioni non cambiano linearmente con
l‚Äôaumentare del ritardo pu√≤ essere spiegato dal fatto che lo sfregamento
delle mani √® un movimento periodico, e un ritardo di circa 300
millisecondi potrebbe riportare l‚Äôaudio in fase con l‚Äôazione compiuta
dal soggetto.&lt;/p&gt;

&lt;h3 id=&quot;lederman-klatzky-morgan-e-hamilton--integrazione-delle-informazioni-multimodali-sulla-tessitura-di-una-superficie-tramite-una-sonda-2002&quot;&gt;Lederman, Klatzky, Morgan e Hamilton ‚Äì Integrazione delle informazioni multimodali sulla tessitura di una superficie tramite una sonda (2002)&lt;/h3&gt;

&lt;p&gt;In uno studio del 1986 &lt;a class=&quot;citation&quot; href=&quot;#art:lederman4&quot;&gt;(Lederman, S. J., Thorne, G., &amp;amp; Jones, B., 1986)&lt;/a&gt;, Lederman ha analizzato le
relazioni che intercorrono nella percezione della ruvidit√† tra le
sensazioni tattili e quelle visive . Agli utenti √® stato chiesto di
giudicare l‚Äôaspetto della superficie mediante il tocco, e la sensazione
tattile che potrebbe fornire solo utilizzando la vista; nel 70% dei casi
√® stato il tatto a fornire maggiori informazioni sulla natura della
superficie. Quando invece √® stato chiesto di giudicare la densit√†
spaziale della stessa, √® stato il senso della vista a fornire pi√π
informazioni nel 70% dei casi. Ci√≤ ha dimostrato che la risoluzione
spaziale del sistema visivo umano √® maggiore rispetto a quella del
sistema cutaneo, mentre quest‚Äôultimo √® pi√π preciso nella stima della
ruvidit√†. Successivamente invece √® stata presa in considerazione
l‚Äôinterazione tra tatto e udito &lt;a class=&quot;citation&quot; href=&quot;#art:lederman3&quot;&gt;(Lederman, S. J., Klatzki, R. L., Hamilton, C., &amp;amp; Morgan, T., 2002)&lt;/a&gt;; contrariamente alle
ricerche gi√† citate, questa volta la superficie veniva esplorata usando
una sonda in plastica rigida al posto delle dita. Dato che il suono
generato dal contatto tra una superficie e una sonda rigida √® pi√π forte
del suono generato quando sono le dita ad entrare in contatto con la
stessa superficie, ci si potrebbe aspettare che le informazioni sonore
giochino un ruolo pi√π importante nella percezione bimodale. Ai soggetti
√® stato chiesto di giudicare la ruvidit√† di una superficie in tre
distinte modalit√†: solo tramite il tatto, solo tramite l‚Äôudito e infine
sfruttando entrambi i sensi.&lt;/p&gt;

&lt;figure class=&quot;image&quot;&gt;
  &lt;img src=&quot;images/lederman.jpg&quot; alt=&quot;&quot; /&gt;
  &lt;figcaption&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Le stime maggiori sono state ottenute quando si poteva utilizzare solo
il tatto, quelle minori invece quando era disponibile solo lo stimolo
acustico (come √® possibile vedere in figura); √® stato calcolato che il peso delle
informazioni tattili √® del 62%, rispetto ad un 38% di quelle acustiche,
anche se queste percentuali variano da soggetto a soggetto: in alcuni si
√® verficata una dominanza del senso del tatto, in pochi altri la
dominanza dell‚Äôudito.&lt;/p&gt;

&lt;p&gt;Variando la distanza tra le asperit√† della superficie, √® stato notato
che la ruvidit√† percepita cresce al crescere della distanza variata fino
ad un certo valore; distanze oltre a questo valore portano ad un
decremento della ruvidit√† percepita. In modo simile, al crescere
dell‚Äôintervallo tra le asperit√† diminuisce la sicurezza con la quale i
soggetti hanno compiuto le loro stime.&lt;/p&gt;

&lt;h2 id=&quot;aspetti-innovativi&quot;&gt;Aspetti innovativi&lt;/h2&gt;

&lt;p&gt;Il nostro esperimento √® stato strutturato traendo spunto da tutte queste
ricerche e basandosi in modo particolare su quelle di Susan Lederman. E‚Äô
stata ripresa l‚Äôidea di analizzare la percezione audio‚Äìaptica in
diverse modalit√†, anche se in una concezione diversa. Se con gli
esperimenti precedenti ai soggetti venivano presentati prima solo gli
stimoli aptici, poi solo quelli uditivi e infine entrambi, nel nostro
caso vengono sempre presentati tutti gli stimoli contemporaneamente. Ci√≤
che cambia √® che nella prima serie di esperimenti gli stimoli audio e
aptico variano concordemente, mentre nelle successive viene variato solo
uno dei due stimoli a seconda della superficie esplorata, mentre l‚Äôaltro
viene mantenuto costante; in tal modo si creano le condizioni per
investigare come e quanto i sensi del tatto e dell‚Äôudito si influenzano
l‚Äôun l‚Äôaltro. E‚Äô stato possibile realizzare ci√≤ grazie al fatto che,
rispetto agli studi precedenti, la superficie che il soggetto deve
analizzare non √® una superficie reale, bens√¨ una rappresentazione
virtuale tridimensionale di una superficie. Inoltre tale
rappresentazione resta immutata al variare della ruvidit√† simulata,
rendendo cos√¨ possibile mascherare importanti informazioni visive senza
mascherare la superficie stessa. Mascherare l‚Äôintera superficie avrebbe
sicuramente creato problemi agli utenti durante l‚Äôesplorazione; infatti
l‚Äôesplorazione ‚Äúcieca‚Äù di un oggetto con l‚Äôuso delle mani √® una cosa
naturale, acquisita nelle esperienze di vita, ma l‚Äôuso di un dispositivo
aptico √® nuovo alla maggior parte degli utenti, cos√¨ come l‚Äôesplorazione
di un ambiente virtuale. Si capisce cos√¨ come sia utile al soggetto
avere un riferimento visivo per effettuare un‚Äôinterazione corretta con
gli oggetti simulati.&lt;/p&gt;

&lt;p&gt;L‚Äôuso di un dispositivo aptico come sonda di esplorazione e oggetti
virtuali al posto di oggetti reali introduce un‚Äôimportante differenza
rispetto agli esperimenti effettuati in passato: infatti non si pu√≤
parlare di stimoli del senso del tatto negli stessi termini usati nella
descrizione delle prove fatte usando l‚Äôesplorazione con le dita. Il
sistema cutaneo e il dispositivo aptico hanno delle risoluzioni diverse;
sicuramente il primo ha una risoluzione maggiore di quella che pu√≤
fornire il secondo. Inoltre la nostra pelle √® sensibile a pressioni,
vibrazioni e deformazioni locali, ed √® da tutti questi eventi che
traiamo informazioni circa la natura dell‚Äôoggetto che tocchiamo. Il
dispositivo aptico pu√≤ trasmette solo forze e vibrazioni, non
deformazioni locali, rendendo quindi pi√π difficile la discriminazione di
tessiture a trame fitte; inoltre tale discriminazione √® pi√π difficile
quanto pi√π bassa √® la risoluzione del dispositivo, implicando che le
sensazioni trasmesse dipendono dal dispositivo utilizzato e dalla
qualit√† dell‚Äôimplementazione software della simulazione.&lt;/p&gt;

&lt;p&gt;Queste ultime considerazioni possono essere applicate anche agli stimoli
uditivi; anche in questo caso infatti lo stimolo trasmesso non √®
prodotto da un‚Äôinterazione reale, ma √® simulato, e risulta tanto pi√π
verosimile quanto pi√π accurati sono i modelli fisici usati e quanto pi√π
efficiente √® la loro implementazione. Tuttavia ci√≤ introduce una
notevole flessibilit√† rispetto all‚Äôuso di campioni audio preregistrati;
questi ultimi infatti devono subire complesse elaborazioni se vogliono
essere usati per simulare materiali o caratteristiche diverse di uno
stesso oggetto, mentre con l‚Äôuso dei modelli fisici √® sufficiente
variare pochi parametri per ottenere suoni notevolmente diversi.&lt;/p&gt;

&lt;p&gt;L‚Äôuso di un ambiente virtuale porta ad un ulteriore elemento nuovo
rispetto agli esperimenti precedenti: proprio perch√© quasi tutti i
soggetti sono nuovi a tale tipo di esplorazione (e quelli che non sono
nuovi hanno avuto solo poche esperienze occasionali), la memoria di
esperienze passate viene a giocare un ruolo di minore importanza.&lt;/p&gt;

&lt;h2 id=&quot;sec:setup&quot;&gt;Attrezzature e organizzazione sperimentale&lt;/h2&gt;

&lt;p&gt;Gli esperimenti sono stati eseguiti utilizzando un notebook dotato di
processore Intel¬† Core¬† Duo T2400, 2 gigabyte di memoria RAM e
acceleratore grafico ATI¬†.&lt;/p&gt;

&lt;p&gt;Il soggetto √® seduto di fronte al notebook, con il dispositivo Phantom¬†
Omni¬† alla sua destra; tramite quest‚Äôultimo viene trasmesso il feedback
aptico, mentre il feedback sonoro √® trasmesso attraverso un paio di
cuffie stereo.&lt;/p&gt;

&lt;figure class=&quot;image&quot;&gt;
  &lt;img src=&quot;images/phantomfriction.jpg&quot; alt=&quot;L'interfaccia grafica dell'applicazione&quot; /&gt;
  &lt;figcaption&gt;L'interfaccia grafica dell'applicazione&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;L‚Äôinterfaccia grafica dell‚Äôapplicazione √® mostrata nella figura; il rettangolo pi√π chiaro rappresenta
la superficie da esplorare, mentre il piccolo cono azzurro rappresenta
l‚Äôestremit√† del dispositivo aptico. Il pannello sulla destra contiene
due insiemi di pulsanti radio (cio√® mutuamente esclusivi): il primo
gruppo costituisce una scala divisa in sei gradazioni (da molto liscia a
molto ruvida) per la stima della ruvidit√† di una superficie, mentre nel
secondo gruppo √® possibile scegliere il grado di sicurezza relativa alla
scelta della ruvidit√†.&lt;/p&gt;

&lt;p&gt;Ad ogni soggetto √® stato illustrato il funzionamento generico del
dispositivo aptico e come interagire con l‚Äôapplicazione; non √® stato
svelato tuttavia lo scopo dell‚Äôesperimento, il dettagli
dell‚Äôimplementazione e della variazione dei parametri. Ad ogni
esperimento, l‚Äôutente ha avuto la possibilit√† di esplorare la superficie
virtuale per il tempo che credeva necessario; terminata l‚Äôesplorazione
doveva scegliere dal pannello a destra un grado per la ruvidit√†
percepita e il grado di sicurezza della risposta. Successivamente,
cliccando sul pulsante &lt;code class=&quot;highlighter-rouge&quot;&gt;Avanti&lt;/code&gt; si passa allo scenario successivo:
graficamente non varia nulla, ma vengono variate le propriet√† aptiche
della superficie e i parametri di generazione del suono. Per fare in
modo che l‚Äôutente si renda conto dell‚Äôeffettivo cambiamento, √® stato
aggiunto un contatore autoincrementante che riporta l‚Äôindice della scena
corrente. I pulsanti radio vengono selezionati tramite l‚Äôuso del mouse o
del touchpad; √® stata implementata anche la possibilit√† di effettuare la
selezione tramite il dispositivo aptico: infatti, nel momento in cui
viene spostato il cursore al di fuori dello scenario tridimensionale
posizionandolo in un altro punto della finestra, su un altra finestra o
sul desktop, si va a controllare il cursore del mouse. Tuttavia tale
funzione √® stata poi disabilitata in quanto sembrava indurre confusione
negli utenti.&lt;/p&gt;

&lt;p&gt;Ogni esperimento prevede l‚Äôesplorazione di un totale di 36 scenari: 9
sono gli scenari distinti, ed ognuno viene ripresentato 4 volte. I primi
9 scenari costituiscono una fase di &lt;em&gt;training implicito&lt;/em&gt;: tutti i
diversi scenari vengono presentati in ordine di ruvidit√† crescente, e
sono utili al soggetto per prendere confidenza con il dispositivo aptico
e con l‚Äôapplicazione; di conseguenza, le percezioni relative a questi
scenari non sono considerate, anche se l‚Äôutente non √® consapevole di
ci√≤. Successivamente vengono presentate tre serie composte dagli stessi
scenari in ordine sparso.&lt;/p&gt;

&lt;p&gt;I nove stimoli sono stati scelti empiricamente, valutando nove diverse
combinazioni tra l‚Äôesponente &lt;script type=&quot;math/tex&quot;&gt;\beta&lt;/script&gt; del rumore &lt;script type=&quot;math/tex&quot;&gt;1/f^\beta&lt;/script&gt; e il fattore
di amplificazione tali da produrre una sensazione di ruvidit√† crescente:&lt;/p&gt;

&lt;dl&gt;
  &lt;dt&gt;scenario 1:&lt;/dt&gt;
  &lt;dd&gt;
    &lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\beta=0.2&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;fattore~di~amplificazione=0.0010;&lt;/script&gt;&lt;/p&gt;
  &lt;/dd&gt;
  &lt;dt&gt;scenario 2:&lt;/dt&gt;
  &lt;dd&gt;
    &lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\beta=1.2&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;fattore~di~amplificazione=0.0012;&lt;/script&gt;&lt;/p&gt;
  &lt;/dd&gt;
  &lt;dt&gt;scenario 3:&lt;/dt&gt;
  &lt;dd&gt;
    &lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\beta=2.0&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;fattore~di~amplificazione=0.0030;&lt;/script&gt;&lt;/p&gt;
  &lt;/dd&gt;
  &lt;dt&gt;scenario 4:&lt;/dt&gt;
  &lt;dd&gt;
    &lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\beta=0.6&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;fattore~di~amplificazione=0.0080;&lt;/script&gt;&lt;/p&gt;
  &lt;/dd&gt;
  &lt;dt&gt;scenario 5:&lt;/dt&gt;
  &lt;dd&gt;
    &lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\beta=1.4&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;fattore~di~amplificazione=0.0100;&lt;/script&gt;&lt;/p&gt;
  &lt;/dd&gt;
  &lt;dt&gt;scenario 6:&lt;/dt&gt;
  &lt;dd&gt;
    &lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\beta=1.8&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;fattore~di~amplificazione=0.0120;&lt;/script&gt;&lt;/p&gt;
  &lt;/dd&gt;
  &lt;dt&gt;scenario 7:&lt;/dt&gt;
  &lt;dd&gt;
    &lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\beta=1.6&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;fattore~di~amplificazione=0.0160;&lt;/script&gt;&lt;/p&gt;
  &lt;/dd&gt;
  &lt;dt&gt;scenario 8:&lt;/dt&gt;
  &lt;dd&gt;
    &lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\beta=1.8&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;fattore~di~amplificazione=0.0180;&lt;/script&gt;&lt;/p&gt;
  &lt;/dd&gt;
  &lt;dt&gt;scenario 9:&lt;/dt&gt;
  &lt;dd&gt;
    &lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\beta=2.0&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;fattore~di~amplificazione=0.0210;&lt;/script&gt;&lt;/p&gt;
  &lt;/dd&gt;
&lt;/dl&gt;

&lt;p&gt;Per ogni soggetto e per ogni scenario distinto, i tre valori di ruvidit√†
percepita raccolti sono stati sottoposti ad un‚Äôelaborazione preliminare,
prima di essere utilizzati per una loro rappresentazione grafica. Per
prima cosa √® stata fatta una media di questi tre valori; successivamente
i valori medi sono stati normalizzati, dividendo ognuno di questi valori
per la media dei valori percepiti dal soggetto e moltiplicando per la
media complessiva relativa a tutti i soggetti. Sulle stime cos√¨ ottenute
√® stata svolta una analisi ANOVA.&lt;/p&gt;

&lt;p&gt;Una volta terminato l‚Äôesperimento, all‚Äôutente √® stato presentato un
semplice questionario contenente le seguenti domande, quasi tutte con
risposta a scelta multipla:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Cosa ti √® sembrato variasse tra i diversi scenari? [&lt;em&gt;La parte
aptica&lt;/em&gt; | &lt;em&gt;L‚Äôaudio&lt;/em&gt; | &lt;em&gt;Entrambi&lt;/em&gt;]&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Su cosa ti sei basato prevalentemente per giudicare la ruvidit√†?
[&lt;em&gt;Sulla parte aptica&lt;/em&gt; | &lt;em&gt;Sul suono&lt;/em&gt; | &lt;em&gt;Su entrambi&lt;/em&gt;]&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Una superficie ruvida √® caratterizzata da asperit√†; secondo te,
nelle superfici che hai percepito come ruvide, la distanza tra le
varie asperit√†: [&lt;em&gt;Rimane sempre costante per tutte le superfici
ruvide&lt;/em&gt; | &lt;em&gt;Varia da superficie a superficie ma resta costante sulla
stessa superficie&lt;/em&gt; | &lt;em&gt;Varia sempre, anche per la stessa
superficie&lt;/em&gt;]&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;L‚Äôaspetto grafico ha influenzato le tue valutazioni? [&lt;em&gt;S√¨&lt;/em&gt; |
&lt;em&gt;No&lt;/em&gt;]&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Se s√¨, in che modo? A cosa ti ha fatto pensare? [&lt;em&gt;Risposta
aperta&lt;/em&gt;]&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In base a quanto hai percepito, secondo te il cursore: [&lt;em&gt;Striscia
sempre sulla superficie&lt;/em&gt; | &lt;em&gt;Rotola sempre sulla superficie&lt;/em&gt; | &lt;em&gt;In
alcuni casi rotola, in altri striscia (in tal caso dire
approssimativamente i casi in cui hai avvertito l‚Äôuno o l‚Äôalto tipo
di movimento)&lt;/em&gt;]&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Secondo te, tra i vari scenari: [&lt;em&gt;Varia solo il materiale della
superficie&lt;/em&gt; | &lt;em&gt;Varia solo il materiale del cursore&lt;/em&gt; | &lt;em&gt;Variano i
materiali di entrambi&lt;/em&gt; | &lt;em&gt;Non variano i materiali&lt;/em&gt;]&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Tutti i soggetti hanno gentilmente offerto la loro partecipazione senza
ricevere alcun compenso.&lt;/p&gt;

&lt;h2 id=&quot;esperimento-1-2&quot;&gt;Esperimento 1&lt;/h2&gt;

&lt;figure class=&quot;image&quot;&gt;
  &lt;img src=&quot;images/lineplot1.jpg&quot; alt=&quot;Grafico 1&quot; /&gt;
  &lt;figcaption&gt;Grafico 1&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;In questa prima serie di esperimenti vengono variati concordemente gli
stimoli aptici e sonori. I partecipanti sono 20 soggetti, di et√†
compresa tra i 21 e i 32 anni, 13 ragazzi e 7 ragazze. Quasi tutti i
partecipanti hanno riportato delle stime concordi con i livelli di
ruvidit√† simulati, come si pu√≤ notare dal grafico 1, il quale riporta la dipendenza tra ruvidit√†
simulata (in ascissa) e la media della ruvidit√† percepita dai soggetti
per lo stesso livello (in ordinata). Nei primi scenari (quelli
utilizzati come training implicito), tutti i soggetti tendono a
sovrastimare la ruvidit√†, giudicando molto ruvide anche le superfici pi√π
lisce. Tale problema viene per√≤ risolto dal training, il quale permette
all‚Äôutente di capire bene la variazione della scala; negli scenari
successivi infatti i soggetti iniziano ad utilizzare tutti i livelli di
ruvidit√† a disposizione.&lt;/p&gt;

&lt;p&gt;Analizzando il grafico si nota un andamento piuttosto lineare che
spazia lungo tutti i valori possibili della scala data, la quale va da 0
a 5, dove 0 indica una superficie molto liscia e 5 una superficie molto
ruvida.&lt;/p&gt;

&lt;figure class=&quot;image&quot;&gt;
  &lt;img src=&quot;images/boxplot1_.jpg&quot; alt=&quot;Grafico 2&quot; /&gt;
  &lt;figcaption&gt;Grafico 2&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Il grafico 2 dimostra invece che per alcuni scenari le
stime variano considerevolmente; questa condizione √® molto evidente ad
esempio per il livello di ruvidit√† 7, e una spiegazione pu√≤ essere
ricercata analizzando la successione nella quale gli scenari vengono
presentati all‚Äôutente. Gli scenari con livello di ruvidit√† 7 infatti
vengono quasi sempre presentati subito dopo uno scenario di livello di
ruvidit√† inferiore, ed √® ipotizzabile quindi che sia questo ad
influenzare la percezione. Se una superficie ruvida viene presentata
subito dopo una superficie molto liscia, la prima viene stimata come pi√π
ruvida di quello che √® in realt√†; se invece vengono presentate
successivamente due superfici poco diverse tra loro si otterranno stime
simili per le due. Queste considerazioni trovano riscontro se si
analizza la varianza dei risultati relativi al livello di ruvidit√† 5:
gli scenari che simulano tale livello di ruvidit√† seguono sempre scenari
che simulano ruvidit√† poco diverse, ed infatti le stime variano in un
intervallo pi√π limitato.&lt;/p&gt;

&lt;p&gt;Due soggetti sono stati scartati dall‚Äôanalisi in quanto considerabili
come &lt;em&gt;outlier&lt;/em&gt;, ovvero soggetti la cui media delle stime si discosta
troppo dalla media di tutti gli altri partecipanti: entrambi non hanno
saputo utilizzare pienamente la scala a disposizione.&lt;/p&gt;

&lt;h2 id=&quot;esperimento-2-2&quot;&gt;Esperimento 2&lt;/h2&gt;

&lt;p&gt;Nella seconda serie di esperimenti i partecipanti sono 20 soggetti di
et√† compresa tra i 19 e i 30 anni, dei quali 14 ragazzi e 6 ragazze. Lo
stimolo acustico viene variato mentre
quello aptico viene mantenuto costante ad un livello tale da simulare
una superficie ruvida o poco ruvida (&lt;script type=&quot;math/tex&quot;&gt;\beta=0.8&lt;/script&gt;, nessuna
amplificazione).&lt;/p&gt;

&lt;p&gt;Anche in questo caso √® stata confermata l‚Äôutilit√† del training implicito
come strumento che permette all‚Äôutente di prendere confidenza con il
dispositivo e con la scala imposta.&lt;/p&gt;

&lt;p&gt;Dall‚Äôanalisi delle stime √® stato notato che la variazione discorde dei
due stimoli ha creato molta confusione nei soggetti che si sono
sottoposti a questa serie di esperimenti; infatti √® risultato che:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;pi√π della met√† degli utenti (11 soggetti) ha basato la propria stima
o solo sullo stimolo acustico o su una combinazione di entrambi gli
stimoli, producendo risultati variabili e concordi con la variazione
della stimolo acustico (tali soggetti verrano indicati come primo
gruppo);&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;alcuni soggetti non hanno preso in considerazione lo stimolo
acustico o si sono fatti influenzare da questo in minima parte,
producendo quindi stime della ruvidit√† che variano nel piccolo
intervallo della scala compreso tra ‚Äúpoco ruvido‚Äù e ‚Äúmolto ruvido‚Äù
(tali soggetti verrano indicati come secondo gruppo).&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;image&quot;&gt;
  &lt;img src=&quot;images/lineplot2a.jpg&quot; alt=&quot;Grafico 3&quot; /&gt;
  &lt;figcaption&gt;Grafico 3&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;figure class=&quot;image&quot;&gt;
  &lt;img src=&quot;images/boxplot2a.jpg&quot; alt=&quot;Grafico 4&quot; /&gt;
  &lt;figcaption&gt;Grafico 4&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;I grafici 3 e 4 si riferiscono alle stime date dal primo
gruppo. Nel primo dei due grafici √® evidente un andamento lineare delle
percezioni simile a quello ottenuto nel primo esperimento; tuttavia si
nota come le variazioni pi√π ampie si hanno in corrispondenza dei primi
due scenari, ovvero quelli in cui la discordanza tra stimolo aptico e
uditivo √® maggiore (mentre l‚Äôaudio simula una superficie liscia, il
dispositivo aptico ne simula una ruvida). Inoltre, i quartili e le
estensioni dei dati si allargano nelle due direzioni intorno al livello
5 (livello centrale), cio√® aumentano le variazioni delle stime man mano
che aumenta l‚Äôincoerenza tra gli stimoli: questo sembra supportare
l‚Äôidea che la coerenza tra le modalit√† influenza la stima.&lt;/p&gt;

&lt;figure class=&quot;image&quot;&gt;
  &lt;img src=&quot;images/lineplot2b.jpg&quot; alt=&quot;Grafico 5&quot; /&gt;
  &lt;figcaption&gt;Grafico 5&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;figure class=&quot;image&quot;&gt;
  &lt;img src=&quot;images/boxplot2b.jpg&quot; alt=&quot;Grafico 6&quot; /&gt;
  &lt;figcaption&gt;Grafico 6&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;La stessa osservazione pu√≤ essere fatta per il secondo gruppo. In questo
caso per√≤ non sono state ottenute delle stime altrettanto lineari (grafici 5 e 6), e la maggior parte di tali stime si colloca
tra i livelli 3 e 4. L‚Äôandamento non lineare delle stime relative ai
livelli dal quarto al nono pu√≤ essere giustificato se si pensa che,
basandosi solo sulla percezione aptica (che resta invariata), la scala
delle risposte possibili diventa troppo ampia ed √® difficile associare
sempre lo stesso livello di ruvidit√† agli stessi scenari; l‚Äôalternanza
infatti √® sempre compresa tra i livelli 3 e 4, i quali corrispondono a
superficie ‚Äúpoco ruvida‚Äù e ‚Äúruvida‚Äù.&lt;/p&gt;

&lt;p&gt;Infine 4 soggetti sono stati considerati outlier in quanto non hanno
saputo utilizzare correttamente la scala a disposizione.&lt;/p&gt;

&lt;h2 id=&quot;esperimento-3-2&quot;&gt;Esperimento 3&lt;/h2&gt;

&lt;p&gt;Alla terza serie di esperimenti hanno partecipato 14 soggetti, di et√†
compresa tra i 20 e i 35 anni, 10 ragazzi e 4 ragazze. A differenza
degli esperimenti precedenti, in questo caso viene variato solo lo
stimolo aptico, mentre
lo stimolo acustico resta costante (&lt;script type=&quot;math/tex&quot;&gt;\beta=0.8&lt;/script&gt;, nessuna
amplificazione). Nuovamente il training implicito si √® rivelato utile.
Rispetto alla seconda serie di esperimenti, la variazione discorde degli
stimoli ha indotto meno confusione nei partecipanti: alcuni di loro sono
riusciti a discriminare correttamente le due variazioni, mentre molti
hanno dichiarato di aver basato le loro percezioni maggiormente sullo
stimolo aptico. Ci√≤ si riflette nella linearit√† del grafico in figura:&lt;/p&gt;

&lt;figure class=&quot;image&quot;&gt;
  &lt;img src=&quot;images/lineplot3.jpg&quot; alt=&quot;&quot; /&gt;
  &lt;figcaption&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Anche qui √® stato riscontrato un fenomeno
simile a quello verificatosi nel secondo esperimento (anche se in misura
minore): nei primi scenari (quelli dove la discordanza tra i due stimoli
√® massima) l‚Äôintervallo di variazione delle stime √® molto ampio. In
questo caso bisogna aggiungere che alcuni soggetti, ad esperimento
finito, sostenevano di essere stati messi in difficolt√† da una scala dei
valori di ruvidit√† troppo ampia. Due soggetti non sono stati in grado di
utilizzare i valori della scala e pertanto sono stati considerati
outlier.&lt;/p&gt;

&lt;h2 id=&quot;risultati-sulla-confidenzialit√†-delle-risposte&quot;&gt;Risultati sulla confidenzialit√† delle risposte&lt;/h2&gt;

&lt;p&gt;Per esprimere il grado di sicurezza relativamente alle stime date, ogni
soggetto poteva scegliere fra tre risposte: ‚Äúmolto sicuro‚Äù, ‚Äúsicuro‚Äù e
‚Äúpoco sicuro‚Äù. Nella maggior parte dei casi i partecipanti hanno scelto
l‚Äôopzione intermedia (cio√® si sentivano ‚Äúsicuri‚Äù della loro stima).
Raramente invece si sono dichiarati ‚Äúpoco sicuri‚Äù (6 partecipanti al
primo esperimento non hanno mai scelto questa risposta); per alcuni ci√≤
√® avvenuto in corrispondenza della stima di una superficie molto ruvida
subito dopo una molto liscia, mentre qualcun altro invece si √®
dichiarato poco sicuro sulla percezione di una superficie di ruvidit√†
poco diversa dalla precedente.&lt;/p&gt;

&lt;p&gt;Nel secondo esperimento si √® verificato un decremento della sicurezza,
imputabile sicuramente alla variazione discorde degli stimoli. Il
decremento della sicurezza nel terzo esperimento invece √® stato molto
pi√π lieve.&lt;/p&gt;

&lt;p&gt;Tutto ci√≤ sembrerebbe confermare il fatto che le persone sono
influenzate maggiormente dal senso del tatto rispetto a quello
dell‚Äôudito. Tuttavia non √® possibile effettuare una analisi esauriente
di questo tipo basandosi sulla confidenzialit√† delle risposte, in quanto
molti soggetti, in base a loro dichiarazioni al termine
dell‚Äôesperimento, non hanno saputo utilizzare questo strumento.&lt;/p&gt;

&lt;h2 id=&quot;risultati-sul-questionario-postesperimento&quot;&gt;Risultati sul questionario post‚Äìesperimento&lt;/h2&gt;

&lt;h3 id=&quot;esperimento-1-3&quot;&gt;Esperimento 1&lt;/h3&gt;

&lt;p&gt;La maggior parte dei partecipanti (15 su 18) hanno percepito una
variazione di entrambi gli stimoli tra i diversi scenari, mentre 11
soggetti hanno dichiarato di aver basato le loro stime sia sullo stimolo
aptico che sullo stimolo acustico. Tre soggetti, pur avendo percepito la
variazione di entrambi gli stimoli, hanno basato la loro stima solo
sulla percezione uditiva, dimostrando una scarsa confidenza nei
confronti di ci√≤ che hanno avvertito tramite il dispositivo aptico. Due
soggetti hanno percepito solo una variazione della parte aptica, basando
le loro stime solo su questa, mentre un solo soggetto ha avvertito una
variazione solo dello stimolo acustico, e pertanto ha basato le sue
stime solo su questo.&lt;/p&gt;

&lt;p&gt;Riguardo alla distribuzione delle asperit√† sulla superficie, secondo il
50% dei partecipanti questa varia da superficie a superficie restando
costante sulla stessa, mentre per il rimanente 50% si ha una variazione
anche sulla stessa superficie. Dato che il profilo della superficie
viene ricavato da un rumore frattale, la distribuzione delle asperit√† √®
del tutto casuale e varia anche sulla stessa superficie; tuttavia
l‚Äôillusione di una distribuzione costante pu√≤ essere dovuta al fatto che
le asperit√† sono molto fitte.&lt;/p&gt;

&lt;p&gt;Nonostante l‚Äôinterfaccia grafica sia stata mantenuta la pi√π semplice ed
essenziale possibile, 5 soggetti hanno dichiarato di essere stati
influenzati dall‚Äôaspetto grafico; tuttavia tale influenza dello stimolo
visivo consiste semplicemente nel constatare la concordanza tra il
movimento del dispositivo aptico e del cursore sullo schermo.&lt;/p&gt;

&lt;p&gt;Per pi√π della met√† dei soggetti (12 su 18) il cursore strisciava sulla
superficie, 2 sostengono che il cursore rotola, mentre secondo i
restanti 4 tale sensazione varia: in corrispondenza di superfici pi√π
ruvide hanno avvertito un moto di sfregamento, mentre per le superfici
lisce il cursore sembra rotolare.&lt;/p&gt;

&lt;h3 id=&quot;esperimento-2-3&quot;&gt;Esperimento 2&lt;/h3&gt;

&lt;p&gt;Pur avendo una variazione solo dello stimolo acustico, 11 soggetti su 16
hanno percepito una variazione di entrambi gli stimoli; tuttavia molti
di loro hanno dichiarato di aver percepito una variazione minima della
parte aptica in confronto alla parte acustica. I rimanenti partecipanti
invece sostengono che varia solamente quest‚Äôultima.&lt;/p&gt;

&lt;p&gt;Due soggetti facenti parte del secondo gruppo hanno dichiarato di
essersi basati prevalentemente sullo stimolo acustico per giudicare la
ruvidit√†, anche se, analizzando le loro stime, si capisce come invece
siano stati fortemente influenzati dallo stimolo aptico. Per contro, tre
utenti del primo gruppo sostengono di aver basato le loro stime
prevalentemente sullo stimolo aptico, mentre l‚Äôanalisi delle stime
dimostra che sono stati influenzati fortemente dall‚Äôaudio. Sempre nel
primo gruppo, i soggetti che hanno percepito una variazione di entrambi
gli stimoli hanno basato le loro stime su entrambi, mentre quelli che
hanno percepito solo una variazione dello stimolo acustico hanno basato
le stime su questo.&lt;/p&gt;

&lt;p&gt;Anche in questo esperimento le risposte relative alla distribuzione
delle asperit√† si distribuiscono in modo uguale tra distribuzione
regolare sulla stessa superficie e distribuzione variabile anche sulla
stessa superficie.&lt;/p&gt;

&lt;p&gt;Solo due partecipanti dichiarano di essere stati influenzati
dall‚Äôaspetto grafico: per entrambi la visione di una superficie sempre
liscia ha creato difficolt√† nel capire come variava la ruvidit√†.&lt;/p&gt;

&lt;p&gt;Secondo dodici soggetti il cursore striscia sempre sulla superficie; due
hanno riportato invece che a volte il cursore sembra rotolare e altre
volte strisciare (sulle superfici pi√π lisce rotola mentre su quelle pi√π
ruvide striscia). Per altri due infine il cursore √® sembrato rotolare
sempre.&lt;/p&gt;

&lt;p&gt;Rispetto al primo esperimento, √® stato ritenuto opportuno chiedere ai
partecipanti se hanno percepito una variazione dei materiali di cursore
e superficie, in quanto superfici costituite di diverso materiale ma
aventi livello di ruvidit√† simile producono suoni diversi. La
maggioranza ha percepito una variazione del materiale della superficie;
secondo tre soggetti invece variano i materiali di entrambi, mentre per
altri tre i materiali non variano n√© per la superficie n√© per il
cursore.&lt;/p&gt;

&lt;h3 id=&quot;esperimento-3-3&quot;&gt;Esperimento 3&lt;/h3&gt;

&lt;p&gt;Secondo una met√† dei partecipanti, tra i vari scenari cambia solo lo
stimolo aptico, mentre l‚Äôaltra met√† ha avvertito una variazione di
entrambi gli stimoli. Solo 3 soggetti hanno basato le loro stime su
entrambi gli stimoli, tutti gli altri si sono basati esclusivamente
sullo stimolo aptico; inoltre, i soggetti che nel questionario hanno
dichiarato di aver basato le loro stime su entrambi gli stimoli hanno
poi specificato di essere stati influenzati in minima parte dallo
stimolo acustico, basandosi principalmente su quello aptico.&lt;/p&gt;

&lt;p&gt;Per 4 utenti le asperit√† sono distribuite in modo non costante anche
sulla stessa superficie, mentre per tutti gli altri la distribuzione
resta costante all‚Äôinterno di una stessa superficie.&lt;/p&gt;

&lt;p&gt;Nessuno √® stato influenzato dall‚Äôaspetto grafico. I due terzi dei
partecipanti hanno avvertito un moto di sfregamento tra cursore e
superficie; i rimanenti, come √® successo negli esperimenti precedenti,
hanno dichiarato che sulle superfici pi√π lisce il cursore sembrava
rotolare, mentre in presenza di superfici ruvide hanno avvertito uno
sfregamento.&lt;/p&gt;

&lt;p&gt;Secondo la maggior parte dei soggetti (7 su 12) tra i vari scenari viene
variato il materiale della superficie, mentre quello del cursore resta
costante. Un solo soggetto ha avvertito una modifica del materiale del
cursore, mentre per tutti gli altri i materiali non cambiavano mai.&lt;/p&gt;

&lt;h2 id=&quot;conclusioni-sperimentali&quot;&gt;Conclusioni sperimentali&lt;/h2&gt;

&lt;p&gt;Negli studi condotti √® stata confermata la predominanza del senso del
tatto sull‚Äôudito, anche nel caso in cui l‚Äôutente non sia chiamato ad
esplorare oggetti reali con l‚Äôuso delle mano ma oggetti virtuali con
l‚Äôuso di un dispositivo aptico. Nonostante questa predominanza, si √®
visto come lo stimolo acustico in molti casi modula quello aptico:
infatti, pi√π della met√† dei partecipanti ha avvertito una variazione di
entrambi gli stimoli quando invece solo uno dei due veniva variato tra i
diversi scenari.&lt;/p&gt;

&lt;p&gt;La scala dei valori di ruvidit√† utilizzata si √® rivelata troppo ampia
per molti soggetti: questi si sono trovati in difficolt√† a scegliere tra
i valori ‚Äúmolto liscia‚Äù e ‚Äúliscia‚Äù e tra ‚Äúpoco ruvida‚Äù e ‚Äúruvida‚Äù.
Questa difficolt√† √® stata anche riscontrata nei partecipanti
classificati come outlier; pur non avendo questi soggetti riportato
osservazioni in proposito, analizzando le loro stime pi√π nel dettaglio
si nota una confusione nell‚Äôutilizzo dei valori estremi della scala. Non
sappiamo per√≤ se una scala ristretta a quattro valori (invece dei sei
attuali) possa risultare corretta o troppo restrittiva.&lt;/p&gt;

&lt;p&gt;L‚Äôindagine sulla confidenzialit√† delle risposte non √® stata condotta nel
modo ottimale. Come abbiamo visto, molti utenti non hanno prestato molta
attenzione a tale domanda, ritenendola poco importante o, in alcuni
casi, addirittura noiosa. Probabilmente poteva essere sufficiente
formulare solamente un quesito sul tema nel questionario
post‚Äìsperimentale.&lt;/p&gt;

&lt;p&gt;Al di l√† di questi accorgimenti, come √® emerso soprattutto dai risultati
del primo esperimento, √® stato raggiunto l‚Äôobiettivo di creare
un‚Äôapplicazione di realt√† virtuale per l‚Äôinterazione continua e bimodale
con superfici simulate. La componente aptica influenza molto la
percezione dell‚Äôutente, mentre la componente sonora, oltre ad aumentare
il realismo degli scenari virtuali, fornisce informazioni utili quando i
limiti del dispositivo aptico non permettono un‚Äôanalisi accurata della
tessitura caratterizzante la superficie esplorata.&lt;/p&gt;

&lt;h2 id=&quot;riferimenti&quot;&gt;Riferimenti&lt;/h2&gt;

&lt;ol class=&quot;bibliography&quot;&gt;&lt;li&gt;&lt;span id=&quot;art:lederman2&quot;&gt;Lederman, S. J. (1979). Auditory texture perception. &lt;i&gt;Perception&lt;/i&gt;, &lt;i&gt;8&lt;/i&gt;(1), 93‚Äì103.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;art:guest&quot;&gt;Guest, S., Catmur, C., Lloyd, D., &amp;amp; Spence, C. (2002). Audiotactile interactions in roughness perception. &lt;i&gt;Experimental Brain Research&lt;/i&gt;, &lt;i&gt;146&lt;/i&gt;(2), 161‚Äì171.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;art:lederman4&quot;&gt;Lederman, S. J., Thorne, G., &amp;amp; Jones, B. (1986). Perception of texture by vision and touch: Multidimensionality and intersensory integration. &lt;i&gt;Journal of Experimental Psychology: Human Perception and Performance&lt;/i&gt;, &lt;i&gt;12&lt;/i&gt;(2), 169‚Äì180.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;art:lederman3&quot;&gt;Lederman, S. J., Klatzki, R. L., Hamilton, C., &amp;amp; Morgan, T. (2002). Integrating Multimodal Information about Surface Texture via a Probe: Relative contributions of haptic and touch produced sound sources. In &lt;i&gt;10th Annual meeting of Haptic Interfaces for Teleoperator and Virtual Environment Systems&lt;/i&gt;. IEEE.&lt;/span&gt;&lt;/li&gt;&lt;/ol&gt;</content><author><name>Andrea Maglie</name></author><category term="haptic feedback" /><category term="pure data" /><summary type="html">Disclaimer: Il contenuto di questo post √® un estratto della mia tesi di laurea, realizzata nel 2006. Alcuni esempi potrebbero risultare obsoleti, tuttavia la componente teorica resta un buon riferimento per chi deve approcciarsi alla materia.</summary></entry><entry><title type="html">L‚Äôapplicazione Phantom Friction</title><link href="/applicazione-phantom-friction.html" rel="alternate" type="text/html" title="L'applicazione Phantom Friction" /><published>2019-02-19T00:00:00+01:00</published><updated>2019-02-19T00:00:00+01:00</updated><id>/applicazione-phantom-friction</id><content type="html" xml:base="/applicazione-phantom-friction.html">&lt;hr /&gt;

&lt;p&gt;&lt;i&gt;Disclaimer: Il contenuto di questo post √® un estratto della mia tesi di laurea, realizzata nel 2006. Alcuni esempi potrebbero risultare obsoleti, tuttavia la componente teorica resta un buon riferimento per chi deve approcciarsi alla materia.&lt;/i&gt;&lt;/p&gt;
&lt;hr /&gt;

&lt;h2 id=&quot;scopo-del-lavoro&quot;&gt;Scopo del lavoro&lt;/h2&gt;

&lt;p&gt;Per poter integrare la simulazione aptica di una tessitura con la
simulazione sonora si √® reso necessario scrivere un‚Äôapplicazione che
implementi tutti i concetti visti finora.&lt;/p&gt;

&lt;figure class=&quot;image&quot;&gt;
  &lt;img src=&quot;images/phantomfriction.jpg&quot; alt=&quot;Interfaccia grafica dell'applicazione Phantom Friction&quot; /&gt;
  &lt;figcaption&gt;Interfaccia grafica dell'applicazione Phantom Friction&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Dato che nei nostri scopi non rientra la realizzazione di particolari
tessiture grafiche o nessun altro tipo di algoritmo di computer
graphics, l‚Äôinterfaccia (mostrata in figura) risulta molto semplice; tuttavia √®
chiara, user‚Äìfriendly ed efficace (come vedremo) per un‚Äôinterazione da
parte dell‚Äôutente di tipo bimodale. Il nostro obiettivo infatti √®
permettere all‚Äôutente di &lt;em&gt;sentire&lt;/em&gt; uno o pi√π oggetti virtuali
utilizzando i sensi del tatto e dell‚Äôudito; non vogliamo assolutamente
fornire suggerimenti di tipo grafico, perch√©, come molti studi di
Lederman e Klatzky &lt;a class=&quot;citation&quot; href=&quot;#art:lederman1&quot;&gt;(Lederman, J. S. &amp;amp; Klatzky, R. L., 2004)&lt;/a&gt; hanno dimostrato, ci√≤ falserebbe la
percezione aptica e uditiva. Secondo tali ricerche il senso della vista
quasi sempre prevale sugli altri sensi quando √® utilizzato per
riconoscere oggetti. In particolare sembra che la vista prevalga
soprattutto quando √® richiesto di riconoscere le propriet√† macroscopiche
di un oggetto, come la sua forma o le tessiture che caratterizzano la
sua superficie; il tatto invece viene usato quasi quanto la vista nel
riconoscimento della ruvidit√†.&lt;/p&gt;

&lt;p&gt;L‚Äôapplicazione Phantom Friction √® stata realizzata in C/C++, su sistema
operativo Microsoft Windows. Le librerie utilizzate sono tutte
multipiattaforma, quindi con poche modifiche al codice √® possibile
effettuare il porting su altri sistemi operativi, come Linux. Allo
stesso modo le patch per Pure Data possono essere riutilizzate in Linux.&lt;/p&gt;

&lt;p&gt;Tale applicazione implementa solamente la simulazione grafica e aptica,
non la simulazione audio. Per quanto riguarda quest‚Äôultima si sono
sfruttate le patch in Pure Data gi√† descritte: una loro nuova
implementazione in un linguaggio di programmazione quale il C++ sarebbe
stata problematica, in quanto sarebbe stato necessario riscrivere non
solo gli algoritmi, ma anche tutte le funzioni gi√† implementate come
primitive in PD. D‚Äôaltro canto non √® stato possibile implementare le
parti grafica e aptica in PD. Come si √® detto, esistono librerie come
GEM che permettono di unire la simulazione grafica a quella audio con
l‚Äôutilizzo di semplici moduli che realizzano oggetti tridimensionali in
OpenGL, ma non √® possibile l‚Äôintegrazione con le funzioni aptiche. Si
potrebbe anche pensare di realizzare un oggetto per PD, appoggiandosi
eventualmente alla libreria flext, che integri le funzioni aptiche e
grafiche, ma ci√≤ introdurrebbe un overhead troppo elevato, tanto da
rendere la simulazione troppo pesante in termini di risorse di calcolo
anche per computer potenti. Si vede quindi che la soluzione di
realizzare da un lato la simulazione grafica/aptica in C/C++ e
dall‚Äôaltro la simulazione audio in PD risulta la scelta migliore.&lt;/p&gt;

&lt;h2 id=&quot;implementazione-grafica&quot;&gt;Implementazione grafica&lt;/h2&gt;

&lt;p&gt;La simulazione grafica √® stata realizzata in OpenGL utilizzando le
librerie incluse nel toolkit OpenHaptics¬†. Dal momento che queste
librerie sono state precompilate per essere usate con l‚Äôambiente di
programmazione &lt;em&gt;Visual Studio .NET&lt;/em&gt; di Microsoft, √® necessario
ricompilarle se devono essere usate con un altro ambiente. Nel nostro
caso l‚Äôapplicazione √® stata sviluppata in &lt;em&gt;Visual Studio 2005&lt;/em&gt;: perci√≤ √®
stato necessario effettuare il download del pacchetto &lt;code class=&quot;highlighter-rouge&quot;&gt;glui_v2_2.zip&lt;/code&gt;
(&lt;a href=&quot;http://glui.sourceforge.net/&quot;&gt;http://glui.sourceforge.net/&lt;/a&gt;); una volta scompattato il file, sar√†
presente una directory &lt;code class=&quot;highlighter-rouge&quot;&gt;msvc&lt;/code&gt; contenente i file dei progetti da aprire
con Visual Studio e ricompilare.&lt;/p&gt;

&lt;h3 id=&quot;rappresentazione-della-scena&quot;&gt;Rappresentazione della scena&lt;/h3&gt;

&lt;p&gt;Tramite un parallelepipedo √® stata rappresentata la superficie sulla
quale un cursore a forma di piccolo cono (controllato tramite il
Phantom¬† Omni) pu√≤ muoversi; questa superficie √® piana, di un solo
colore, priva di texture e non viene mai variata, in accordo con la
condizione secondo la quale non devono essere dati suggerimenti visivi
sul tipo di superficie. Il parallelepipedo viene creato come lista
composta da un solo oggetto:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;DraggableObject dro;
dro.displayList = glGenLists(1);
dro.transform = hduMatrix::createTranslation(surfx,surfy,surfz);
glNewList(dro.displayList, GL_COMPILE);
drawSurface(1000.0f, 800.0f, 20.0f, texturized);
glEndList();
draggableObjects.push_back(dro);
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;dove il metodo &lt;code class=&quot;highlighter-rouge&quot;&gt;drawSurface&lt;/code&gt; costruisce il parallelepipedo
specificandone i vertici, accettando come argomenti le dimensioni del
parallelepipedo lungo gli assi &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;z&lt;/script&gt; e &lt;script type=&quot;math/tex&quot;&gt;y&lt;/script&gt; e un valore booleano che
indica se all‚Äôoggetto deve essere applicata la texture grafica associata
(tale valore √® sempre impostato a falso nell‚Äôattuale implementazione).
Il comando&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;glCallList(obj.displayList);
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;effettua il rendering a video della superficie. E‚Äô possibile aggiungere
alla lista altri oggetti, nel caso ad esempio si volessero rappresentare
pi√π superfici o aggiungere poligoni in modo da rendere pi√π complessa la
forma della superficie; le modifiche alle propriet√† aptiche e grafiche
effettuate sulla lista verranno propagate contemporaneamente a tutti gli
oggetti della lista. Anche il cursore, rappresentato da un cono, viene
disegnato come una lista costituita da un oggetto singolo tramite la
funzione:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;redrawCursor(const boolean&amp;amp; h, const int&amp;amp; nShadow);
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;il parametro booleano &lt;code class=&quot;highlighter-rouge&quot;&gt;h&lt;/code&gt; deve essere impostato a 1 se si vuole che
venga fatto sia il rendering grafico che aptico del cursore, mentre
impostandolo a 0 verr√† fatto solo il rendering grafico; il secondo
parametro √® utilizzato nel calcolo delle ombre. Le istruzioni per il
rendering grafico sono le seguenti:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;gCursorDisplayList = glGenLists(1);
glNewList(gCursorDisplayList, GL_COMPILE);
qobj = gluNewQuadric();
gluQuadricDrawStyle(qobj,GLU_FILL);
gluQuadricNormals(qobj,GLU_SMOOTH);
gluQuadricOrientation(qobj,GLU_OUTSIDE);
gluCylinder(qobj, 0, 5, 15, 30, 1);
glTranslatef(0, 0, 15);
gluDisk(qobj, 0, 5, 30, 1);
gluDisk(qobj, 0, 1, 15, 1);
gluDeleteQuadric(qobj);
glEndList();
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Dal momento che il proxy √® rappresentato da un unico punto nello spazio,
l‚Äôistruzione &lt;code class=&quot;highlighter-rouge&quot;&gt;glTranslatef(0, 0, 15)&lt;/code&gt; permette di traslare il cono lungo
l‚Äôasse &lt;script type=&quot;math/tex&quot;&gt;y&lt;/script&gt; in modo tale che la sua punta coincida con la posizione del
proxy.&lt;/p&gt;

&lt;p&gt;Tramite i comandi:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;glBlendFunc(GL_SR_ALPHA, GL_ONE_MINUS_SRC_ALPHA);
glEnable(GL_BLEND);
glEnable(GL_POINT_SMOOTH);
glHint(GL_POINT_SMOOTH_HINT, GL_NICEST);
glEnable(GL_LINE_SMOOTH);
glHint(GL_LINE_SMOOTH_HINT, GL_NICEST);
glEnable(GL_POLYGON_SMOOTH);
glHint(GL_POLYGON_SMOOTH_HINT, GL_NICEST);
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;contenuti all‚Äôinterno della routine &lt;code class=&quot;highlighter-rouge&quot;&gt;initGL()&lt;/code&gt; √® stato aggiunto il
supporto all‚Äôanti‚Äìaliasing; l‚Äôeffettivo uso di tale tecnica di
rendering dipende tuttavia dalle impostazioni dell‚Äôhardware grafico.&lt;/p&gt;

&lt;h3 id=&quot;rendering-delle-ombre&quot;&gt;Rendering delle ombre&lt;/h3&gt;

&lt;p&gt;Una caratteristica grafica che si √® voluta implementare per aumentare il
realismo dello scenario senza aggiungere troppi dettagli √® costituita
dalle ombre; questa scelta √® opportuna in quanto le ombre forniscono
informazioni circa la posizione degli oggetti l‚Äôuno rispetto agli altri
(e rispetto alla sorgente di luce, anche quando questa non viene
rappresentata nella scena), ma non forniscono informazioni circa le
propriet√† aptiche, il materiale di cui √® costituito l‚Äôoggetto o le sue
tessiture pi√π di quanto non faccia gi√† la rappresentazione priva di
ombre.&lt;/p&gt;

&lt;p&gt;Un‚Äôombra viene prodotta quando una fonte di luce colpisce un oggetto che
oscura un altro oggetto o una superficie; i lati del primo oggetto che
non vengono colpiti dalla luce vengono rappresentati con un colore pi√π
scuro, ma di default non viene proiettata nessuna ombra sugli altri
oggetti o sulle superfici. Anche se esistono diversi modi per
implementare questa caratteristica, la nostra attenzione si √®
focalizzata su due di questi.&lt;/p&gt;

&lt;h4 id=&quot;trasposizione-bidimensionale-delloggetto&quot;&gt;Trasposizione bidimensionale dell‚Äôoggetto&lt;/h4&gt;

&lt;p&gt;Con questo metodo, disegnare un‚Äôombra √® semplice: si crea una copia
‚Äúappiattita‚Äù dell‚Äôoggetto e la si trasla secondo una matrice di
trasposizione per farla giacere sullo stesso piano sul quale giace
l‚Äôoggetto; la forma e la dimensione di questa trasposizione sono
determinate dalla posizione della fonte di luce. Nelle librerie
&lt;code class=&quot;highlighter-rouge&quot;&gt;glTools&lt;/code&gt; &lt;a class=&quot;citation&quot; href=&quot;#book:openglsuperbible&quot;&gt;(Wright, R. S. Jr. &amp;amp; Lipchak, B., 2004)&lt;/a&gt; √® contenuta la funzione
&lt;code class=&quot;highlighter-rouge&quot;&gt;gltMakeShadowMatrix&lt;/code&gt; che calcola la matrice di trasposizione sul piano
bidimensionale; richiede in input un vettore di tre punti giacenti sul
piano sul quale si vuole far apparire l‚Äôombra, un vettore contenente la
posizione della sorgente di luce e un puntatore alla matrice di
trasformazione che deve essere creata.&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;void gltMakeShadowMatrix(GLTVector3 vPoints[3], GLTVector4 vLightPos, GLTMatrix destMat)
{
    GLTVector4 vPlaneEquation;
    GLfloat dot;
    gltGetPlaneEquation(vPoints[0], vPoints[1],  vPoints[2], vPlaneEquation);

    // Prodotto punto per punto dei vettori contenenti
    // le posizioni del piano e della luce
    dot = vPlaneEquation[0]*vLightPos[0] +
          vPlaneEquation[1]*vLightPos[1] +
          vPlaneEquation[2]*vLightPos[2] +
          vPlaneEquation[3]*vLightPos[3];

    // Calcolo della matrice di proiezione
    // Prima colonna
    destMat[0]  = dot  - vLightPos[0] * vPlaneEquation[0];
    destMat[4]  = 0.0f - vLightPos[0] * vPlaneEquation[1];
    destMat[8]  = 0.0f - vLightPos[0] * vPlaneEquation[2];
    destMat[12] = 0.0f - vLightPos[0] * vPlaneEquation[3];

    // Seconda colonna
    destMat[1]  = 0.0f - vLightPos[1] * vPlaneEquation[0];
    destMat[5]  = dot  - vLightPos[1] * vPlaneEquation[1];
    destMat[9]  = 0.0f - vLightPos[1] * vPlaneEquation[2];
    destMat[13] = 0.0f - vLightPos[1] * vPlaneEquation[3];

    // Terza colonna
    destMat[2]  = 0.0f - vLightPos[2] * vPlaneEquation[0];
    destMat[6]  = 0.0f - vLightPos[2] * vPlaneEquation[1];
    destMat[10] = dot  - vLightPos[2] * vPlaneEquation[2];
    destMat[14] = 0.0f - vLightPos[2] * vPlaneEquation[3];

    // Quarta colonna
    destMat[3]  = 0.0f - vLightPos[3] * vPlaneEquation[0];
    destMat[7]  = 0.0f - vLightPos[3] * vPlaneEquation[1];
    destMat[11] = 0.0f - vLightPos[3] * vPlaneEquation[2];
    destMat[15] = dot  - vLightPos[3] * vPlaneEquation[3];
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Moltiplicando la matrice ottenuta per la matrice della vista corrente,
tutte le modifiche successive vengono trasposte sul piano. Nella nostra
applicazione la sorgente di luce ha coordinate &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; e &lt;script type=&quot;math/tex&quot;&gt;z&lt;/script&gt; nulle, quindi √®
stato necessario effettuare i calcoli relativi solo alla seconda colonna
della matrice (infatti i calcoli relativi alle altre colonne
restituiscono sempre &lt;script type=&quot;math/tex&quot;&gt;0&lt;/script&gt;).&lt;/p&gt;

&lt;p&gt;Come si pu√≤ ben capire, questa tecnica pu√≤ essere utilizzata solo in
rendering di scene nelle quali √® presente un solo oggetto su un piano,
oppure con pi√π oggetti giacenti sullo stesso piano ma opportunamente
spaziati (in modo che non ci si aspetti che un oggetto proietti la
propria ombra su un altro), in quanto l‚Äôombra viene proiettata solamente
sul piano e non sugli altri poligoni.&lt;/p&gt;

&lt;figure class=&quot;image&quot;&gt;
  &lt;img src=&quot;images/shadow1.jpg&quot; alt=&quot;Esempio di ombre ottenute tramite trasposizione bidimensionale degli oggetti&quot; /&gt;
  &lt;figcaption&gt;Esempio di ombre ottenute tramite trasposizione bidimensionale degli oggetti&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h4 id=&quot;shadow-mapping&quot;&gt;Shadow mapping&lt;/h4&gt;

&lt;p&gt;Questa tecnica √® pi√π complessa, ma l‚Äôidea su cui si basa √® molto
semplice: le zone in ombra sono quelle che non vengono colpite dalla
luce. Se poniamo il nostro punto di vista nella stessa posizione in cui
si trova la luce e guardiamo nella direzione in cui quest‚Äôultima √®
diretta, vediamo tutto quello che dovr√† essere illuminato; ci√≤ che non
vediamo √® in ombra.&lt;/p&gt;

&lt;p&gt;Effettuando il rendering della scena dal punto di vista della sorgente
di luce, otteniamo un depth buffer contenente, per ogni pixel, le
informazioni circa la distanza relativa tra questa sorgente e la
superficie pi√π vicina in una certa direzione; questa superficie √®
illuminata, tutte quelle che si trovano oltre restano nell‚Äôombra.
L‚Äôalgoritmo di shadow mapping consiste proprio nell‚Äôeffettuare il
rendering della scena dal punto di vista della sorgente di luce, copiare
successivamente il contenuto del depth buffer in una texture, fare il
rendering dal punto di vista della camera e applicare la nuova texture
per determinare le zone di ombra.&lt;/p&gt;

&lt;p&gt;Nell‚Äôapplicazione Phantom Friction la tecnica di shadow mapping non
viene utilizzata di default; per attivarla √® sufficiente compilare il
codice specificando la direttiva &lt;code class=&quot;highlighter-rouge&quot;&gt;_SHADOWMAPPING_&lt;/code&gt; per il preprocessore.
L‚Äôinizializzazione √® contenuta nella routine &lt;code class=&quot;highlighter-rouge&quot;&gt;ShadowMappingInit()&lt;/code&gt;, la
quale comprende il caricamento delle texture che non devono essere
modificate e l‚Äôabilitazione dell‚Äôestensione &lt;code class=&quot;highlighter-rouge&quot;&gt;GL_ARB_shadow&lt;/code&gt; (se
disponibile, questa estensione consente di eseguire un passaggio in meno
nel calcolo delle ombre). La funzione di callback per il ridisegno della
scena √®:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;void glutDisplay()
{ 
  ShadowMappingFirst(gCameraPosWC);
  drawScene();
  ShadowMappingSecond();
  RegenerateShadowMap(light0_position, shadowSize);
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Per prima cosa vengono impostate le luci:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;glLightfv(GL_LIGHT0, GL_AMBIENT, ambientLight);
glLightfv(GL_LIGHT0, GL_DIFFUSE, diffuseLight);
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;si effettua un confronto sulle ombre e si imposta il piano della visuale
corrente:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;glEnable(TEXTURETYPE);
glTexEnvi(GL_TEXTURE_ENV, GL_TEXTURE_ENV_MODE, GL_MODULATE);
glTexParameteri(TEXTURETYPE, GL_TEXTURE_COMPARE_MODE, 
                                        GL_COMPARE_R_TO_TEXTURE);
glTexParameteri(TEXTURETYPE, GL_TEXTURE_MIN_FILTER, GL_NEAREST);
glTexParameteri(TEXTURETYPE, GL_TEXTURE_MAG_FILTER, GL_NEAREST);

glEnable(GL_TEXTURE_GEN_S);
glEnable(GL_TEXTURE_GEN_T);
glEnable(GL_TEXTURE_GEN_R);
glEnable(GL_TEXTURE_GEN_Q);
glTexGenfv(GL_S, GL_EYE_PLANE, sPlane);
glTexGenfv(GL_T, GL_EYE_PLANE, tPlane);
glTexGenfv(GL_R, GL_EYE_PLANE, rPlane);
glTexGenfv(GL_Q, GL_EYE_PLANE, qPlane);
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Tramite i comandi:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;glMatrixMode(GL_MODELVIEW);
glLoadIdentity();
gluLookAt(light[0], light[1], light[2], 
                    0.0f, 0.0f, 0.0f, 0.0f, 1.0f, 0.0f);
glGetFloatv(GL_MODELVIEW_MATRIX, lightModelview);
glViewport(0, 0, shadowsize, shadowsize)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;viene spostato il punto di vista in corrispondenza della sorgente di
luce. Si copia il contenuto del depth buffer in una texture
bidimensionale:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;glCopyTexImage2D(TEXTURETYPE, 0, GL_DEPTH_COMPONENT,
                  0, 0, shadowsize, shadowsize, 0);
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Al successivo rendering della scena si torna alla prospettiva originale
con i comandi:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;glMatrixMode(GL_PROJECTION);
glLoadIdentity();
gluPerspective(40.0f, 1.0f, 274.0f, 1899.0f);
glMatrixMode(GL_MODELVIEW);
glLoadIdentity();
gluLookAt(cam[0], cam[1], cam[2], 0.0f, 0.0f, 0.0f, 0.0f, 1.0f, 0.0f);
glViewport(0, 0, windowWidth, windowHeight);
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;figure class=&quot;image&quot;&gt;
  &lt;img src=&quot;images/shadow2.jpg&quot; alt=&quot;Esempio di applicazione dello shadow
mapping.&quot; /&gt;
  &lt;figcaption&gt;Esempio di applicazione dello shadow
mapping.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h4 id=&quot;scelta-della-tecnica-di-rendering-delle-ombre&quot;&gt;Scelta della tecnica di rendering delle ombre&lt;/h4&gt;

&lt;p&gt;Nell‚Äôapplicazione Phantom Friction l‚Äôunico oggetto che deve proiettare
un‚Äôombra √® il cursore, e la proietta su un piano; quindi la tecnica pi√π
adatta (e implementata di default) √® la trasposizione bidimensionale.
Effettuando questa scelta si √® anche tenuto conto del fatto che questa
tecnica √® meno dispendiosa in termini di risorse di calcolo rispetto
allo shadow mapping, e quindi √® meno probabile che causi il verificarsi
di latenze. Il cursore viene disegnato tramite il comando:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;redrawCursor(true,0);
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;poi viene trasposto e disegnato una seconda volta, questa volta come
ombra:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;glMultMatrixf((GLfloat *)shadowMat);
redrawCursor(true,1);
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;L‚Äôunico inconveniente nel quale si incorre √® che, se il cursore viene
spostato oltre il limite della superficie, l‚Äôombra viene disegnata lo
stesso, anche se l‚Äôutente si aspetta di non vederla; per ovviare a
questo problema sono state apportate due semplici modifiche:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;l‚Äôombra viene disegnata solo se la posizione lungo l‚Äôasse $y$ del
proxy √® maggiore o uguale della posizione del piano lungo lo stesso
asse;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;l‚Äôombra e lo sfondo hanno lo stesso colore: in tal modo, quando
l‚Äôombra esce dalla superficie, si confonde con lo sfondo e, per
l‚Äôocchio umano, non √® distinguibile da questo.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;aggiunta-di-texture-grafiche&quot;&gt;Aggiunta di texture grafiche&lt;/h3&gt;

&lt;p&gt;E‚Äô stata prevista la possibilit√† di aggiungere texture grafiche
bidimensionali alle superfici degli oggetti, anche se ci√≤ non rientra
negli scopi dell‚Äôapplicazione in quanto la percezione visiva di una
tessitura distoglie l‚Äôutente dalle percezioni aptiche e uditive; tale
caratteristica √® disattivata per default, e per attivarla occorre
procedere alla ricompilazione del codice sorgente includendo la stringa
&lt;code class=&quot;highlighter-rouge&quot;&gt;_TEXTURE&lt;/code&gt; tra le direttive del preprocessore.&lt;/p&gt;

&lt;p&gt;La funzione:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;loadTexture();
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;carica in memoria tutte le texture indicate nell‚Äôarray &lt;code class=&quot;highlighter-rouge&quot;&gt;textureFiles&lt;/code&gt;; √®
sufficiente effettuare il caricamento durante l‚Äôinizializzazione
dell‚Äôapplicazione, successivamente le texture resteranno disponibili in
memoria fino alla chiusura dell‚Äôapplicazione stessa. Il comando:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;glBindTexture(GL_TEXTURE_2D,textures[n]);
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;√® utilizzato per applicare effettivamente la texture in posizione $n$
nell‚Äôarray all‚Äôoggetto corrente.&lt;/p&gt;

&lt;h2 id=&quot;implementazione-aptica&quot;&gt;Implementazione aptica&lt;/h2&gt;

&lt;h3 id=&quot;rendering-aptico-degli-oggetti-virtuali&quot;&gt;Rendering aptico degli oggetti virtuali&lt;/h3&gt;

&lt;p&gt;Per l‚Äôimplementazione del rendering grafico si sono utilizzate sia le
HDAPI che le HLAPI. Le HLAPI sono molto utili quando si vogliono
impostare le propriet√† aptiche degli oggetti virtuali e per gestire i
frame; ad esempio, per effettuare il rendering aptico della superficie
sono sufficienti le seguenti istruzioni:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;hlBeginShape(HL_SHAPE_FEEDBACK_BUFFER, obj.shapeId);

hlMaterialf(HL_FRONT_AND_BACK, HL_STIFFNESS, obj.hap_stiffness);
hlMaterialf(HL_FRONT, HL_DAMPING, obj.hap_damping);
hlMaterialf(HL_FRONT, HL_STATIC_FRICTION, obj.hap_static_friction);
hlMaterialf(HL_FRONT, HL_DYNAMIC_FRICTION, obj.hap_dynamic_friction);

glCallList(obj.displayList);

hlEndShape();
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;con le quali vengono impostate la rigidit√†, il coefficiente di
smorzamento, frizione statica e frizione dinamica. Le primitive grafiche
utilizzate sono davvero poche, e ci√≤ consiglia l‚Äôutilizzo del feedback
buffer per il rendering aptico (e di conseguenza anche per il rendering
grafico); questo viene impostato tramite il primo argomento di
&lt;code class=&quot;highlighter-rouge&quot;&gt;hlBeginShape&lt;/code&gt;.&lt;/p&gt;

&lt;p&gt;Nella rappresentazione del cursore non devono essere impostate propriet√†
aptiche, ma √® necessario calcolare le traslazioni e le trasformazioni
del proxy:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;hlGetDoublev(HL_PROXY_TRANSFORM, proxytransform);
glMultMatrixd(proxytransform);
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Tutti i comandi riguardanti il rendering grafico della superficie, del
cursore e delle ombre sono contenuti all‚Äôinterno di un blocco
&lt;code class=&quot;highlighter-rouge&quot;&gt;hlBeginFrame&lt;/code&gt;‚Äì&lt;code class=&quot;highlighter-rouge&quot;&gt;hlEndFrame&lt;/code&gt;:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;hlBeginFrame();

glClear(GL_COLOR_BUFFER_BIT | GL_DEPTH_BUFFER_BIT);

// disegna la superficie
drawDraggableObjects(false,0);

glPushMatrix();
glEnable(GL_LIGHTING);
glLightfv(GL_LIGHT0,GL_POSITION,lightPos);

// disegna il cursore
redrawCursor(true,0);

glPopMatrix();
glDisable(GL_DEPTH_TEST);
glDisable(GL_LIGHTING);
glPushMatrix();

// disegna l'ombra del cursore
hlGetDoublev(HL_PROXY_POSITION, posHD);
if(posHD[1] &amp;gt; minCursorY )
{
    glMultMatrixf((GLfloat *)shadowMat);
    redrawCursor(true,1);
}

// ripristina la normale prospettiva
glPopMatrix();

glEnable(GL_DEPTH_TEST);

hlEndFrame();
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Sempre utilizzando le HLAPI √® stata abilitata l‚Äôottimizzazione &lt;em&gt;haptic
camera view&lt;/em&gt;:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;hlEnable(HL_HAPTIC_CAMERA_VIEW);
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;in modo che venga effettuato il rendering aptico anche della porzione di
superficie non visibile nella scena. Con l‚Äôistruzione:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;hlHinti(HL_SHAPE_FEEDBACK_BUFFER_VERTICES, 100);
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;viene allocata memoria per 100 vertici, sufficienti per la
rappresentazione in feedback buffer delle poche primitive usate; ci√≤ si
traduce in un risparmio di memoria in quanto di default viene riservata
memoria sufficiente per 65536 vertici.&lt;/p&gt;

&lt;h3 id=&quot;rilevamento-dei-contatti-e-rendering-delle-tessiture-aptiche&quot;&gt;Rilevamento dei contatti e rendering delle tessiture aptiche&lt;/h3&gt;

&lt;p&gt;Tutte le istruzioni viste fino a questo punto sono sufficienti a fare in
modo che, tramite l‚Äôuso del Phantom¬† Omni¬†, il parallelepipedo possa
essere avvertito come un oggetto solido, senza la possibilit√† di
penetrarlo. Tuttavia le propriet√† aptiche restano costanti lungo tutta
la superficie dell‚Äôoggetto; se si aggiunge il fatto che tale superficie
√® piana, si pu√≤ intuire come questa sia costituita da una tessitura
aptica uniforme su tutto l‚Äôoggetto. Durante la simulazione bimodale
(cio√® quando le simulazioni aptica e sonora sono eseguite
contemporaneamente) si viene ad avvertire cos√¨ una superficie che al
tatto si presenta come costantemente ruvida (con livello di ruvidit√†
proporzionale ai valori scelti di frizione statica e dinamica)
accompagnata da suoni di sfregamento che descrivono una tessitura
frattale e non costante. La discrepanza tra le due modalit√† non √®
avvertibile quando vengono simulate superfici lisce o poco ruvide,
mentre si fa pi√π evidente all‚Äôaumentare della ruvidit√†. Sono state
valutate le seguenti metodologie per l‚Äôeffettiva implementazione di una
tessitura aptica.&lt;/p&gt;

&lt;h4 id=&quot;modifica-della-geometria-della-superficie&quot;&gt;Modifica della geometria della superficie&lt;/h4&gt;

&lt;p&gt;La superficie viene avvertita apticamente come piana in quanto
graficamente √® piana. Un modo per far avvertire una superficie ruvida √®
modificarne la geometria, con l‚Äôaggiunta ad esempio di diversi poligoni
a forma di piccoli coni che ne rappresentino le asperit√†; aggiungendo
queste primitive alla lista contenente il parallelepipedo, tutti gli
oggetti avranno le stesse propriet√† aptiche e verranno rappresentati
(sia dal punto di vista grafico che dal punto di vista aptico) come un
unico oggetto. Disponendo i piccoli coni in modo opportuno, si ottiene
una superficie che pu√≤ essere avvertita come ruvida da entrambi i punti
di vista aptico e grafico, e il livello di ruvidit√† dipende dalla
dimensione, quantit√† e disposizione dei coni.&lt;/p&gt;

&lt;p&gt;Tale soluzione per√≤ contrasta con i nostri scopi in quanto si d√† un
chiaro suggerimento visivo all‚Äôutente sulla tipologia di superficie,
facendo passare totalmente in secondo piano le caratteristiche aptiche e
sonore.&lt;/p&gt;

&lt;p&gt;Si potrebbe pensare di rappresentare le asperit√† (cio√® i piccoli coni)
solo dal punto di vista aptico e non da quello grafico; dato che anche
tutte le primitive vengono rappresentate come oggetti solidi (e quindi
non penetrabili dalla sonda), con tale metodo si verrebbero a creare
zone in cui il proxy si ‚Äúblocca‚Äù senza urtare la porzione di superficie
rappresentata graficamente (si verifica un urto aptico ma non un urto
grafico), portando ad una notevole discrepanza tra le componenti
aptica/grafica e creando di conseguenza solo confusione nell‚Äôutente.&lt;/p&gt;

&lt;h4 id=&quot;modifica-delle-propriet√†-aptiche-secondo-pattern-geometrici&quot;&gt;Modifica delle propriet√† aptiche secondo pattern geometrici&lt;/h4&gt;

&lt;p&gt;Una seconda soluzione consiste nel modificare le propriet√† aptiche della
superficie lungo la superficie stessa, invece di mantenerle costanti. Il
coefficiente di rigidit√† non pu√≤ essere variato in quanto il
parallelepipedo che rappresenta la superficie deve essere avvertito
sempre come solido; la modifica del coefficiente della forza di
smorzamento non influenza il rendering aptico in questa implementazione;
quindi gli unici due parametri che possono essere variati sono le
frizioni statica e dinamica. Entrambe sono forze che si oppongono al
movimento tangenziale di un corpo.&lt;/p&gt;

&lt;p&gt;Se pensiamo ad una superficie liscia come una superficie sulla quale i
corpi strisciano senza incontrare resistenza, mentre pensiamo ad una
superficie ruvida come una superficie sulla quale gli oggetti strisciano
incontrando una forza di resistenza ogniqualvolta urtano un‚Äôasperit√†,
allora vediamo come un aumento improvviso del coefficiente di frizione
statica in alcuni punti pu√≤ simulare la presenza di asperit√† in questi
stessi punti, senza ricorrere a variazioni della geometria dell‚Äôoggetto.&lt;/p&gt;

&lt;p&gt;In una prima implementazione si √® scelto di mantenere i coefficienti
delle due frizioni costanti e inferiori a 0.5 (ricordiamo che i
coefficienti possono assumere valori compresi tra 0 e 1) nella
simulazione di superfici lisce o poco lisce. Per le superfici ruvide i
coefficienti vengono mantenuti ad un valore basso (0.2) e incrementati
solo in alcuni punti, determinati in base alle coordinate geometriche
del punto stesso: se il cursore si trova a contatto con la superficie e
la sua posizione lungo l‚Äôasse &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; soddisfa la condizione:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;(( (int)posHD[0]*10  ) % 2) != 0
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;(dove &lt;code class=&quot;highlighter-rouge&quot;&gt;posHD[0]&lt;/code&gt; indica la posizione del proxy lungo l‚Äôasse &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt;) allora
la frizione viene impostata ad un valore tanto pi√π elevato quanto pi√π
ruvida deve risultare la superficie. Dal momento che si considera solo
l‚Äôasse &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt;, le asperit√† vengono posizionate lungo linee parallele
all‚Äôasse &lt;script type=&quot;math/tex&quot;&gt;z&lt;/script&gt; e giacenti sullo stesso piano della superficie (ovvero
quello determinato dagli assi &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; e &lt;script type=&quot;math/tex&quot;&gt;z&lt;/script&gt;).&lt;/p&gt;

&lt;p&gt;Se la precedente condizione viene sostituita dalla seguente:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;( (int)( sqrt(posHD[0]*posHD[0]+posHD[2]*posHD[2])*10  ) % 2) != 0
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;(dove &lt;code class=&quot;highlighter-rouge&quot;&gt;posHD[0]&lt;/code&gt; indica ancora la posizione del proxy lungo l‚Äôasse &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt;,
mentre &lt;code class=&quot;highlighter-rouge&quot;&gt;posHD[2]&lt;/code&gt; indica la posizione lungo l‚Äôasse &lt;script type=&quot;math/tex&quot;&gt;z&lt;/script&gt;), le asperit√†
vengono posizionate secondo cerchi concentrici giacenti sul piano &lt;script type=&quot;math/tex&quot;&gt;xz&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;Possono essere implementati facilmente altri pattern geometrici; i
micro‚Äìcontatti dei quali viene simulato il suono emesso non sono per√≤
disposti secondo pattern regolari, ma casualmente secondo un pattern
frattale. Rimane quindi una certa discrepanza tra le sensazioni aptiche
e le sensazioni uditive per superfici non lisce.&lt;/p&gt;

&lt;h4 id=&quot;modifica-delle-propriet√†-aptiche-secondo-pattern-frattali&quot;&gt;Modifica delle propriet√† aptiche secondo pattern frattali&lt;/h4&gt;

&lt;p&gt;Il passo successivo consiste nella creazione di una tessitura aptica
frattale.&lt;/p&gt;

&lt;p&gt;Si potrebbe pensare di scrivere una funzione che implementi l‚Äôalgoritmo
discusso nel &lt;a href=&quot;/pure-data-modelli-audio.html&quot;&gt;post sui modelli audio&lt;/a&gt;; ci√≤ introdurrebbe un ulteriore
appesantimento nel carico computazionale, senza garantire che il pattern
generato nella patch per PD e quello implementato nell‚Äôapplicazione
varino concordemente. Il metodo pi√π semplice e pi√π efficiente risulta
invece quello di utilizzare i dati calcolati nella subpatch
&lt;code class=&quot;highlighter-rouge&quot;&gt;holy-roller~&lt;/code&gt;. Il blocco &lt;code class=&quot;highlighter-rouge&quot;&gt;circ_max_filter~&lt;/code&gt; calcola il
profilo della superficie sulla quale avvengono i micro‚Äìcontatti; il
valore inviato in output viene letto dall‚Äôapplicazione Phantom Friction
(vedremo nella prossima sezione come avviene questa lettura) e,
opportunamente scalato, viene utilizzato come coefficiente di
proporzionalit√† per i valori di frizione statica e dinamica.&lt;/p&gt;

&lt;p&gt;Nella sezione ‚ÄúRendering delle tessiture aptiche‚Äù di &lt;a href=&quot;/dispositivi-aptici&quot;&gt;questo post&lt;/a&gt; sono state analizzate diverse
tecniche di rendering delle tessiture. Quella qui implementata pu√≤
essere descritta come una perturbazione delle forze (senza l‚Äôutilizzo
della formula di Max e Becker); inoltre, in modo simile a quanto avviene
per le tessiture aptiche basate sulle immagini, come indicatore della
profondit√† della tessitura viene usato il valore del profilo della
superficie calcolato nelle patch in Pure Data.&lt;/p&gt;

&lt;p&gt;Per implementare questo procedimento sono state utilizzate tre funzioni
di callback. La prima rileva il verificarsi del primo contatto tra proxy
e superficie e imposta a &lt;code class=&quot;highlighter-rouge&quot;&gt;true&lt;/code&gt; un valore booleano:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;void HLCALLBACK touchCallback(HLenum event, 
                              HLuint object, 
                              HLenum thread,HLcache *cache, 
                              void *userdata)
{
    touching = true;
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;La seconda invece imposta questo valore a &lt;code class=&quot;highlighter-rouge&quot;&gt;false&lt;/code&gt; quando il proxy non √®
pi√π in contatto con la superficie:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;void HLCALLBACK untouchCallback(HLenum event, 
                                HLuint object, 
                                HLenum thread, 
                                HLcache *cache, 
                                void *userdata)
{
    touching = false;
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Tale valore booleano viene usato dalla terza funzione di callback come
condizione di abilitazione del calcolo (e del conseguente invio alla
patch in PD) della velocit√† corrente e del fattore di amplificazione
proporzionale alla forza normale.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;HDCallbackCode HDCALLBACK velocitaCallback(void *pUserData)
{
    if ( touching )
    {
        // richiesta della velocit√† corrente
        hdGetDoublev(HD_CURRENT_VELOCITY, veloHD);
        // richiesta della forza corrente
        hdGetDoublev(HD_CURRENT_FORCE, forceHD);

        velocita = sqrt( pow(veloHD[0],2) + pow(veloHD[2],2) );
                
        // scrittura in memoria dei dati aptici dello scenario
        setOpeData( &amp;amp;data,
                (float)( sqrt( abs(velocita * forceHD[1]) )/1000 ), 
                velocita/1000, 
                0,
                sceneDepth[indexScene],
                0,
                0,
                sceneNoiseFile[indexScene]);
        
        WriteOpeData(&amp;amp;data);
        
        pdSurface = ReadSurface();
        
        hap_static_friction = ( pdSurface - a )/ ( b );
    }
    else
    {
        setOpeData( &amp;amp;data, 0, sceneDepth[indexScene], 0, 0, 
                                0, 0, sceneNoiseFile[indexScene]);
        
        WriteOpeData(&amp;amp;data);
    }
    
    return HD_CALLBACK_CONTINUE;
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Questa callback √® stata implementata utilizzando le HDAPI come callback
asincrona, in modo tale che venga eseguita non appena viene schedulata.
Ci√≤ √® importante in quanto qui vengono generate le tessiture aptiche, e
tale funzione deve essere eseguita alla velocit√† propria del rendering
aptico, applicando l‚Äôeffetto ad ogni iterazione del servo loop.
L‚Äôistruzione&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;hap_static_friction = ( pdSurface - a )/ ( b );
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;imposta i coefficienti di frizione al valore ricevuto da PD
(&lt;code class=&quot;highlighter-rouge&quot;&gt;pdSurface&lt;/code&gt;) e opportunamente scalato nell‚Äôintervallo &lt;code class=&quot;highlighter-rouge&quot;&gt;(a,b)&lt;/code&gt;, dove &lt;code class=&quot;highlighter-rouge&quot;&gt;a&lt;/code&gt;
√® impostato a &lt;script type=&quot;math/tex&quot;&gt;0&lt;/script&gt; (si suppone che la superficie abbia sempre un valore
positivo) e &lt;code class=&quot;highlighter-rouge&quot;&gt;b&lt;/code&gt; corrisponde a &lt;script type=&quot;math/tex&quot;&gt;0.04&lt;/script&gt; (calcolato empiricamente). La
velocit√† viene calcolata come modulo delle componenti lungo gli assi &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt;
e &lt;script type=&quot;math/tex&quot;&gt;z&lt;/script&gt; (il piano su cui giace la superficie) e viene divisa per 1000 per
adeguarla alle unit√† di misura usate dalla patch &lt;code class=&quot;highlighter-rouge&quot;&gt;sliding.pd&lt;/code&gt;: in
quest‚Äôultima la velocit√† √® assunta in &lt;script type=&quot;math/tex&quot;&gt;m/s&lt;/script&gt;, mentre le HDAPI ritornano
tale valore in &lt;script type=&quot;math/tex&quot;&gt;mm/s&lt;/script&gt;. Tramite l‚Äôistruzione:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;sqrt( abs(velocita * forceHD[1]) ),
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;dove &lt;code class=&quot;highlighter-rouge&quot;&gt;forceHD[1]&lt;/code&gt; √® la componente della forza esercitata dal dispositivo
aptico lungo l‚Äôasse &lt;script type=&quot;math/tex&quot;&gt;y&lt;/script&gt;, viene calcolato il fattore di proporzionalit√†
dell‚Äôampiezza del suono rispetto alla forza normale (vedi pagina ).&lt;/p&gt;

&lt;h2 id=&quot;scambio-dei-dati-tra-lapplicazione-phantom-friction-e-le-patch-in-pure-data&quot;&gt;Scambio dei dati tra l‚Äôapplicazione Phantom Friction e le patch in Pure Data&lt;/h2&gt;

&lt;p&gt;Punto cruciale dello sviluppo di questa applicazione √® stata
l‚Äôimplementazione di un‚Äôinterfaccia di comunicazione con Pure Data, in
modo da consentire lo scambio dei dati tra i due processi in tempo reale
durante la simulazione bimodale. E‚Äô stato importante curare questo
aspetto perch√© la comunicazione dei dati deve avvenire alla stessa
frequenza del rendering aptico, quindi 1000 volte al secondo; se le
operazioni di scambio di informazioni svolte ad ogni ciclo sono troppo
onerose, si rischia di introdurre latenze eccessive.&lt;/p&gt;

&lt;p&gt;La simulazione bimodale ha significato se gli stimoli aptico, visivo e
uditivo vengono percepiti dall‚Äôutente simultaneamente. Possiamo
distinguere tre momenti ad ogni ciclo della simulazione: il momento in
cui il proxy incontra una asperit√†, il momento in cui il dispositivo
aptico rileva questo contatto e invia una forza in retroazione e il
momento in cui viene generato il suono per questo evento. Tutti e tre i
momenti elencati devono essere distanti (nel tempo) tra di loro il meno
possibile, in accordo con la percezione dell‚Äôutente; se uno dei tre
momenti √® temporalmente troppo distante dagli altri (si verifica cio√®
una latenza), lo stimolo associato verr√† percepito come estraneo
all‚Äôevento. Se invece tutti e tre gli stimoli si verificano con latenze
minori di certi livelli, allora verranno percepiti come contemporanei e
l‚Äôutente li individuer√† come componenti di uno stesso evento.&lt;/p&gt;

&lt;p&gt;Si possono distinguere latenze &lt;em&gt;intramodali&lt;/em&gt; e latenze &lt;em&gt;intermodali&lt;/em&gt;:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;latenze intramodali: le latenze intramodali si verificano tra stimoli dello stesso tipo:
  tra due stimoli sonori, tra due stimoli aptici o tra due stimoli
  visivi; i valori di latenza minimi percepibili variano a seconda
  della modalit√† considerata e corrispondono a 2 millisecondi per il
  suono, 27 millisecondi per il tatto e 43 millisecondi per la vista
  &lt;a class=&quot;citation&quot; href=&quot;#proc:levitin&quot;&gt;(Levitin, D. J., Mathews, M. V., &amp;amp; MacLean, K., 1999)&lt;/a&gt;;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;latenze intermodali: le latenze intermodali si verificano tra stimoli di tipo diverso (ad
  esempio si considera la latenza dello stimolo sonoro rispetto agli
  stimoli visivo e aptico), e i valori minimi sono diversi a seconda
  che lo stimolo preceda o segua gli altri: se un suono precede gli
  stimoli visivo e aptico, la massima latenza accettata √® di 25
  millisecondi, mentre se li segue tale latenza sale a 42 millisecondi
  &lt;a class=&quot;citation&quot; href=&quot;#proc:levitin&quot;&gt;(Levitin, D. J., Mathews, M. V., &amp;amp; MacLean, K., 1999)&lt;/a&gt;.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Se consideriamo che Pure Data, sintetizzando i suoni a 44100 Hz e avendo
un buffer di 64 campioni, introduce una latenza di 1,45 millisecondi,
mentre nell‚Äôapplicazione Phantom Friction l‚Äôuso delle HLAPI introduce
una latenza di 10 millisecondi (per le HDAPI la latenza √® di 1
millisecondo), si vede che resta un margine di circa 15 millisecondi per
una discrepanza temporale tra gli stimoli che sia accettabile.&lt;/p&gt;

&lt;p&gt;Le soluzioni per la comunicazione tra i due processi sono essenzialmente
tre &lt;a class=&quot;citation&quot; href=&quot;#phd:crosato&quot;&gt;(Crosato, P., 2006)&lt;/a&gt;.&lt;/p&gt;

&lt;h4 id=&quot;protocollo-di-rete&quot;&gt;Protocollo di rete&lt;/h4&gt;

&lt;p&gt;Dato che in Pure Data √® supportata la lettura dei dati (via protocollo
TCP e UDP) tramite l‚Äôoggetto &lt;code class=&quot;highlighter-rouge&quot;&gt;netreceive~&lt;/code&gt;, sarebbe sufficiente
implementare la lettura/scrittura dei dati nell‚Äôapplicazione Phantom
Friction tramite l‚Äôuso dei socket. Il vantaggio √® che i due processi
potrebbero cos√¨ comunicare anche in remoto, facendo eseguire la
simulazione aptica e quella sonora su due macchine diverse. Lo
svantaggio √® che la latenza dipende fortemente dalle caratteristiche
dell‚Äôinterfaccia di rete della macchina e dal traffico presente sulla
rete. Inoltre viene ridotta la portabilit√† del codice, in quanto la
programmazione dei socket in C++ √® dipendente dal sistema operativo.&lt;/p&gt;

&lt;h4 id=&quot;driver-di-periferica&quot;&gt;Driver di periferica&lt;/h4&gt;

&lt;p&gt;Si potrebbe creare un external per Pure Data che legga i dati
utilizzando i driver del dispositivo Phantom¬† Omni¬†. Questa soluzione √®
notevolmente complessa, a causa della scarsit√† di documentazione in
merito e a causa del fatto che non si potrebbero estrarre i dati
relativi alla geometria degli oggetti e agli istanti di collisione.
Inoltre resterebbe irrisolto il problema di poter leggere i dati di Pure
Data tramite l‚Äôapplicazione Phantom Friction.&lt;/p&gt;

&lt;h4 id=&quot;memoria-condivisa&quot;&gt;Memoria condivisa&lt;/h4&gt;

&lt;figure class=&quot;image&quot;&gt;
  &lt;img src=&quot;images/memoria_condivisa.jpg&quot; alt=&quot;&quot; /&gt;
  &lt;figcaption&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;La tecnica pi√π efficiente si √® rivelata la predisposizione di un‚Äôarea di
memoria condivisa per la lettura/scrittura dei dati da parte di entrambi
i processi. Lo svantaggio di questa soluzione √® che dipende dalla
piattaforma sulla quale il programma viene eseguito; inoltre Pure Data
non possiede un oggetto interno che implementi una funzione utile allo
scopo, perci√≤ √® stato necessario scrivere un external. Il vantaggio
maggiore (che compensa gli svantaggi citati) √® la velocit√† con la quale
avvengono le operazioni di lettura e scrittura.&lt;/p&gt;

&lt;p&gt;In particolare √® stata creata una struttura dati che contenga tutti i
valori che dovranno essere trasferiti dall‚Äôapplicazione Phantom Friction
a Pure Data:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;typedef struct
{
    double nForce;
    double velocita;
    float kappa;
    float depth;
    float statFr;
    float dynFr;
    int noiseFile;
}  OpeData;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I valori che costituiscono la struttura &lt;code class=&quot;highlighter-rouge&quot;&gt;OpeData&lt;/code&gt; sono i seguenti:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;nForce&lt;/code&gt; √® la radice quadrata del prodotto tra forza normale e
velocit√†;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;velocita&lt;/code&gt; indica la velocit√† corrente del proxy in $m/s$;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;kappa&lt;/code&gt; √® il coefficiente di elasticit√† della superficie;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;depth&lt;/code&gt; √® il fattore di amplificazione del segnale di rumore
frattale;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;statFr&lt;/code&gt; √® il coefficiente di frizione statica;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;dynFr&lt;/code&gt; √® il coefficiente di frizione dinamica;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;noiseFile&lt;/code&gt; √® un numero intero che indica l‚Äôindice del file
contenente il rumore frattale.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Nell‚Äôimplementazione attuale solo i valori &lt;code class=&quot;highlighter-rouge&quot;&gt;nForce&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;velocita&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;depth&lt;/code&gt;
e &lt;code class=&quot;highlighter-rouge&quot;&gt;noiseFile&lt;/code&gt; vengono effettivamente utilizzati. Infatti la patch in PD
non ha bisogno di conoscere i coefficienti di frizione statica e
dinamica, mentre il coefficiente di elasticit√† resta invariato per
scelte implementative. E‚Äô possibile modificare i valori che fanno parte
della struttura dati tramite la funzione:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;void setOpeData(OpeData *data, double nForce, 
                                double velocita, float kappa, 
                                float depth, float statFr, 
                                float dynFr, int noiseFile)
{
    data-&amp;gt;nForce=nForce;
    data-&amp;gt;velocita=velocita;
    data-&amp;gt;kappa=kappa;
    data-&amp;gt;depth=depth;
    data-&amp;gt;statFr=statFr;
    data-&amp;gt;dynFr=dynFr;
    data-&amp;gt;noiseFile=noiseFile;
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;La gestione della memoria √® implementata nel file &lt;code class=&quot;highlighter-rouge&quot;&gt;PhantomMemory.h&lt;/code&gt;.
Tramite la seguente funzione avviene la creazione dell‚Äôarea di memoria
condivisa tra i processi:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;HANDLE hFile;

int CreatePhantomMemory()
{
    hFile = CreateFileMappingW(INVALID_HANDLE_VALUE,NULL,PAGE_READWRITE,
                                                                        0,sizeMem,(LPCWSTR)&quot;PhantomMemory&quot;);
    if (hFile == NULL)
    {
        printf(&quot;ERROR: Unable to create a fileMapping.\n&quot;);
        return 1;
    }
    
    hView = MapViewOfFile(hFile,FILE_MAP_ALL_ACCESS,0,0,0);
    
    if (hView == NULL)
    {
        printf(&quot;ERROR: Unable to map a viewOfFile.\n&quot;);
        return 2;
    }
    return 0;
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;E‚Äô importante fare attenzione al parametro &lt;code class=&quot;highlighter-rouge&quot;&gt;sizeMem&lt;/code&gt;, il quale indica la
dimensione dell‚Äôarea di memoria condivisa e viene utilizzato anche per
calcolare la posizione dei diversi dati in memoria; un valore errato di
&lt;code class=&quot;highlighter-rouge&quot;&gt;sizeMem&lt;/code&gt; porta alla scrittura e lettura di dati errati.&lt;/p&gt;

&lt;p&gt;Il codice seguente si occupa dell‚Äôapertura e chiusura dell‚Äôarea di
memoria:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;int OpenPhantomMemory(void)
{
    hFile = OpenFileMappingW(FILE_MAP_ALL_ACCESS,FALSE,
                            (LPCWSTR)&quot;PhantomMemory&quot;);
    if (hFile == NULL)
    {
        printf(&quot;ERROR: Unable to open the FileMapping.\n&quot;);
        return 3;
    } 
    
    hView = MapViewOfFile(hFile,FILE_MAP_ALL_ACCESS,0,0,0);
    
    if (hView == NULL)
    {
        printf(&quot;ERROR: Unable to open the FileMapping.\n&quot;);
        return 4;
    }
    hookOped = (OpeData*)(hView);
    hookSurface = (float*)(hookOped+1);
    return 0;
}

int ClosePhantomMemory(void)
{
    if (!UnmapViewOfFile(hView))
    {
        printf(&quot;ERROR: Could not unmap viewOfFile.\n&quot;);
        return 5;
    }
    CloseHandle(hFile);
    return 0;
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Un oggetto &lt;code class=&quot;highlighter-rouge&quot;&gt;OpeData&lt;/code&gt; pu√≤ essere scritto in memoria e letto dalla memoria
usando le due funzioni &lt;code class=&quot;highlighter-rouge&quot;&gt;WriteOpeData(OpeData *elemento)&lt;/code&gt; e
&lt;code class=&quot;highlighter-rouge&quot;&gt;ReadOpeData()&lt;/code&gt; (nella prima deve essere passato come parametro un
puntatore all‚Äôoggetto):&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;void WriteOpeData(OpeData *elemento)
{
    hookOped-&amp;gt;nForce = elemento-&amp;gt;nForce;
    hookOped-&amp;gt;velocita = elemento-&amp;gt;velocita;
    hookOped-&amp;gt;kappa = elemento-&amp;gt;kappa;
    hookOped-&amp;gt;depth = elemento-&amp;gt;depth;
    hookOped-&amp;gt;statFr = elemento-&amp;gt;statFr;
    hookOped-&amp;gt;dynFr = elemento-&amp;gt;dynFr;
    hookOped-&amp;gt;noiseFile = elemento-&amp;gt;noiseFile;
}

OpeData ReadOpeData()
{
    return *hookOped;
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Con semplici modifiche a questi due comandi si implementano la scrittura
e lettura del valore della superficie generata dall‚Äôoggetto
&lt;code class=&quot;highlighter-rouge&quot;&gt;circ_max_exp~&lt;/code&gt;:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;void WriteSurface(float *elemento)
{
    *hookSurface = *elemento;
}

float ReadSurface()
{
    return *hookSurface;
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;la-scrittura-e-la-lettura-della-memoria-da-parte-dellapplicazione-phantom-friction&quot;&gt;La scrittura e la lettura della memoria da parte dell‚Äôapplicazione Phantom Friction&lt;/h3&gt;

&lt;p&gt;E‚Äô l‚Äôapplicazione Phantom Friction che si occupa di creare l‚Äôarea di
memoria condivisa, e lo fa non appena viene avviata; subito dopo la
memoria viene aperta e si crea un oggetto &lt;code class=&quot;highlighter-rouge&quot;&gt;data&lt;/code&gt; di tipo &lt;code class=&quot;highlighter-rouge&quot;&gt;OpeData&lt;/code&gt;
contenente tutti i valori impostati a &lt;script type=&quot;math/tex&quot;&gt;0&lt;/script&gt;, il quale viene scritto
assieme ad un valore nullo di &lt;code class=&quot;highlighter-rouge&quot;&gt;pdSurface&lt;/code&gt; allo scopo di effettuare
un‚Äôinizializzazione della memoria stessa, cancellando eventuali dati
presenti. All‚Äôinterno della callback asincrona &lt;code class=&quot;highlighter-rouge&quot;&gt;velocitaCallback&lt;/code&gt;, dopo
aver calcolato i valori necessari alla impostazione corretta
dell‚Äôoggetto &lt;code class=&quot;highlighter-rouge&quot;&gt;data&lt;/code&gt;, avvengono in sequenza le seguenti operazioni:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;viene scritto in memoria l‚Äôoggetto &lt;code class=&quot;highlighter-rouge&quot;&gt;data&lt;/code&gt;;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;viene letto dalla memoria il valore &lt;code class=&quot;highlighter-rouge&quot;&gt;pdSurface&lt;/code&gt;;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;vengono calcolati i coefficienti di frizione statica e dinamica
correnti.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Si √® scelto di non fare un‚Äôapertura e una chiusura della memoria per
ogni ciclo di queste operazioni, in primo luogo per evitare conflitti
con le operazioni di lettura e scrittura svolte dalla patch in Pure
Data; in secondo luogo ci√≤ comporterebbe un overhead considerevole nel
carico computazionale; basti pensare che, su un PC dotato di processore
Intel¬† Core¬† Duo T2400 con 2GB di RAM, un milione di cicli
lettura/scrittura con apertura/chiusura della memoria richiede tra i 12
e i 13 secondi, mentre eliminando l‚Äôapertura/chiusura della memoria si
scende a tempi dell‚Äôordine di pochi millisecondi.&lt;/p&gt;

&lt;h3 id=&quot;la-scrittura-e-la-lettura-della-memoria-da-parte-delle-patch-in-pure-data&quot;&gt;La scrittura e la lettura della memoria da parte delle patch in Pure Data&lt;/h3&gt;

&lt;p&gt;Per consentire alle patch in Pure Data di leggere e scrivere dati in RAM
√® stato necessario scrivere un external; per semplicit√† ci si √®
appoggiati alla libreria &lt;em&gt;flext&lt;/em&gt; discussa nel &lt;a href=&quot;/pure-data-modelli-audio.html&quot;&gt;post sui modelli audio&lt;/a&gt;. 
Tale oggetto √® stato chiamato &lt;code class=&quot;highlighter-rouge&quot;&gt;ReadOperativo&lt;/code&gt;; non necessita di argomenti di
costruzione e possiede 2 inlet e 5 outlet: i due inlet ricevono
rispettivamente un bang e il valore della superficie, mentre vengono
mandati in output, in ordine:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;la radice quadrata del prodotto tra la velocit√† e la forza normale;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;la velocit√† corrente del proxy;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;il coefficiente di elasticit√†;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;il fattore di amplificazione del rumore frattale;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;il nome del file contenente il rumore frattale.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;image&quot;&gt;
  &lt;img src=&quot;images/readoperativo.jpg&quot; alt=&quot;&quot; /&gt;
  &lt;figcaption&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Tutto ci√≤ viene inizializzato tramite il costruttore:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ReadOperativo::ReadOperativo()
{
    // aggiunta degli inlet
    AddInAnything(&quot;bang&quot;);//(0)
    AddInFloat(&quot;surface&quot;);//(1)

    // aggiunta degli outlet
    AddOutFloat(&quot;nForce&quot;);//(0)
    AddOutFloat(&quot;Velocita'&quot;);//(1)
    AddOutFloat(&quot;Kappa&quot;);//(2)
    AddOutFloat(&quot;Depth&quot;);//(3)
    AddOutSymbol(&quot;NoiseFile&quot;);//(4)

    // inizializzazione delle variabili
    lastResult = 0;
    lastDepth = 0;
    fileName = &quot;&quot;;
    open = false;

    // registrazione dei metodi
    FLEXT_ADDBANG(0,ope_bang);
    FLEXT_ADDMETHOD(1,writeSurface);
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Un oggetto &lt;code class=&quot;highlighter-rouge&quot;&gt;metro&lt;/code&gt; invia un bang all‚Äôoggetto &lt;code class=&quot;highlighter-rouge&quot;&gt;ReadOperativo&lt;/code&gt; ogni 0.5
millisecondi. Al primo bang ricevuto avviene l‚Äôapertura dell‚Äôarea di
memoria condivisa. Un ciclo di lettura e scrittura avviene ogni volta
che viene ricevuto un bang, quindi la frequenza con la quale avvengono
queste operazioni √® di 2 KHz. Ai primi due outlet i valori vengono
inviati aggiornati ad ogni ciclo, mentre i restanti vengono inviati solo
quando vengono modificati; ci√≤ √® in accordo con il fatto che i primi due
valori sono misure istantanee, mentre gli altri sono parametri che
riguardano uno scenario e restano invariati all‚Äôinterno dello stesso
scenario.&lt;/p&gt;

&lt;p&gt;Il valore della superficie non viene scritto direttamente in memoria, ma
viene prima filtrato attraverso un filtro passa‚Äìbasso a 2 KHz, in modo da evitare il verificarsi di
aliasing.&lt;/p&gt;

&lt;figure class=&quot;image&quot;&gt;
  &lt;img src=&quot;images/lop_surface.jpg&quot; alt=&quot;&quot; /&gt;
  &lt;figcaption&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Infine il distruttore si occupa della chiusura della memoria condivisa,
che pertanto viene effettivamente chiusa solo alla chiusura della patch
&lt;code class=&quot;highlighter-rouge&quot;&gt;sliding.pd&lt;/code&gt; (e solo se era stata aperta):&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;ReadOperativo::~ReadOperativo()
{
  if(open)
  ClosePhantomMemory();   
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h3 id=&quot;analisi-delle-prestazioni&quot;&gt;Analisi delle prestazioni&lt;/h3&gt;

&lt;p&gt;Grazie alle scelte implementative fatte, si √® giunti ad un‚Äôapplicativo
che consente una simulazione bimodale realistica. Le latenze sono
ridotte entro i limiti mediamente accettabili dall‚Äôuomo, quindi l‚Äôutente
percepisce gli stimoli visivo, aptico e sonoro come stimoli coerenti. Le
prove sono state effettuate con un PC notebook dotato di processore
Intel¬† Core¬† Duo T2400 e 2 GByte di memoria RAM; tuttavia la scheda
audio √® di fascia medio‚Äìbassa, e nonostante ci√≤ le prestazioni restano
soddisfacenti. Utilizzando un hardware audio di fascia alta sicuramente
i valori di latenza degli stimoli sonori verrebbero ridotti al minimo,
permettendo l‚Äôuso della simulazione anche su PC meno potenti.&lt;/p&gt;

&lt;h2 id=&quot;riferimenti&quot;&gt;Riferimenti&lt;/h2&gt;

&lt;ol class=&quot;bibliography&quot;&gt;&lt;li&gt;&lt;span id=&quot;art:lederman1&quot;&gt;Lederman, J. S., &amp;amp; Klatzky, R. L. (2004). Multisensory Texture Perception. In Calvert, G. A., Spence, C., &amp;amp; Stein, B. E. (Eds.), &lt;i&gt;The Handbook Of Multisensory Processes&lt;/i&gt; (pp. 107‚Äì122). MIT Press.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;book:openglsuperbible&quot;&gt;Wright, R. S. Jr., &amp;amp; Lipchak, B. (2004). &lt;i&gt;OpenGL¬Æ SuperBible, Third Edition&lt;/i&gt;. Sams Publishing.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;proc:levitin&quot;&gt;Levitin, D. J., Mathews, M. V., &amp;amp; MacLean, K. (1999). The perception of cross-modal simultaneity. In &lt;i&gt;Proc. of International Journal of Computing Anticipatory Systems&lt;/i&gt;.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;phd:crosato&quot;&gt;Crosato, P. (2006). Una piattaforma per il rendering audio‚Äìaptico di interazioni di contatto.&lt;/span&gt;&lt;/li&gt;&lt;/ol&gt;</content><author><name>Andrea Maglie</name></author><category term="haptic feedback" /><category term="pure data" /><summary type="html">Disclaimer: Il contenuto di questo post √® un estratto della mia tesi di laurea, realizzata nel 2006. Alcuni esempi potrebbero risultare obsoleti, tuttavia la componente teorica resta un buon riferimento per chi deve approcciarsi alla materia.</summary></entry><entry><title type="html">Il Phantom¬† Omni</title><link href="/phantom-omni.html" rel="alternate" type="text/html" title="Il Phantom¬† Omni" /><published>2019-02-12T00:00:00+01:00</published><updated>2019-02-12T00:00:00+01:00</updated><id>/phantom-omni</id><content type="html" xml:base="/phantom-omni.html">&lt;hr /&gt;

&lt;p&gt;&lt;i&gt;Disclaimer: Il contenuto di questo post √® un estratto della mia tesi di laurea, realizzata nel 2006. Alcuni esempi potrebbero risultare obsoleti, tuttavia la componente teorica resta un buon riferimento per chi deve approcciarsi alla materia.&lt;/i&gt;&lt;/p&gt;
&lt;hr /&gt;

&lt;h2 id=&quot;le-interfacce-phantom&quot;&gt;Le interfacce Phantom&lt;/h2&gt;

&lt;p&gt;Lo sviluppo del Personal Haptic Interface Mechanism (Phantom) ha avuto
inizio al MIT Artificial Intelligence Laboratory. L‚Äôinterazione da parte
dell‚Äôutente avveniva inserendo un dito in un apposito ditale (sistema
che poi √® stato sostituito dall‚Äôuso di uno stilo): il Phantom legge la
posizione e in base a questa esercita una forza corrispondente sul dito.&lt;/p&gt;

&lt;p&gt;Lo sviluppo del Phantom √® basato su tre importanti osservazioni:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;Le caratteristiche aptiche pi√π importanti sono la forza e il moto.&lt;/em&gt;
Le informazioni su come un oggetto si muove in risposta ad una forza
applicata e le forze che nascono dal tentativo di muoverlo possono
essere sufficienti per capire la geometria (forma, posizione), le
propriet√† (frizione, elasticit√†) degli oggetti e gli eventi in un
ambiente. Le interazioni aptiche, al contrario degli altri tipi di
interazioni, permettono uno scambio di dati bidirezionale.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;Molte interazioni aptiche significative non coinvolgono movimenti
di torsione.&lt;/em&gt; Tale osservazione ha portato all‚Äôuso del ditale, e
pertanto il dito dell‚Äôutente pu√≤ essere modellato come un punto o
una piccola sfera nell‚Äôambiente virtuale (lo stesso avviene se si
usa uno stilo). Tutto ci√≤ semplifica notevolmente sia la
programmazione che la progettazione.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;Uno spazio di lavoro piccolo e centrato sul polso √® sufficiente.&lt;/em&gt;
Molte interazioni aptiche avvengono all‚Äôinterno dello spazio che pu√≤
essere coperto dal movimento delle dita, mentre l‚Äôavambraccio compie
movimenti limitati. Dai risultati di alcuni esperimenti si √® deciso
di costruire il Phantom in modo tale che un utente possa muovere
liberamente il polso senza uscire dallo spazio di lavoro (come
avviene per i mouse pad e le tastiere).&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Inoltre il design e la progettazione sono state pensate nel rispetto di
tre regole fondamentali per garantire una riflessione delle forze
corretta ed efficiente:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;Il movimento libero deve essere avvertito come tale&lt;/em&gt; (il
dispositivo non deve esercitare forze esterne sull‚Äôutente).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;Gli oggetti virtuali solidi devono essere avvertiti come oggetti
rigidi&lt;/em&gt;. Il Phantom (nel modello Omni) √® in grado di riflettere una
rigidit√† massima di circa 35 N/cm, sufficienti per simulare una
resistenza da parte degli oggetti rigidi al tocco.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;I vincoli virtuali non possono essere violati&lt;/em&gt;. Ad esempio non pu√≤
accadere che, imprimendo una grande forza contro un muro virtuale,
l‚Äôutente sia in grado di attraversarlo.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Il Phantom¬† Omni¬† √® una delle interfacce aptiche pi√π economiche
attualmente disponibili sul mercato. Si tratta di un dispositivo aptico
ground‚Äìbased a sei gradi di libert√† in input, in grado di leggere la
posizione sui tre assi principali &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;y&lt;/script&gt; e &lt;script type=&quot;math/tex&quot;&gt;z&lt;/script&gt; e di computare le forze
lungo gli stessi. E‚Äô compatibile con tutti i PC intel‚Äìbased ed √® dotato
di una connessione FireWire¬† IEEE-1394a che assicura un trasferimento
dei dati ad alta velocit√†.&lt;/p&gt;

&lt;figure class=&quot;image&quot;&gt;
  &lt;img src=&quot;images/phantomomni.jpg&quot; alt=&quot;Il dispositivo aptico Phantom¬† Omni¬† di SensAble Tecnologies.&quot; /&gt;
  &lt;figcaption&gt;Il dispositivo aptico Phantom¬† Omni¬† di SensAble Tecnologies.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h2 id=&quot;il-toolkit-openhaptics&quot;&gt;Il toolkit OpenHaptics¬†&lt;/h2&gt;

&lt;p&gt;La programmazione del Phantom¬† Omni¬† avviene tramite il toolkit
&lt;em&gt;OpenHaptics&lt;/em&gt; di SensAble. Tale toolkit include:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;HDAPI (Haptic Device API): Costituiscono lo strato di livello pi√π basso per la programmazione
  aptica; permettono il rendering diretto delle forze e offrono un
  controllo sulla configurazione in runtime.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;HLAPI (Haptic Library API): Si appoggiano alle HDAPI per fornire un controllo di pi√π alto
  livello; risultano familiari a chi gi√† conosce l‚ÄôOpenGL e sono pi√π
  facili da usare in quanto il programmatore non deve preoccuparsi di
  eventi critici come il rendering di equazioni fisiche o la sicurezza
  dei thread.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Le HDAPI possono essere usate per richiedere le propriet√† del
dispositivo, come i gradi di libert√† in input e output, la forza
nominale massima, le dimensioni dello spazio di lavoro. Devono essere
utilizzate per inizializzare e configurare l‚ÄôHHD (haptic device handle)
e inoltre possono essere usate per modificare la frequenza del &lt;em&gt;servo
loop&lt;/em&gt;; il servo loop √® il ciclo di controllo usato per calcolare le
forze da inviare al dispositivo aptico. Per avere un feedback stabile,
il ciclo deve essere eseguito ad una frequenza pari a 1 KHz o superiore
(maggiore √® la frequenza, maggiore √® l‚Äôutilizzo di CPU), e pertanto
viene eseguito in un thread separato ad alta priorit√† (&lt;em&gt;servo thread&lt;/em&gt;).&lt;/p&gt;

&lt;p&gt;Le HLAPI permettono l‚Äôaggiunta di effetti personalizzati, ovvero
l‚Äôaggiunta di forze da inviare al dispositivo aptico. Dato che le forze
sono computate nel servo thread, possono essere usate in aggiunta alle
callback HLAPI per avere ulteriori informazioni sul dispositivo.&lt;/p&gt;

&lt;h2 id=&quot;creazione-di-un-ambiente-aptico&quot;&gt;Creazione di un ambiente aptico&lt;/h2&gt;

&lt;p&gt;Nei dispositivi aptici le forze sono usate per resistere o assistere il
movimento del dispositivo stesso. Le interazioni delle forze derivano
dal considerare la posizione del dispositivo in relazione agli oggetti
presenti nell‚Äôambiente virtuale: se la forza √® nulla il movimento del
dispositivo √® libero. Quando l‚Äôutente muove il dispositivo, viene
effettuato il rendering delle forze alla frequenza di 1000 Hz, impedendo
all‚Äô&lt;em&gt;end effector&lt;/em&gt; di penetrare la superficie degli oggetti; il modo in
cui vengono renderizzate varia a seconda dell‚Äôeffetto che si deve
ottenere sulle superfici (dure, soffici, elastiche, ruvide) o
sull‚Äôambiente (come viscosit√† e inerzia). Un altro effetto desiderato
potrebbe essere quello di costringere il movimento del dispositivo lungo
un determinato percorso.&lt;/p&gt;

&lt;p&gt;Il vettore delle forze √® l‚Äôunit√† di output per un dispositivo aptico; le
tre classi principali di forze sono:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Forze dipendenti dal moto.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Forze dipendenti dal tempo.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Forze dipendenti sia dalla posizione che dal tempo.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;forze-dipendenti-dal-moto&quot;&gt;Forze dipendenti dal moto&lt;/h3&gt;

&lt;p&gt;Una forza √® dipendente dal moto quando viene calcolata in base al
movimento del dispositivo aptico; tali forze sono:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Forza elastica: La forza elastica pu√≤ essere calcolata usando la legge di Hooke
  &lt;script type=&quot;math/tex&quot;&gt;F=kx ,&lt;/script&gt; dove &lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt; √® la costante elastica e &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; √® il vettore della
  posizione. La molla che rappresenta la forza √® attaccata, ad una
  estremit√†, ad un punto fisso &lt;script type=&quot;math/tex&quot;&gt;p_0&lt;/script&gt;, solitamente collocato sulla
  superficie dell‚Äôoggetto che l‚Äôutente sta toccando, mentre l‚Äôaltra
  estremit√† coincide con la posizione &lt;script type=&quot;math/tex&quot;&gt;p_1&lt;/script&gt; del dispositivo; il
  vettore &lt;script type=&quot;math/tex&quot;&gt;x=p_0-p_1&lt;/script&gt; √® orientato in modo tale che la forza elastica
  sia sempre diretta verso il punto fisso.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Frizione viscosa: L‚Äôutilit√† principale della frizione viscosa √® ridurre la vibrazione
  opponendosi al moto; in generale tale forza √® proporzionale alla
  velocit√† del dispositivo: &lt;script type=&quot;math/tex&quot;&gt;F=-bv&lt;/script&gt;, dove &lt;script type=&quot;math/tex&quot;&gt;b&lt;/script&gt; √® la costante di
  smorzamento e &lt;script type=&quot;math/tex&quot;&gt;v&lt;/script&gt; la velocit√† del dispositivo.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Frizione statica: E‚Äô una forza che si oppone alla direzione del moto con modulo
  costante: &lt;script type=&quot;math/tex&quot;&gt;F=-c\cdot sgn(v) ,&lt;/script&gt; dove &lt;script type=&quot;math/tex&quot;&gt;v&lt;/script&gt; √® la velocit√† del
  dispositivo e &lt;script type=&quot;math/tex&quot;&gt;c&lt;/script&gt; √® la costante di frizione, dipendente dalla forza
  normale. Pu√≤ essere utile per creare una transizione smorzata nei
  cambiamenti di direzione.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Frizione dinamica: E‚Äô simile alla frizione viscosa, e pertanto viene calcolata mediante
  la formula &lt;script type=&quot;math/tex&quot;&gt;F=-bv&lt;/script&gt;, dove &lt;script type=&quot;math/tex&quot;&gt;b&lt;/script&gt; dipende dalla forza normale.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Inerzia: E‚Äô associata alla massa in movimento e si calcola, una volta nota la
  traiettoria, con la legge di Newton &lt;script type=&quot;math/tex&quot;&gt;F=ma&lt;/script&gt;.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;forze-dipendenti-dal-tempo&quot;&gt;Forze dipendenti dal tempo&lt;/h3&gt;

&lt;p&gt;Le forze funzioni del tempo possono essere suddivise in:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Costanti: Si tratta di forze costanti sia in modulo che in direzione. Possono
  essere usate per far sentire il dispositivo pi√π pesante oppure, al
  contrario, pi√π leggero effettuando una compensazione della forza di
  gravit√†.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Periodiche: Derivano dall‚Äôapplicazione di un pattern (dente di sega, onda
  quadra, sinusoide) che si ripete nel tempo. Una forza periodica √®
  descritta da una direzione, da una costante di tempo che controlla
  il periodo del pattern e un‚Äôampiezza che determina quanto forte
  dovr√† essere la forza al suo picco massimo.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Impulsive: Sono costituite da un vettore delle forze che viene applicato
  istantaneamente; nella pratica, un impulso con un dispositivo aptico
  √® applicato su un piccolo intervallo di tempo.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;contatti-e-vincoli&quot;&gt;Contatti e vincoli&lt;/h2&gt;

&lt;p&gt;Simulare contatti con un oggetto virtuale significa computare le forze
che impediscono all‚Äô&lt;em&gt;end‚Äìeffector&lt;/em&gt; del dispositivo di penetrare la
superficie dell‚Äôoggetto virtuale. Ci√≤ pu√≤ essere simulato tramite il
concetto di &lt;em&gt;proxy&lt;/em&gt; che segue la trasformazione dell‚Äôend‚Äìeffector
nell‚Äôambiente virtuale.&lt;/p&gt;

&lt;p&gt;Il proxy √® identificato con un punto, una sfera o un insieme di punti;
se si tratta di un punto lo definiamo &lt;em&gt;SCP&lt;/em&gt; (&lt;em&gt;surface contact point&lt;/em&gt; o
punto di contatto di superficie.&lt;/p&gt;

&lt;figure class=&quot;image&quot;&gt;
  &lt;img src=&quot;images/scp.jpg&quot; alt=&quot;SCP - surface contact point.&quot; /&gt;
  &lt;figcaption&gt;SCP - surface contact point.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Quando l‚Äôend‚Äìeffector penetra la superficie, viene calcolata una
trasformazione del proxy che raggiunga la configurazione ad energia
minima tra la superficie di contatto e l‚Äôend‚Äìeffector. Successivamente
vengono determinate le forze che impediscono al moto del dispositivo di
penetrare ulteriormente la superficie, usando un controllo di
elasticit√†‚Äìsmorzamento; pi√π precisamente, il punto SCP √® un punto che
segue la posizione dell‚Äôend‚Äìeffector pur essendo costretto a restare
sulla superficie dell‚Äôoggetto. Nello spazio libero l‚ÄôSCP si trova nella
stessa posizione dell‚Äôend‚Äìeffector (&lt;script type=&quot;math/tex&quot;&gt;t_0&lt;/script&gt; in figura); al
momento del contatto con un oggetto l‚ÄôSCP pu√≤ essere calcolato muovendo
l‚Äôultimo SCP verso la posizione dell‚Äôend‚Äìeffector senza oltrepassare la
superficie. La forza viene calcolata simulando una molla collegata da un
lato all‚ÄôSCP e dall‚Äôaltro alla posizione dell‚Äôend‚Äìeffector: in figura &lt;script type=&quot;math/tex&quot;&gt;t_1&lt;/script&gt;
indica la penetrazione nell‚Äôoggetto e &lt;script type=&quot;math/tex&quot;&gt;t_2&lt;/script&gt; mostra una penetrazione
ulteriore (la molla √® maggiormente allungata facendo sentire una
maggiore resistenza all‚Äôutente).&lt;/p&gt;

&lt;h2 id=&quot;sincronizzazione&quot;&gt;Sincronizzazione&lt;/h2&gt;

&lt;p&gt;La sincronizzazione √® importante quando l‚Äôinterfaccia utente √® composta
sia di una parte grafica che di una parte aptica, in quanto i due cicli
di rendering devono accedere alle stesse informazioni; ci√≤ implica la
creazione di copie dei dati in memoria, resi cos√¨ disponibili in modo
sicuro ad entrambi i thread. Non pu√≤ essere usata la mutua esclusione in
primo luogo perch√© il ciclo di rendering aptico deve sempre girare ad
una frequenza di 1000 Hz e non pu√≤ quindi attendere altri processi, in
secondo luogo perch√© la diversa frequenza dei due cicli facilita la
presenza di inconsistenze se sono in movimento pi√π oggetti
contemporaneamente.&lt;/p&gt;

&lt;p&gt;Anche nella gestione degli eventi il ciclo di rendering aptico deve
avere la maggiore priorit√†. Quando si verifica un evento (come il tocco
di una superficie o l‚Äôapplicazione di un particolare vincolo), questo
viene prima gestito dal thread aptico, in modo da fornire una risposta
al dispositivo immediata, e poi accodato dal thread grafico che si
occuper√† di aggiornare la visualizzazione sullo schermo al successivo
frame.&lt;/p&gt;

&lt;h2 id=&quot;convenzioni-nellinterfaccia-grafica-e-aptica&quot;&gt;Convenzioni nell‚Äôinterfaccia grafica e aptica&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Pozzo di gravit√†: Il pozzo di gravit√† viene usato per attirare il dispositivo verso un
  determinato punto, solitamente indicato come &lt;em&gt;vincolo istantaneo&lt;/em&gt;.
  Al pozzo √® associato un raggio di influenza: quando il dispositivo
  si trova all‚Äôinterno di tale raggio, viene applicata una forza che
  lo attira verso il centro del pozzo (solitamente viene usata una
  forza elastica).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Trasformazioni relative: I dispositivi aptici hanno una posizione assoluta, dal momento che
  si trovano ancorati al tavolo di lavoro. L‚Äôunico modo per ottenere
  una manipolazione relativa √® applicare delle trasformazioni
  addizionali alle coordinate del dispositivo, cos√¨ da dare
  l‚Äôimpressione che quest‚Äôultimo si stia muovendo relativamente ad una
  data posizione e orientazione.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Accoppiamento delle informazione aptiche e visive: Il senso aptico del contatto pu√≤ essere migliorato fornendo una
  rappresentazione visuale del contatto stesso; ci√≤ si ottiene
  semplicemente dando una corretta visuale. Ad esempio, la sensazione
  di contatto sar√† pi√π verosimile se il cursore non penetra mai la
  superficie (rappresentando quindi il proxy e non la posizione del
  dispositivo).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Stabilizzazione della manipolazione con la frizione: Applicare una piccola frizione aiuta l‚Äôutente a stabilizzare la mano
  mentre cerca di arrivare alla posizione desiderata; senza frizione,
  il dispositivo √® troppo ‚Äúlibero‚Äù, rendendo difficile effettuare
  posizionamenti precisi.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;programmare-con-le-hdapi&quot;&gt;Programmare con le HDAPI&lt;/h2&gt;

&lt;p&gt;Le Haptic Device API consistono di due componenti: le API relative al
dispositivo e quelle relative allo scheduler. Le prime si occupano
dell‚Äôastrazione di ogni meccanismo tridimensionale supportato. Le API
dello scheduler invece permettono di introdurre dei comandi che verranno
eseguiti all‚Äôinterno del servo loop thread. Il tipico uso delle HDAPI
prevede l‚Äôinizializzazione del dispositivo e dello scheduler, l‚Äôavvio di
quest‚Äôultimo, l‚Äôesecuzione di alcuni comandi tramite lo scheduler stesso
e l‚Äôuscita. Ad esempio, consideriamo la creazione di un piano che
respinga il dispositivo quando questo cerca di penetrarlo; la creazione
di tale ambiente si divide in 5 passi:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;Inizializzazione del dispositivo.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Creazione delle callback dello scheduler che chiedono la posizione
del dispositivo e comandano la forza che deve respingere il
dispositivo al momento della penetrazione.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Abilitazione delle forze del dispositivo.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Avvio dello scheduler.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Reset del dispositivo e dello scheduler quando l‚Äôapplicazione viene
terminata.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Le routine relative al dispositivo possono essere raggruppate come
segue:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Inizializzazione del dispositivo (creazione dell‚Äôhandle del
dispositivo, abilitazione delle forze, calibrazione).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Sicurezza del dispositivo (controllo della sicurezza del force
feedback, come temperatura dei motori, forze e velocit√† eccessive).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Stato del dispositivo (richiesta di posizione, velocit√†, pulsanti e
matrici di trasformazione).&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;image&quot;&gt;
  &lt;img src=&quot;images/hdapi.jpg&quot; alt=&quot;Schema di programmazione con le HDAPI.&quot; /&gt;
  &lt;figcaption&gt;Schema di programmazione con le HDAPI.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h3 id=&quot;operazioni-del-dispositivo&quot;&gt;Operazioni del dispositivo&lt;/h3&gt;

&lt;p&gt;Le operazioni di dispositivo sono tutte quelle inerenti la richiesta e
l‚Äôimpostazione dello stato corrente; sono tutte operazioni che
dovrebbero essere eseguite esclusivamente all‚Äôinterno del servo loop
utilizzando le callback dello scheduler.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Inizializzazione: Sia il dispositivo che lo scheduler devono essere inizializzati
  prima dell‚Äôuso; al momento dell‚Äôinizializzazione le forze sono
  disattivate, verranno invece attivate nel momento in cui verr√† fatto
  partire lo scheduler. Se vengono utilizzati pi√π dispositivi, ognuno
  deve essere inizializzato separatamente, mentre viene avviato un
  solo scheduler.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Dispositivo corrente: Nel caso in cui vengano utilizzati pi√π dispositivi, uno deve essere impostato come il dispositivo corrente; per fare ci√≤ viene usato il comando &lt;code class=&quot;highlighter-rouge&quot;&gt;mdMakeCurrentDevice(hHD)&lt;/code&gt;. Nell‚Äôutilizzo di un singolo dispositivo tale comando non deve mai essere utilizzato.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Caratteristiche del dispositivo: Alcune caratteristiche del dispositivo possono essere abilitate o
  disabilitate a seconda delle necessit√†, usando i comandi &lt;code class=&quot;highlighter-rouge&quot;&gt;hdEnable&lt;/code&gt;
  e &lt;code class=&quot;highlighter-rouge&quot;&gt;hdDisable&lt;/code&gt;. Tali istruzioni devono essere utilizzate con
  attenzione e sempre tramite callback dello scheduler.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;frame-aptici&quot;&gt;Frame aptici&lt;/h3&gt;

&lt;p&gt;I frame aptici definiscono i limiti all‚Äôinterno dei quali lo stato del
dispositivo √® consistente. All‚Äôavvio del frame, lo stato del dispositivo
viene aggiornato e memorizzato per l‚Äôuso in quel frame; successivamente
tutte le operazioni dovrebbero essere eseguite all‚Äôinterno dello stesso
frame, dato che cercando di ottenere lo stato del dispositivo al di
fuori del frame si pu√≤ ottenere invece lo stato del frame precedente. Ad
ogni istante lo scheduler dovrebbe avere un solo frame attivo per ogni
dispositivo; tuttavia i frame per dispositivi diversi possono essere
annidati.&lt;/p&gt;

&lt;h3 id=&quot;operazioni-dello-scheduler&quot;&gt;Operazioni dello scheduler&lt;/h3&gt;

&lt;p&gt;Come gi√† detto, lo scheduler si occupa della gestione del servo loop
thread e opera ad una frequenza di circa 1000 Hz; diventa perci√≤
pericoloso accedere manualmente alle variabili: per fare ci√≤ conviene
sempre utilizzare le callback. Le chiamate allo scheduler sono di due
tipi:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Chiamate sincrone: Ritornano solamente quando sono complete, e il thread deve attendere
  il completamento prima di continuare. Sono usate principalmente per
  richiedere lo stato del sistema.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Chiamate asincrone: Ritornano non appena sono state schedulate. Sono usate per
  rappresentare un effetto aptico e perci√≤ risiedono nello scheduler,
  applicando l‚Äôeffetto ad ogni iterazione. Al momento della
  schedulazione di una callback asincrona viene restituito un handle
  che pu√≤ essere riutilizzato successivamente per eseguire operazioni
  sulla callback, come l‚Äôeliminazione della stessa dallo scheduler o
  il suo blocco fino al completamento.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Tutte le callback hanno associata una priorit√†, in base alla quale viene
determinato l‚Äôordine di esecuzione all‚Äôinterno dello scheduler; per ogni
ciclo di scheduler viene eseguita sempre ogni callback. In ogni momento
viene eseguito un solo thread di schedulazione e, se sono presenti pi√π
dispositivi, questi condividono lo stesso thread.&lt;/p&gt;

&lt;h3 id=&quot;stato-del-sistema&quot;&gt;Stato del sistema&lt;/h3&gt;

&lt;h4 id=&quot;ottenimento-dello-stato&quot;&gt;Ottenimento dello stato&lt;/h4&gt;

&lt;p&gt;Lo stato del dispositivo (assieme ad altre informazioni) pu√≤ essere
richiesto tramite l‚Äôuso delle funzioni della famiglia &lt;code class=&quot;highlighter-rouge&quot;&gt;hdGet&lt;/code&gt;, come ad
esempio &lt;code class=&quot;highlighter-rouge&quot;&gt;hdGetDoublev&lt;/code&gt;. Tali funzioni richiedono un parametro valido ed
un singolo indirizzo di ritorno o un array. Le richieste possono essere
fatte al frame corrente o al precedente; in generale, se vengono
eseguite all‚Äôesterno di un frame, vengono riferite a quello precedente.
Per i parametri di forze in output il valore viene impostato a zero
automaticamente all‚Äôinizio di ogni frame.&lt;/p&gt;

&lt;h4 id=&quot;impostazione-dello-stato&quot;&gt;Impostazione dello stato&lt;/h4&gt;

&lt;p&gt;L‚Äôimpostazione dello stato deve essere sempre eseguita all‚Äôinterno di
uno stesso frame, ed √® necessario passare il numero corretto di
parametri; questo per evitare l‚Äôintroduzione di errori dato che si va a
modificare le caratteristiche o il comportamento del dispositivo. Le
forze non vengono inviate al dispositivo fino alla fine del frame,
quindi se viene impostato due volte lo stesso stato, la seconda
impostazione sostituisce la prima (se ad esempio si vogliono sommare pi√π
forze, sar√† necessario farlo in una variabile separata).&lt;/p&gt;

&lt;h4 id=&quot;sincronizzazione-dello-stato&quot;&gt;Sincronizzazione dello stato&lt;/h4&gt;

&lt;p&gt;Lo scheduler fornisce una sincronizzazione dello stato tra thread
diversi. Un esempio √® costituito dal caso in cui uno stato deve essere
aggiornato alla frequenza del servo loop, e contemporaneamente un altro
thread (come il thread grafico) accede e modifica lo stato. E‚Äô possibile
ottenere lo stato attuale da un singolo frame all‚Äôinterno del ciclo
aptico usando le callback sincrone.&lt;/p&gt;

&lt;h2 id=&quot;programmare-con-le-hlapi&quot;&gt;Programmare con le HLAPI&lt;/h2&gt;

&lt;p&gt;Le HLAPI sono delle API in C di alto livello per il rendering aptico e
si accompagnano alle API OpenGL per il rendering grafico. Le HLAPI
permettono al programmatore di specificare primitive geometriche come
triangoli, linee e punti assieme a propriet√† aptiche come frizione e
rigidit√†; tramite tali informazioni il motore di rendering aptico
calcola poi le forze appropriate. Inoltre queste API permettono sia di
impostare che di richiedere lo stato degli oggetti, richiedere lo stato
del Phantom (posizione e orientamento) e impostare le funzioni di
callback.&lt;/p&gt;

&lt;h3 id=&quot;generazione-delle-forze&quot;&gt;Generazione delle forze&lt;/h3&gt;

&lt;p&gt;Esistono tre modi per generare un feedback aptico usando le HLAPI:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Rendering delle forme: Permette di specificare primitive geometriche (tramite istruzioni
  OpenGL) che il motore di rendering usa per computare le giuste forze
  di reazione per simulare il tocco della superficie.
  L‚Äôidentificazione delle geometrie create in OpenGL pu√≤ avvenire in
  due modi: tramite il &lt;em&gt;depth buffer&lt;/em&gt; oppure tramite il &lt;em&gt;feedback
  buffer&lt;/em&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Rendering degli effetti: Permettono di specificare forze globali non definibili tramite
  primitive geometriche (cio√® non legate al tocco di una figura
  geometrica).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Rendering diretto del proxy: Permettono di impostare un orientamento per il dispositivo aptico il
  quale verr√† portato nella giusta posizione dal motore di rendering.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;threading&quot;&gt;Threading&lt;/h3&gt;

&lt;p&gt;Dato che il rendering aptico necessita di un aggiornamento molto pi√π
frequente rispetto all‚Äôapplicazione grafica, il motore HLAPI crea due
thread in aggiunta a quello dell‚Äôapplicazione principale: il &lt;em&gt;servo
thread&lt;/em&gt; e il &lt;em&gt;collision thread&lt;/em&gt;.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Servo thread: Gestisce la comunicazione diretta con il dispositivo aptico,
  leggendo la posizione e l‚Äôorientamento del dispositivo e aggiornando
  le forze ad una frequenza molo alta (generalmente 1000 Hz). La
  differenza rispetto alle HDAPI √® che le HLAPI nascondono il servo
  thread all‚Äôutente.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Collision thread: Determina quali primitive geometriche sono in contatto con il proxy
  ad una frequenza di 100 Hz (minore rispetto al servo thread ma
  maggiore del client thread). Una volta determinato quale oggetto √®
  in contatto con il proxy, viene elaborata un‚Äôapprossimazione della
  forma locale dell‚Äôoggetto, la quale viene inviata al servo thread
  che la utilizza nel calcolo delle forze.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;struttura-della-programmazione-con-le-hlapi&quot;&gt;Struttura della programmazione con le HLAPI&lt;/h3&gt;

&lt;figure class=&quot;image&quot;&gt;
  &lt;img src=&quot;images/hlapi.jpg&quot; alt=&quot;Schema di programmazione con le HLAPI.&quot; /&gt;
  &lt;figcaption&gt;Schema di programmazione con le HLAPI.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;In figura si pu√≤
vedere la struttura tipica di un programma che implementa le HLAPI. Il
primo passo √® l‚Äôinizializzazione dell‚Äôambiente OpenGL con la creazione
di un contesto grafico e la rispettiva finestra; segue
l‚Äôinizializzazione delle HLAPI con la creazione del contesto aptico.
Successivamente viene specificato come le coordinate fisiche (quelle del
Phantom) devono essere mappate nello spazio delle coordinate usato
dall‚Äôambiente grafico. A questo punto √® possibile effettuare il
rendering grafico e inizia la cattura degli eventi aptici, procedendo
cos√¨ con il rendering aptico; in aggiunta viene rappresentato un cursore
tridimensionale in corrispondenza della posizione del proxy. Il ciclo
prosegue tornando alla cattura degli eventi.&lt;/p&gt;

&lt;h3 id=&quot;setup-del-dispositivo&quot;&gt;Setup del dispositivo&lt;/h3&gt;

&lt;p&gt;Come con le HDAPI, l‚Äôinizializzazione del dispositivo avviene tramite il
comando &lt;code class=&quot;highlighter-rouge&quot;&gt;hdInitDevice&lt;/code&gt;, seguito dal comando &lt;code class=&quot;highlighter-rouge&quot;&gt;hlCreateContext&lt;/code&gt;, il quale
crea il contesto aptico. Infine il contesto viene impostato come
corrente con l‚Äôistruzione &lt;code class=&quot;highlighter-rouge&quot;&gt;hlMakeCurrent&lt;/code&gt; (ci√≤ √® richiesto da tutti i
comandi delle HLAPI). Il rendering del contesto deve essere attivo per
un solo thread alla volta, in quanto le routine HLAPI, come quelle
OpenGL, non sono &lt;em&gt;thread safe&lt;/em&gt;; ci√≤ pu√≤ essere implementato con l‚Äôuso di
un &lt;em&gt;mutex&lt;/em&gt; per sincronizzare le chiamate a &lt;code class=&quot;highlighter-rouge&quot;&gt;hlMakeCurrent&lt;/code&gt; per i
contesti condivisi.&lt;/p&gt;

&lt;h3 id=&quot;frame-aptici-1&quot;&gt;Frame aptici&lt;/h3&gt;

&lt;figure class=&quot;image&quot;&gt;
  &lt;img src=&quot;images/hlapiframe.jpg&quot; alt=&quot;Collocazione dei frame con le HLAPI.&quot; /&gt;
  &lt;figcaption&gt;Collocazione dei frame con le HLAPI.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Tutti i comandi implementati dalle HLAPI devono essere inseriti
all‚Äôinterno di un &lt;em&gt;frame aptico&lt;/em&gt;, delimitato dalle chiamate a
&lt;code class=&quot;highlighter-rouge&quot;&gt;hlBeginFrame&lt;/code&gt; (posta all‚Äôinizio del ciclo di rendering) e &lt;code class=&quot;highlighter-rouge&quot;&gt;hlEndFrame&lt;/code&gt;
(alla fine del ciclo di rendering), come evidenziato in
&lt;a href=&quot;#fig:hlapiframe&quot;&gt;1.4&lt;/a&gt;{reference-type=‚Äùref‚Äù reference=‚Äùfig:hlapiframe‚Äù}.
In generale sar√† presente un frame aptico per ogni frame grafico. Ci√≤ √®
molto diverso rispetto alla programmazione con le HDAPI, perch√© in
questo caso il frame grafico viene aggiornato alla stessa frequenza di
quello aptico: le chiamate avvengono nel thread dato che entrambi
accedono alle medesime informazioni geometriche. &lt;code class=&quot;highlighter-rouge&quot;&gt;hlBeginFrame&lt;/code&gt; campiona
lo stato corrente del rendering aptico dal thread aptico;
successivamente &lt;code class=&quot;highlighter-rouge&quot;&gt;hlEndFrame&lt;/code&gt; aggiorna la posizione del proxy in base ai
cambiamenti nella dinamica degli oggetti. In aggiunta &lt;code class=&quot;highlighter-rouge&quot;&gt;hlBeginFrame&lt;/code&gt; si
occupa di aggiornare le coordinate globali usate dal motore di rendering
aptico. Tutte le richieste dello stato del dispositivo o del proxy
effettuate all‚Äôinterno di uno stesso frame riportano lo stesso
risultato, ovvero lo stato presente al momento in cui √® stato richiamato
&lt;code class=&quot;highlighter-rouge&quot;&gt;hlBeginFrame&lt;/code&gt;. Alla fine del frame, tutti i cambiamenti allo stato che
sono intervenuti vengono trasmessi al rendering aptico. Ci√≤ fa si che
pi√π cambiamenti allo scenario all‚Äôinterno di uno stesso frame vengano
trasmessi ai cicli aptico e grafico simultaneamente alla fine del frame.&lt;/p&gt;

&lt;h3 id=&quot;sec:rendering_forme&quot;&gt;Rendering delle forme&lt;/h3&gt;

&lt;p&gt;Il rendering delle forme con le HLAPI √® usato per rappresentare
superfici e oggetti solidi. Una forma pu√≤ essere creata combinando
insieme pi√π primitive grafiche come linee e poligoni.&lt;/p&gt;

&lt;h4 id=&quot;inizio-e-fine-di-una-forma&quot;&gt;Inizio e fine di una forma&lt;/h4&gt;

&lt;p&gt;Le forme geometriche vengono specificate attraverso istruzioni OpenGL
racchiuse tra le chiamate a &lt;code class=&quot;highlighter-rouge&quot;&gt;hlBeginShape&lt;/code&gt; e &lt;code class=&quot;highlighter-rouge&quot;&gt;hlEndShape&lt;/code&gt;. Le HLAPI
catturano tali geometrie per calcolare il rendering aptico.&lt;/p&gt;

&lt;h4 id=&quot;identificatori-delle-forme&quot;&gt;Identificatori delle forme&lt;/h4&gt;

&lt;p&gt;Ogni forma deve essere identificata univocamente da un intero che sar√†
usato dal motore di rendering per i cambiamenti della forma da frame a
frame in modo da calcolare correttamente le forze. Tale identificatore
pu√≤ essere rilasciato quando la forma non √® pi√π usata.&lt;/p&gt;

&lt;h4 id=&quot;depth-buffer&quot;&gt;Depth buffer&lt;/h4&gt;

&lt;p&gt;La geometria degli oggetti pu√≤ essere catturata dal &lt;em&gt;depth buffer&lt;/em&gt; usato
dalle OpenGL: quando viene richiamata l‚Äôistruzione &lt;code class=&quot;highlighter-rouge&quot;&gt;hlEndShape&lt;/code&gt; le API
leggono un‚Äôimmagine da tale buffer, immagine che viene poi passata al
collision thread e usata per calcolare le collisioni con il proxy. Ogni
modifica al depth buffer verr√† riconosciuta come modifica di un oggetto
e quindi ne verr√† fatto il rendering aptico. Dato che gli oggetti
vengono convertiti in un immagine, √® fondamentale utilizzare il punto di
vista corretto durante il rendering, in quanto non √® possibile sentire
quelle parti della scena che non sono visibili da tale punto. Esiste la
possibilit√† di abilitare l‚Äôottimizzazione del punto di vista, con la
quale le HLAPI correggono automaticamente i parametri di visualizzazione
OpenGL in base alla mappatura del dispositivo aptico sulla scena; il
comando corrispondente √®&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;hlEnable(HL_HAPTIC_CAMERA_VIEW)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Grazie al &lt;em&gt;depth buffer&lt;/em&gt; √® possibile effettuare il rendering aptico e
grafico delle forme in un unico ciclo, semplicemente includendo il
codice di rendering grafico all‚Äôinterno di un blocco &lt;code class=&quot;highlighter-rouge&quot;&gt;hlBeginShape&lt;/code&gt; -
&lt;code class=&quot;highlighter-rouge&quot;&gt;hlEndShape&lt;/code&gt;. Se √® attiva l‚Äôottimizzazione &lt;em&gt;haptic camera view&lt;/em&gt;, tale
metodo non funzioner√† in quanto il rendering grafico verr√† effettuato
dal punto di vista modificato.&lt;/p&gt;

&lt;h4 id=&quot;feedback-buffer&quot;&gt;Feedback buffer&lt;/h4&gt;

&lt;p&gt;Il rendering tramite &lt;em&gt;feedback buffer&lt;/em&gt; usa il feedback buffer delle
OpenGL per catturare le primitive geometriche: quando l‚Äôistruzione
&lt;code class=&quot;highlighter-rouge&quot;&gt;hlBeginShape&lt;/code&gt; viene richiamata, le HLAPI automaticamente allocano tale
buffer e impostano la modalit√† di rendering OpenGL a feedback buffer;
tutte le primitive vengono cos√¨ salvate, ma solo i comandi che generano
punti, linee o vertici vengono catturati dalla routine aptica (gli altri
comandi, come quelli relativi all‚Äôimpostazione delle texture, vengono
ignorati). Con il comando &lt;code class=&quot;highlighter-rouge&quot;&gt;hlEndShape&lt;/code&gt; le primitive salvate nel feedback
buffer vengono usate per il rendering aptico. E‚Äô opportuno utilizzare
l‚Äôistruzione &lt;code class=&quot;highlighter-rouge&quot;&gt;hlHinti&lt;/code&gt; per impostare il numero di vertici che saranno
presenti nella scena, numero che verr√† usato dalle HLAPI per allocare la
memoria per il feedback buffer; ad esempio il comando seguente alloca
memoria per 4 vertici:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;hlHinti(HL_SHAPE_FEEDBACK_BUFFER_VERTICES, 4);
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;il valore di default √® 65536.&lt;/p&gt;

&lt;p&gt;Durante la creazione di un oggetto con il feedback buffer √® importante
non richiamare l‚Äôistruzione &lt;code class=&quot;highlighter-rouge&quot;&gt;glCullFace&lt;/code&gt; (o la relativa
abilitazione/disabilitazione) all‚Äôinterno della coppia di istruzioni
&lt;code class=&quot;highlighter-rouge&quot;&gt;hlBeginShape&lt;/code&gt; - &lt;code class=&quot;highlighter-rouge&quot;&gt;hlEndShape&lt;/code&gt;, altrimenti pu√≤ accadere di non essere in
grado di percepire alcune parti dell‚Äôoggetto.&lt;/p&gt;

&lt;h4 id=&quot;ottimizzazione-del-rendering&quot;&gt;Ottimizzazione del rendering&lt;/h4&gt;

&lt;p&gt;Come nella grafica √® possibile ottimizzare il rendering effettuandolo
solo sulle forme che vengono visualizzate al momento, √® possibile
ottimizzare il rendering aptico effettuandolo solo sulle forme che
possono essere toccate (ad esempio considerando solo le parti degli
oggetti vicini alla posizione corrente del proxy). Di seguito √®
riportato un elenco delle possibili ottimizzazioni che possono essere
effettuate.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Adaptive Viewport: Tale ottimizzazione consiste nel limitare la regione del depth
  buffer che viene letta nella memoria per il rendering aptico
  all‚Äôarea prossima alla posizione corrente del proxy. L‚Äôincremento di
  prestazioni dipende dalla velocit√† con la quale la scheda grafica
  riesce a leggere il depth buffer dalla memoria. Il comando per
  abilitare tale opzione √® &lt;code class=&quot;highlighter-rouge&quot;&gt;hlEnable(HL_ADAPTIVE_VIEWPORT)&lt;/code&gt;.
  Per utilizzare l‚Äôadaptive viewport la scena deve essere ridisegnata
  regolarmente quando il dispositivo aptico √® in movimento, ad una
  frequenza tanto pi√π alta quanto pi√π veloce √® il movimento del
  dispositivo.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Haptic Camera View: Con tale opzione attivata, le HLAPI modificano automaticamente i
  parametri di visualizzazione usati per il rendering del depth buffer
  o del feedback buffer, in modo tale che solo le forme vicino alla
  posizione attuale del proxy vengono visualizzate. Nel caso venga
  utilizzato il feedback buffer, tale tecnica pu√≤ portare ad un
  incremento delle prestazioni in quanto viene ridotto il numero delle
  primitive geometriche prese in considerazione dal rendering aptico
  (l‚Äôincremento effettivo dipende dalla densit√† degli oggetti presenti
  sulla scena). Nel caso invece di utilizzo del depth buffer
  l‚Äôincremento sar√† di minore entit√†, in quanto il rendering aptico
  del depth buffer √® indipendente dal numero di primitive; inoltre
  l‚Äôimmagine generata dal depth buffer √® solo un sottoinsieme
  dell‚Äôintero buffer ed √® possibile sentire parti degli oggetti che
  non sono visibili dal punto di vista grafico. Come per l‚Äôadaptive
  viewport, l‚Äôabilitazione dell‚Äôopzione &lt;em&gt;haptic camera view&lt;/em&gt; richiede
  che la scena venga ridisegnata ad una frequenza proporzionale alla
  velocit√† di movimento del dispositivo.
  L‚Äôutilizzo di questa opzione disabilita l‚Äôadaptive viewport.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Culling con partizioni spaziali: Quando nella scena sono presenti degli oggetti costituiti da un
  numero molto grande di primitive il culling pu√≤ diventare molto
  costoso in termini di risorse di calcolo; usando strutture
  particolari (come alberi binari) √® possibile effettuare il culling
  di molte primitive in un‚Äôunica operazione.
  Per prima cosa √® necessario determinare la regione dello spazio che
  viene presa in considerazione per il rendering aptico; tale regione
  √® semplicemente quella che viene impostata dalle HLAPI nella
  chiamata a &lt;code class=&quot;highlighter-rouge&quot;&gt;hlBeginShape&lt;/code&gt; quando √® abilitata la &lt;em&gt;haptic camera
  view&lt;/em&gt;. Una volta individuata la regione, si utilizzano delle
  partizioni dello spazio per trovare il sottoinsieme di primitive che
  si trovano all‚Äôinterno (completamente o parzialmente) di essa; tali
  geometrie vengono visualizzate tramite OpenGL.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In conclusione, se si lavora con un numero elevato di primitive √®
conveniente utilizzare il depth buffer mentre, al contrario, √®
conveniente utilizzare il feedback buffer se il numero di primitive √®
ridotto.&lt;/p&gt;

&lt;p&gt;Il depth buffer √® meno accurato del feedback buffer, anche se tale
differenza non √® percettibile: con il depth buffer infatti le forme
vengono trasformate in un‚Äôimmagine bidimensionale prima di calcolarne il
rendering aptico, e tale trasformazione comporta una perdita di
informazioni. L‚Äôopzione &lt;em&gt;haptic camera view&lt;/em&gt; permette di individuare una
vista che minimizzi la perdita di dettagli dell‚Äôimmagine, anche se non
sempre √® possibile catturare tutte le informazioni. Se vengono usate
linee o punti come vincoli, deve essere usato il feedback buffer in
quanto tali primitive non possono essere catturate dal depth buffer.&lt;/p&gt;

&lt;h3 id=&quot;mappatura-del-dispositivo-aptico-sulla-scena&quot;&gt;Mappatura del dispositivo aptico sulla scena&lt;/h3&gt;

&lt;p&gt;Di seguito viene descritto come i movimenti del dispositivo aptico
vengono tradotti in movimenti nella rappresentazione grafica.&lt;/p&gt;

&lt;h4 id=&quot;lo-spazio-di-lavoro-aptico&quot;&gt;Lo spazio di lavoro aptico&lt;/h4&gt;

&lt;p&gt;Lo spazio di lavoro aptico √® lo spazio che pu√≤ essere raggiunto dal
dispositivo aptico, e le dimensioni (in millimetri) di tale spazio
possono essere ottenute con il comando&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;HLdouble workspaceDims[6];
hlGetDoublev(HL_WORKSPACE, workspaceDims);
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Si pu√≤ scegliere di non utilizzare l‚Äôintero spazio di lavoro scegliendo
la porzione utilizzabile con la chiamata a &lt;code class=&quot;highlighter-rouge&quot;&gt;hlWorkspace&lt;/code&gt;.&lt;/p&gt;

&lt;h4 id=&quot;stack-di-matrici&quot;&gt;Stack di matrici&lt;/h4&gt;

&lt;p&gt;Sono previsti due stack di matrici 4x4 tramite le quali viene effettuata
la mappatura: &lt;code class=&quot;highlighter-rouge&quot;&gt;HL_VIEWTOUCH_MATRIX&lt;/code&gt; e &lt;code class=&quot;highlighter-rouge&quot;&gt;HL_TOUCHWORKSPACE_MATRIX&lt;/code&gt;. Mentre
il primo ha la funzione di definire una mappatura tra lo spazio di
lavoro e le coordinate visive, il secondo serve ad orientare lo spazio
di lavoro secondo le coordinate visive.&lt;/p&gt;

&lt;figure class=&quot;image&quot;&gt;
  &lt;img src=&quot;images/matrixstack1.jpg&quot; alt=&quot;Mappatura dallo spazio di lavoro aptico alla scena grafica.&quot; /&gt;
  &lt;figcaption&gt;Mappatura dallo spazio di lavoro aptico alla scena grafica.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Le coordinate globali vengono prima trasformate nelle coordinate locali
(le coordinate della vista corrente); successivamente si ottengono le
coordinate di tocco, le quali rappresentano la base della mappatura
dello spazio di lavoro sulle coordinate della vista corrente. Infine
l‚Äôultima trasformazione porta ad ottenere le coordinate dello spazio di
lavoro (coordinate locali del dispositivo aptico).&lt;/p&gt;

&lt;p&gt;Il funzionamento dei due stack √® analogo a quello delle OpenGL: le HLAPI
mantengono uno stack corrente e tutte le operazioni hanno effetto su
questo.&lt;/p&gt;

&lt;h3 id=&quot;propriet√†-dei-materiali-ed-effetti-aptici&quot;&gt;Propriet√† dei materiali ed effetti aptici&lt;/h3&gt;

&lt;p&gt;Controllando le propriet√† dei materiali si controllano le propriet√†
tattili della superficie, analogamente a come vengono controllate le
propriet√† visive. Il comando per specificare tali propriet√† √®
&lt;code class=&quot;highlighter-rouge&quot;&gt;hlMaterial&lt;/code&gt;, e pu√≤ essere applicato al fronte o retro della superficie
o ad entrambe le facce.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Rigidit√†: La rigidit√† di una superficie viene impostata tramite il comando &lt;code class=&quot;highlighter-rouge&quot;&gt;hlMaterialf(HL_FRONT_AND_BACK, HL_STIFFNESS, 0.7)&lt;/code&gt; dove la &lt;code class=&quot;highlighter-rouge&quot;&gt;f&lt;/code&gt; aggiunta indica che il parametro numerico √® un &lt;em&gt;float&lt;/em&gt;.
  Matematicamente rappresenta il tasso con il quale la forza aumenta
  mano a mano che il dispositivo tenta di penetrare la superficie
  secondo la legge di Hooke &lt;script type=&quot;math/tex&quot;&gt;F=kx&lt;/script&gt;, dove &lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt; √® la costante di rigidit√†
  e &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; √® il vettore rappresentante la penetrazione.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Smorzamento: Impostando tale propriet√† tramite il comando &lt;code class=&quot;highlighter-rouge&quot;&gt;hlMaterialf(HL_FRONT_AND_BACK, HL_DAMPING, 0.1)&lt;/code&gt;
  viene aggiunta una forza di resistenza dipendente dalla velocit√†
  secondo la legge &lt;script type=&quot;math/tex&quot;&gt;F=kv&lt;/script&gt;, con &lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt; costante di smorzamento e &lt;script type=&quot;math/tex&quot;&gt;v&lt;/script&gt;
  velocit√† del dispositivo.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;Frizione: La frizione indica la resistenza di un oggetto al movimento laterale
  su di esso. Si distingue tra frizione &lt;em&gt;statica&lt;/em&gt; e &lt;em&gt;dinamica&lt;/em&gt;: la
  prima rappresenta la resistenza che si avverte quando il dispositivo
  inizia il suo moto sull‚Äôoggetto, mentre la seconda indica la
  resistenza che si avverte quando il dispositivo √® in movimento
  sull‚Äôoggetto. Si pu√≤ pensare al ghiaccio come esempio di materiale
  dotato di un‚Äôalta frizione statica ma bassa frizione dinamica; la
  gomma invece ha coefficienti elevati per entrambi i tipi di
  frizione. Il comando per settare le due propriet√† √®:
    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;hlMaterialf(HL_FRONT_AND_BACK, HL_STATIC_FRICTION, 0.3)
hlMaterialf(HL_FRONT_AND_BACK, HL_DYNAMIC_FRICTION, 0.2)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;
  &lt;/li&gt;
  &lt;li&gt;Vincoli: Sulla superficie possono essere specificati dei vincoli che
  costringono il proxy in una determinata posizione (come nella
  simulazione di una superficie magnetica). Per attivare questa
  modalit√† √® sufficiente eseguire il seguente comando prima della
  creazione dell‚Äôoggetto:
    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;hlTouchModel(HL_COSTRAINT);
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;

    &lt;p&gt;per tornare alla modalit√† di default si richiama nuovamente la
  funzione &lt;code class=&quot;highlighter-rouge&quot;&gt;hlTouchModel&lt;/code&gt; con il parametro &lt;code class=&quot;highlighter-rouge&quot;&gt;HL_CONTACT&lt;/code&gt;. E‚Äô possibile
  inoltre impostare il raggio d‚Äôazione del vincolo tramite il comando:&lt;/p&gt;
    &lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;hlTouchModelf(HL_SNAP_DISTANCE, 1.5);
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;    &lt;/div&gt;

    &lt;p&gt;oggi volta che il proxy viene a trovarsi ad una distanza dal vincolo
  minore del valore specificato da questa istruzione, verr√† applicato
  il vincolo con una forza proporzionale alla distanza.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Per tutte le propriet√† il coefficiente varia da 0 a 1. E‚Äô opportuno
porre attenzione all‚Äôutilizzo di valori troppo elevati in quanto possono
causare instabilit√† del dispositivo.&lt;/p&gt;

&lt;h3 id=&quot;eventi&quot;&gt;Eventi&lt;/h3&gt;

&lt;p&gt;Tramite le HLAPI √® possibile associare una funzione al verificarsi di un
determinato evento. Ci√≤ avviene eseguendo il comando:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;hlAddEventCallback(HL_EVENT_TOUCH, HL_OBJECT_ANY, HL_CLIENT_THREAD, &amp;amp;function, NULL);
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;in questo modo verr√† eseguita la funzione &lt;code class=&quot;highlighter-rouge&quot;&gt;function&lt;/code&gt; ogni volta che
un‚Äôoggetto verr√† toccato. La funzione di callback deve essere
specificata come segue:&lt;/p&gt;
&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;void HLCALLBACK function(HLENUM event, HLuint object, HLenum thread, HLcache *cache, void *userdata)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Le callback possono essere associate ai seguenti eventi:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Tocco: Un tocco (&lt;em&gt;touch&lt;/em&gt;) viene individuato quando il motore di rendering
  determina un contatto tra il proxy e un oggetto di cui √® stato fatto
  il rendering nell‚Äôultimo frame; se il proxy resta in contatto con
  l‚Äôoggetto, solo il primo istante di contatto viene rilevato come
  tocco. Analogamente il distacco (&lt;em&gt;untouch&lt;/em&gt;) dalla superficie viene
  rilevato quando il proxy smette di essere in contatto con l‚Äôoggetto.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Movimento: Il moto viene rilevato come tale quando la posizione o
  l‚Äôorientamento del proxy cambiano. La variazione di movimento e
  orientamento che viene rilevata come movimento pu√≤ essere modificata
  tramite le variabili &lt;code class=&quot;highlighter-rouge&quot;&gt;HL_EVENT_MOTION_TOLERANCE&lt;/code&gt; e
  &lt;code class=&quot;highlighter-rouge&quot;&gt;HL_EVENT_ANGULAR_TOLERANCE&lt;/code&gt;.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Pulsante: Possono essere associate delle funzioni anche alla pressione dei
  pulsanti presenti sul dispositivo usando come identificatori per il
  primo pulsante &lt;code class=&quot;highlighter-rouge&quot;&gt;HL_EVENT1_BUTTON_DOWN&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;HL_EVENT1_BUTTON_UP&lt;/code&gt; e per
  il secondo pulsante &lt;code class=&quot;highlighter-rouge&quot;&gt;HL_EVENT2_BUTTON_DOWN&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;HL_EVENT2_BUTTON_UP&lt;/code&gt;.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;</content><author><name>Andrea Maglie</name></author><category term="haptic feedback" /><category term="pure data" /><summary type="html">Disclaimer: Il contenuto di questo post √® un estratto della mia tesi di laurea, realizzata nel 2006. Alcuni esempi potrebbero risultare obsoleti, tuttavia la componente teorica resta un buon riferimento per chi deve approcciarsi alla materia.</summary></entry><entry><title type="html">Dispositivi Aptici</title><link href="/dispositivi-aptici" rel="alternate" type="text/html" title="Dispositivi Aptici" /><published>2019-02-05T00:00:00+01:00</published><updated>2019-02-05T00:00:00+01:00</updated><id>/dispositivi-aptici</id><content type="html" xml:base="/dispositivi-aptici">&lt;hr /&gt;

&lt;p&gt;&lt;i&gt;Disclaimer: Il contenuto di questo post √® un estratto della mia tesi di laurea, realizzata nel 2006. Alcuni esempi potrebbero risultare obsoleti, tuttavia la componente teorica resta un buon riferimento per chi deve approcciarsi alla materia.&lt;/i&gt;&lt;/p&gt;
&lt;hr /&gt;

&lt;h2 id=&quot;cosa-sono-i-dispositivi-aptici&quot;&gt;Cosa sono i dispositivi aptici&lt;/h2&gt;

&lt;p&gt;Negli anni settanta e ottanta la ricerca nel campo della robotica ha
iniziato a dirigere la sua attenzione verso la percezione e la
manipolazione tattile per la creazione di robot autonomi; negli anni
novanta invece la presenza di nuove tecnologie ha permesso la nascita
della &lt;em&gt;computer haptics&lt;/em&gt;, cio√® una virtualizzazione che, come nella
computer graphics, permette di simulare oggetti in maniera interattiva.
Sebbene Knoll abbia dimostrato gi√† negli anni sessanta la possibilit√† di
eseguire una interazione aptica con semplici oggetti, solo le tecnologie
recenti hanno reso possibile la realizzazione di tale interazione con
oggetti sempre pi√π complessi, combinando insieme dispositivi ad alte
prestazioni, modelli di calcolo geometrici, tecniche di rilevamento
delle collisioni e comprensione del sistema tattile umano.&lt;/p&gt;

&lt;p&gt;I dispositivi aptici si comportano come piccoli robot che effettuano uno
scambio di energia con l‚Äôutente; in particolare permettono all‚Äôutente di
interagire con un ambiente virtuale ricevendo un feedback tattile,
ottenuto applicando delle forze opposte al movimento dell‚Äôutente lungo i
tre assi &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;y&lt;/script&gt; e &lt;script type=&quot;math/tex&quot;&gt;z&lt;/script&gt;. La caratteristica principale che li distingue
dalle altre tipologie di dispositivi √® la loro &lt;em&gt;bidirezionalit√†&lt;/em&gt;, cio√®
la possibilit√† di ricevere informazioni dall‚Äôutente e di inviare
informazioni all‚Äôutente. Infatti, durante il normale utilizzo di un tale
dispositivo, l‚Äôuomo imprime una forza al meccanismo aptico (uno stilo,
un ditale o un altro componente diverso a seconda del tipo di
interfaccia) e ne cambia la posizione; successivamente √® il dispositivo
a riflettere una forza calcolata in base al movimento al quale √® stato
sottoposto.&lt;/p&gt;

&lt;p&gt;I &lt;em&gt;Bilateral Master‚ÄìSlave Manipulator&lt;/em&gt; (MSMs) fanno parte dei primi
dispositivi aptici sviluppati e venivano impiegati nell‚Äôindustria
nucleare per manipolare in modo sicuro e remoto il materiale irradiato.
Il termine MSMs deriva dal fatto che il braccio meccanico &lt;em&gt;master&lt;/em&gt; √®
tipicamente una riproduzione di un braccio remoto &lt;em&gt;slave&lt;/em&gt;, e i due sono
legati da catene, cavi o altri sistemi elettromeccanici. Un altro dei
primi usi dei bracci meccanici master √® avvenuto nell‚Äôambito della
realt√† virtuale per la manipolazione delle molecole e l‚Äôinterazione
aptica con la simulazione della forza elettrostatica
molecola‚Äìsubstrato; un esempio di questo √® mostrato in figura  &lt;a class=&quot;citation&quot; href=&quot;#art:stone&quot;&gt;(Stone, R. J., 2000)&lt;/a&gt;:&lt;/p&gt;

&lt;figure class=&quot;image&quot;&gt;
  &lt;img src=&quot;images/molecole.jpg&quot; alt=&quot;Uso di un braccio meccanico per la manipolazione di molecole in un ambiente di realt√† virtuale.&quot; /&gt;
  &lt;figcaption&gt;Uso di un braccio meccanico per la manipolazione di molecole in un ambiente di realt√† virtuale.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Altri progetti importanti sono stati sviluppati nel campo
del force feedback per la chirurgia, come il progetto IERAPSI dello
European Union Framework, un ambiente per le prove e la pianificazione
degli interventi chirurgici, oppure nel campo militare per il training
sul rilevamento e disinnesco delle mine.&lt;/p&gt;

&lt;figure class=&quot;image&quot;&gt;
  &lt;img src=&quot;images/chirurgia.jpg&quot; alt=&quot;Sistema di feedback aptico usato per la pianificazione di interventi chirurgici.&quot; /&gt;
  &lt;figcaption&gt;Sistema di feedback aptico usato per la pianificazione di interventi chirurgici.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Il termine &lt;em&gt;aptico&lt;/em&gt; (dal greco &lt;em&gt;haptesthai&lt;/em&gt; che significa &lt;em&gt;toccare&lt;/em&gt;)
indica la parte di fisiologia inerente il senso del tatto, mentre con
&lt;em&gt;feedback aptico&lt;/em&gt; (haptic feedback) si indica sia la consapevolezza
degli stimoli ricevuti in conseguenza di un contatto con una superficie,
sia le informazioni cinetiche ricevute in base alla posizione e al
movimento del corpo. Lo scambio bidirezionale di informazioni con un
dispositivo aptico viene chiamato &lt;em&gt;percezione aptica&lt;/em&gt; (haptic
perception).&lt;/p&gt;

&lt;p&gt;In figura √® riportato lo schema di un‚Äôinterazione aptica:&lt;/p&gt;

&lt;figure class=&quot;image&quot;&gt;
  &lt;img src=&quot;images/interazione_aptica.jpg&quot; alt=&quot;Interazione aptica tra utente e dispositivo.&quot; /&gt;
  &lt;figcaption&gt;Interazione aptica tra utente e dispositivo.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;nella parte sinistra si vede come avvengono le
elaborazioni all‚Äôinterno del corpo umano: i recettori posti sulla pelle
ricevono le informazioni sulla forza e le trasmettono al cervello, il
quale comanda poi i muscoli del braccio e della mano. Il dispositivo
invece (parte destra) rileva il moto attraverso dei sensori che inviano
i dati sulla posizione al computer al quale il dispositivo stesso √®
collegato; quest‚Äôultimo elabora i dati ricevuti e invia i comandi di
torsione agli attuatori che generano la forza di resistenza nel
dispositivo.&lt;/p&gt;

&lt;h2 id=&quot;tipi-di-dispositivi&quot;&gt;Tipi di dispositivi&lt;/h2&gt;

&lt;p&gt;I dispositivi aptici possono essere distinti in base al loro
posizionamento:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Dispositivi ground based&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;I dispositivi &lt;em&gt;ground based&lt;/em&gt; sono quelli che vengono appoggiati ad
un tavolo di lavoro e includono i joystick che riflettono le forze e
le interfacce aptiche desktop, come il Phantom¬† Omni¬† o il Phantom¬†
Desktop¬†di SensAble Tecnologies.&lt;/p&gt;

&lt;figure class=&quot;image&quot;&gt;
  &lt;img src=&quot;images/PhantomDesktop.jpg&quot; alt=&quot;Dispositivo aptico Phantom¬†Desktop¬†di SensAble Technologies.&quot; /&gt;
  &lt;figcaption&gt;Dispositivo aptico Phantom¬†Desktop¬†di SensAble Technologies.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;&lt;em&gt;Esoscheletri meccanici&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Gli &lt;em&gt;esoscheletri meccanici&lt;/em&gt; sono dei dispositivi che vengono
indossati dall‚Äôutente su braccia o gambe.&lt;/p&gt;

&lt;figure class=&quot;image&quot;&gt;
  &lt;img src=&quot;images/esoscheletro.jpg&quot; alt=&quot;Dispositivo aptico CyberForce¬†composto da un esoscheletro poggiato direttamente al suolo, [www.immersion.com](www.immersion.com)&quot; /&gt;
  &lt;figcaption&gt;Dispositivo aptico CyberForce¬†composto da un esoscheletro poggiato direttamente al suolo, [www.immersion.com](www.immersion.com)&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;&lt;em&gt;Guanti force feedback&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;I &lt;em&gt;guanti force feedback&lt;/em&gt; leggono le informazioni di contatto delle
singole dita e restituiscono le forze di resistenza, ma non possono
riprodurre sensazioni di oggetti pesanti o inerzie&lt;/p&gt;

&lt;figure class=&quot;image&quot;&gt;
  &lt;img src=&quot;images/cybergrasp.jpg&quot; alt=&quot;&quot; /&gt;
  &lt;figcaption&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Un altro tipo di distinzione √® tra dispositivi a &lt;em&gt;impedenza&lt;/em&gt; o
&lt;em&gt;ammettenza&lt;/em&gt;: i primi leggono informazioni relative alla posizione e
inviano in output informazioni sulle forze, i secondi leggono le forze e
inviano i dati sulla posizione. Il terzo modo di classificare i
dispositivi √® in base ai gradi di libert√† che caratterizzano il loro
movimento:&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Dispositivi ad un grado di libert√†&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Misurano e applicano le forze lungo una sola direzione. Esempi di
movimenti reali ad un grado di libert√† includono l‚Äôapertura di una
porta, l‚Äôuso di un paio di forbici o la pressione del pistone di una
siringa. La simulazione aptica ad un grado di libert√† pi√π comune √®
il &lt;em&gt;muro virtuale&lt;/em&gt;, che corrisponde al rendering di una forza che si
manifesta al contatto con una superficie molto rigida.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Dispositivi a due gradi di libert√†&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Il mouse √® un semplice esempio di dispositivo a due gradi di
libert√†; possono essere utilizzati per interagire con oggetti
tridimensionali mappando opportunamente i punti di contatto a tre
gradi di libert√† su un piano.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Dispositivi a tre gradi di libert√†&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;In questo caso la mappatura dallo spazio tridimensionale del
dispositivo a quello virtuale √® pi√π immediata, in quanto ogni
componente della posizione nel sistema di riferimento solidale al
dispositivo deve essere trasformata nella corrispondente componente
nel sistema di riferimento virtuale. Nell‚Äôinterazione con gli
oggetti virtuali il modulo della forza √® calcolato in base a quanto
il dispositivo penetra l‚Äôoggetto, mentre per il calcolo della
direzione sono stati sviluppati vari metodi, tra i quali quello
maggiormente utilizzato √® l‚Äôalgoritmo del &lt;em&gt;proxy&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;&lt;em&gt;Dispositivi a pi√π di tre gradi di libert√†&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Per rendere pi√π realistici gli scenari virtuali si √® reso necessario
introdurre la possibilit√† di effettuare torsioni. Barbagli
&lt;a class=&quot;citation&quot; href=&quot;#art:barbagli1&quot;&gt;(Barbagli, F., Salisbury, K., &amp;amp; Devengenzo, R., 2003)&lt;/a&gt; ha scritto un algoritmo che supporta l‚Äôinterazione
con un punto di contatto con frizione e momenti per simulare quattro
gradi di libert√†; la simulazione di cinque gradi di libert√† (come
nel caso del contatto tra un segmento e un oggetto) √® stata
implementata da Basdogan &lt;a class=&quot;citation&quot; href=&quot;#art:basdogan1&quot;&gt;(Basdogan, C., Ho, C. H., &amp;amp; Srinivasan, M. A., 1997)&lt;/a&gt;, mentre studi sulle
interazioni a sei gradi di libert√† sono stati eseguiti da McNeely
&lt;a class=&quot;citation&quot; href=&quot;#art:mcneely&quot;&gt;(McNeely, W., Puterbaugh, K., &amp;amp; Troy, J., 1999)&lt;/a&gt; e Otaduy e Lin &lt;a class=&quot;citation&quot; href=&quot;#art:otaduy&quot;&gt;(Otaduy, M.A. &amp;amp; Lin, M., 2003)&lt;/a&gt;.&lt;/p&gt;

&lt;h4 id=&quot;caratteristiche-e-prestazioni-dei-dispositivi&quot;&gt;Caratteristiche e prestazioni dei dispositivi&lt;/h4&gt;

&lt;p&gt;Le prestazioni dei vari tipi di dispositivi dipendono dalle abilit√† e
limitazioni umane e dell‚Äôutente; le simulazioni di oggetti e ambienti
virtuali sono sempre basate su calcoli approssimati, e sono proprio le
limitazioni della sensibilit√† dell‚Äôutente a determinare se tali
approssimazioni sono sufficienti. Le caratteristiche che un‚Äôinterfaccia
aptica dovrebbe possedere sono &lt;a class=&quot;citation&quot; href=&quot;#art:basdogan2&quot;&gt;(Srinivasan, M. A. &amp;amp; Basdogan, C., 1997)&lt;/a&gt;:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Livelli di inerzia e frizione bassi, assieme ad assenza di
costrizioni cinematiche imposte dal dispositivo, in modo tale che il
movimento libero sia avvertito effettivamente come tale (in altre
parole, l‚Äôutente non deve avvertire forze di resistenza quando
queste non vengono simulate dall‚Äôapplicazione).&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;La risoluzione, sia in termini di posizione che di forza riflessa,
deve corrispondere a quella del sistema tattile umano e dipende dal
processo nel quale viene impiegato il dispositivo. In particolare
l‚Äôutente:&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;
        &lt;p&gt;non deve poter attraversare gli oggetti applicando una forza
eccessiva;&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;non deve avvertire vibrazioni non volute (come quelle dovute a
quantizzazioni);&lt;/p&gt;
      &lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;non deve avvertire come elastici gli oggetti che invece sono
rigidi e viceversa.&lt;/p&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Ergonomia e comfort: i dispositivi non devono arrecare disturbi
all‚Äôutente che li indossa o li utilizza, in quanto tali disturbi
potrebbero sovrastare tutte le altre sensazioni, in particolare
quelle aptiche.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;sec:rendering_aptico&quot;&gt;Struttura di una pipeline di rendering aptico&lt;/h2&gt;

&lt;p&gt;La struttura di una pipeline di rendering aptico √® relativamente
semplice.&lt;/p&gt;

&lt;figure class=&quot;image&quot;&gt;
  &lt;img src=&quot;images/renderingaptico.jpg&quot; alt=&quot;Processo associato al rendering delle forze. Le linee in grassetto rappresentano i flussi del processo, mentre quelle tratteggiate rappresentano lo scambio di informazioni.&quot; /&gt;
  &lt;figcaption&gt;Processo associato al rendering delle forze. Le linee in grassetto rappresentano i flussi del processo, mentre quelle tratteggiate rappresentano lo scambio di informazioni.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Quando l‚Äôutente muove la sonda del
dispositivo, i nuovi valori di posizione e orientazione vengono
catturati dai codificatori; successivamente vengono rilevate le
eventuali collisioni con gli oggetti virtuali: se la sonda entra in
contatto con un oggetto, viene calcolata la forza di reazione in base
alla profondit√† della penetrazione della sonda nell‚Äôoggetto. La forza
calcolata viene poi mappata sulla superficie dell‚Äôoggetto in modo da
tenere in considerazione i dettagli di quest‚Äôultimo; il vettore delle
forze cos√¨ modificato viene inviato al dispositivo e, tramite questo,
all‚Äôutente.&lt;/p&gt;

&lt;p&gt;Le diverse tecniche di rendering possono essere distinte a seconda del
tipo di interazione in &lt;em&gt;point based&lt;/em&gt; e &lt;em&gt;ray based&lt;/em&gt;:&lt;/p&gt;

&lt;figure class=&quot;image&quot;&gt;
  &lt;img src=&quot;images/pointray.jpg&quot; alt=&quot;Interazioni aptiche point--based e ray--based.&quot; /&gt;
  &lt;figcaption&gt;Interazioni aptiche point--based e ray--based.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Con l‚Äôinterazione point‚Äìbased solo l‚Äôestremit√† del dispositivo
(&lt;em&gt;HIP&lt;/em&gt;, haptic interface point) entra in contatto con gli oggetti.
Dato che questi hanno una rigidit√† finita, il punto HIP li penetra e
la profondit√† della penetrazione √® calcolata come la minima distanza
tra HIP e superficie dell‚Äôoggetto virtuale.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Nell‚Äôinterazione ray‚Äìbased la sonda √® modellata come un raggio
finito; l‚Äôalgoritmo di collisione individua come punto di contatto
l‚Äôintersezione tra il raggio e la superficie dell‚Äôoggetto, mentre
come profondit√† della penetrazione si considera la distanza tra HIP
e punto di collisione lungo la normale alla superficie.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In entrambi i casi la forza √® calcolata usando la legge di Hooke &lt;script type=&quot;math/tex&quot;&gt;F=kx&lt;/script&gt;,
dove &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; √® la stessa direzione lungo la quale viene calcolata la
profondit√† di penetrazione; se le interazioni sono prive di forze di
frizione, la forza di reazione √® normale alla superficie nel punto di
contatto.&lt;/p&gt;

&lt;p&gt;Zilles e Salisbury &lt;a class=&quot;citation&quot; href=&quot;#art:zilles&quot;&gt;(Zilles, C. B. &amp;amp; Salisbury, J. K., 1995)&lt;/a&gt; hanno sviluppato un algoritmo pi√π
sofisticato per il rendering delle forze basato sulla tecnica
point‚Äìbased; hanno definito un nuovo punto, detto &lt;em&gt;god‚Äìobject&lt;/em&gt; o
&lt;em&gt;proxy&lt;/em&gt;, rappresentante la locazione sulla superficie del punto di
contatto che viene calcolato ogni volta che il dispositivo viene mosso,
con il vincolo ulteriore che la distanza tra god‚Äìobject e HIP deve
essere minimizzata, mentre il god‚Äìobject resta sempre sulla superficie
dell‚Äôoggetto anche quando il punto HIP lo penetra.&lt;/p&gt;

&lt;p&gt;Un approccio pi√π semplice consiste nel calcolare un‚Äôapprossimazione
della superficie come un piano tangente al punto di contatto; tale piano
viene aggiornato ad una frequenza inferiore a quella propria del
rendering delle forze, pertanto pu√≤ portare a sentire delle
discontinuit√† se il dispositivo viene mosso su grandi distanze prima che
il piano venga aggiornato.&lt;/p&gt;

&lt;p&gt;Un‚Äôaltra tecnica consiste nel considerare gli oggetti virtuali come
volumi formati da &lt;em&gt;voxel&lt;/em&gt; (elementi di volume rappresentanti un valore
su una griglia regolare nello spazio tridimensionale); ad ogni voxel
vengono associati otto byte di informazioni, includenti i valori di
densit√† del materiale, gradiente di densit√†, colore e altre propriet√†
aptiche come viscosit√† ed elasticit√†. Quando il punto HIP si trova
all‚Äôinterno dell‚Äôoggetto, il valore scalare di densit√† al punto di
contatto viene usato per calcolare la forza di reazione attraverso delle
trasformazioni lineari, mentre il valore di gradiente della densit√†
viene usato per determinare la direzione normale alla superficie.&lt;/p&gt;

&lt;p&gt;Basdogan ha sviluppato una tecnica di rendering ray‚Äìbased
&lt;a class=&quot;citation&quot; href=&quot;#art:basdogan1&quot;&gt;(Basdogan, C., Ho, C. H., &amp;amp; Srinivasan, M. A., 1997)&lt;/a&gt; nella quale le coordinate delle due estremit√† della
sonda vengono aggiornate ad ogni movimento, rilevando ogni collisione
tra il raggio e l‚Äôoggetto virtuale e calcolando la forza secondo la
legge di Hooke. Il rilevamento delle collisioni avviene in tre passi:
per prima cosa viene rilevata la collisione tra il raggio e lo spazio
rettangolare contenente l‚Äôoggetto virtuale, poi tra il raggio e lo
spazio che circonda un qualsiasi elemento triangolare; infine viene
rilevato il contatto tra il raggio e l‚Äôelemento triangolare usando
calcoli geometrici. La divisione in pi√π passi porta ad un aumento di
prestazioni consentendo di lavorare a frame rate pi√π elevati.&lt;/p&gt;

&lt;h2 id=&quot;rendering-dei-dettagli-della-superficie&quot;&gt;Rendering dei dettagli della superficie&lt;/h2&gt;

&lt;figure class=&quot;image&quot;&gt;
  &lt;img src=&quot;images/forcedetail.jpg&quot; alt=&quot;Tecniche di rendering aptico dei dettagli di una superficie; le frecce rappresentano i vettori delle forze riflesse. L'area in grigio rappresenta la geometria dell'oggetto mentre la linea nera indica la geometria della superficie come viene percepita dall'utente.&quot; /&gt;
  &lt;figcaption&gt;Tecniche di rendering aptico dei dettagli di una superficie; le frecce rappresentano i vettori delle forze riflesse. L'area in grigio rappresenta la geometria dell'oggetto mentre la linea nera indica la geometria della superficie come viene percepita dall'utente.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Esistono diverse tecniche per effettuare il rendering dei dettagli della
superficie, diverse a seconda del tipo di sensazione che deve essere
data: superficie smussata, superficie con tessitura o superficie con
frizione.&lt;/p&gt;

&lt;h3 id=&quot;rendering-di-superfici-smussate&quot;&gt;Rendering di superfici smussate&lt;/h3&gt;

&lt;p&gt;Quando gli oggetti vengono rappresentati tramite insiemi di poligoni,
l‚Äôutente non percepisce la forma che si intendeva rappresentare, ma
avverte i lati e gli angoli dei vari poligoni. Per minimizzare tale
effetto Morgenbesser e Srinivasan &lt;a class=&quot;citation&quot; href=&quot;#art:srinivasan2&quot;&gt;(Srinivasan, M. A. &amp;amp; Morgenbesser, H. B., 1996)&lt;/a&gt; hanno elaborato una
tecnica chiamata &lt;em&gt;force shading&lt;/em&gt;: con tale metodo il vettore della forza
viene interpolato lungo la superficie in modo che la sua direzione vari
con continuit√†, di conseguenza l‚Äôutente avverte una
superficie pi√π smussata rispetto alla sua rappresentazione originale.
Diventa cos√¨ possibile sviluppare modelli geometrici di diverso livello
di dettaglio ed effettuare poi il rendering con il modello di forza
dettagliato o con il modello force‚Äìshaded a seconda dei requisiti
dell‚Äôapplicazione.&lt;/p&gt;

&lt;p&gt;Il force shading potrebbe diventare poco efficiente quando gli angoli
tra i poligoni che rappresentano una superficie sono al di sotto di un
certo valore; in questo caso √® sufficiente diminuire il numero di
poligoni utilizzati fino a quando gli angoli tra questi sono tutti
maggiori del valore critico.&lt;/p&gt;

&lt;h3 id=&quot;sec:rendering_tessiture&quot;&gt;Rendering delle tessiture aptiche&lt;/h3&gt;

&lt;p&gt;Srinivasan e Basdogan &lt;a class=&quot;citation&quot; href=&quot;#art:basdogan2&quot;&gt;(Srinivasan, M. A. &amp;amp; Basdogan, C., 1997)&lt;/a&gt; hanno sviluppato due metodologie
per il rendering delle tessiture di superfici tridimensionali che
prendono spunto da quelle usate nella computer graphics:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;Perturbazione delle forze&lt;/em&gt;: La perturbazione delle forze consiste nel modificare la direzione e
  il modulo del vettore delle forze per generare effetti sulla
  superficie (come la ruvidit√†). Max e Becker &lt;a class=&quot;citation&quot; href=&quot;#art:max&quot;&gt;(Max, N. L. &amp;amp; Becker, B. G., 1994)&lt;/a&gt; hanno
  sviluppato una formula che permette di generare delle ruvidit√†
  visuali, basandosi sulla normale alla superficie originale:
  &lt;script type=&quot;math/tex&quot;&gt;M = N - \nabla h + (\nabla h \cdot N)N&lt;/script&gt; dove &lt;script type=&quot;math/tex&quot;&gt;M&lt;/script&gt; √® la normale
  modificata, &lt;script type=&quot;math/tex&quot;&gt;N&lt;/script&gt; √® la normale originale e &lt;script type=&quot;math/tex&quot;&gt;\nabla h&lt;/script&gt; √® il gradiente
  della profondit√† della tessitura. Lo stesso metodo pu√≤ essere usato
  per generare la ruvidit√† aptica.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;Displacement mapping&lt;/em&gt;:  In questo caso, invece di modificare il vettore delle forze, viene
  modificata la geometria della superficie. Per generare micro‚Äìtessiture √®
  necessario che la superficie sia composta da un numero elevato di
  poligoni, in modo tale da rendere possibile la sua modifica punto
  per punto; tutto ci√≤ per√≤ incrementa considerevolmente il carico
  computazionale. Usare questa tecnica per il rendering aptico
  comporta ulteriori problemi: gli oggetti virtuali non possono essere
  infinitamente rigidi, perci√≤ il dispositivo penetra all‚Äôinterno
  della superficie; oppure il rilevamento delle collisioni o la
  localizzazione dei punti sulla superficie ogni volta che la sonda
  viene mossa crea ambiguit√† dovute all‚Äôesistenza di micro‚Äìdettagli,
  portando a delle discontinuit√† delle forze. Si capisce come il
  displacement mapping sia pi√π indicato per il rendering di
  macro‚Äìtessiture.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Altre tecniche invece si basano sull‚Äôuso della frequenza e della
profondit√† delle tessiture:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;Tessiture aptiche basate sulle immagini&lt;/em&gt;: L‚Äôidea consiste nell‚Äôutilizzare le informazioni dell‚Äôimmagine
  bidimensionale usata come texture dell‚Äôoggetto virtuale; assumendo
  che le intensit√† di grigio dell‚Äôimmagine possano essere usate
  direttamente come indicatori della profondit√† della tessitura
  aptica, possiamo associare ogni coordinata della texture &lt;script type=&quot;math/tex&quot;&gt;(u,v)&lt;/script&gt;
  alle coordinate di ogni vertice &lt;script type=&quot;math/tex&quot;&gt;(x,y,z)&lt;/script&gt;. Viene poi calcolato il
  gradiente della profondit√† al punto di collisione e il vettore della
  forza √® perturbato di conseguenza.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;Tessiture aptiche procedurali&lt;/em&gt;: L‚Äôobiettivo √® generare le tessiture aptiche mediante funzioni
  matematiche, usandole poi per modificare il gradiente &lt;script type=&quot;math/tex&quot;&gt;\nabla h&lt;/script&gt; e
  la forza di reazione al punto di contatto, o per modificare la
  geometria dell‚Äôoggetto; un esempio √® la simulazione di tessiture
  stocastiche &lt;a class=&quot;citation&quot; href=&quot;#art:fritz&quot;&gt;(Fritz, J. P. &amp;amp; Barner, K. E., 1996)&lt;/a&gt;.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;rendering-delle-superfici-con-frizione&quot;&gt;Rendering delle superfici con frizione&lt;/h3&gt;

&lt;p&gt;L‚Äôaggiunta di forze di frizione rende la simulazione aptica pi√π
realistica; ad esempio, senza frizione non saremmo in grado di premere
un pulsante virtuale in quanto il dispositivo scivolerebbe su di esso.&lt;/p&gt;

&lt;p&gt;Molti ricercatori hanno proposto l‚Äôutilizzo di frizioni di Coulomb o
frizioni viscose; le prime hanno una componente di frizione statica e
una di frizione dinamica, mentre le altre sono dipendenti dalla
velocit√†. Entrambi i tipi esercitano una forza opposta e tangenziale al
punto di contatto. Se la misurazione della velocit√† di contatto √®
affetta da rumore o i cambiamenti di posizione sono troppo veloci,
l‚Äôinterazione aptica potrebbe diventare instabile.&lt;/p&gt;

&lt;h2 id=&quot;sec:tessiture_stocastiche&quot;&gt;Generazione di tessiture casuali con il metodo stocastico&lt;/h2&gt;

&lt;p&gt;Definiamo &lt;em&gt;spazio di tessitura&lt;/em&gt; un volume di tessitura rappresentato da
vettori tridimensionali; un campione in un punto qualsiasi √® un vettore
tridimensionale senza vincoli su modulo o direzione. Questo spazio
continuo viene generato tramite vari processi che possono essere
deterministici, stocastici o misti.&lt;/p&gt;

&lt;p&gt;Le caratteristiche di una tessitura possono essere descritte tramite il
suo spettro di potenza, e variando lo spettro si ottengono diverse
tessiture.&lt;/p&gt;

&lt;h3 id=&quot;modelli-stocastici&quot;&gt;Modelli stocastici&lt;/h3&gt;

&lt;p&gt;I modelli stocastici sono caratterizzati da misure statistiche di una
immagine di texture; spesso sono sufficienti le misure di basso ordine.
Il pattern della texture viene sintetizzato applicando ad un rumore
bianco un filtro basato sulla funzione di autocorrelazione.&lt;/p&gt;

&lt;p&gt;Dato che l‚Äôobiettivo non √® simulare tessiture reali, bens√¨ tessiture che
siano percettibilmente diverse l‚Äôuna dall‚Äôaltra, √® possibile generare le
texture proprio processando un rumore tramite un filtro. Il rumore deve
essere a banda limitata (rumore rosa), stazionario (invariante rispetto
alla traslazione), isotropico e non deve essere periodico; il rumore
bianco filtrato possiede queste propriet√†. In particolare si considera
il &lt;em&gt;rumore bianco gaussiano&lt;/em&gt;: una trasformazione di un vettore casuale
gaussiano resta gaussiano ed √® completamente definito dalle sue
statistiche di primo e secondo ordine. Semplicemente modificando la
varianza (e di conseguenza lo spettro di potenza) √® possibile ottenere
tessiture che vanno da lisce a ruvide &lt;a class=&quot;citation&quot; href=&quot;#art:siira&quot;&gt;(Siira, J. &amp;amp; Pai, D. K., 1996)&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Per tessiture complesse si pu√≤ utilizzare una &lt;em&gt;pdf&lt;/em&gt; (probability density
function) ottenuta da varie pdf gaussiane:
&lt;script type=&quot;math/tex&quot;&gt;f(x) = \sum_{i=1}^{M} a_i N({\mu}_i,C_i),&lt;/script&gt; dove
&lt;script type=&quot;math/tex&quot;&gt;\sum_{i=1}^{M} a_i = 1&lt;/script&gt; e &lt;script type=&quot;math/tex&quot;&gt;N(\cdot)&lt;/script&gt; √® l‚Äôi-esima pdf gaussiana con
media &lt;script type=&quot;math/tex&quot;&gt;{\mu}_i&lt;/script&gt; e matrice di covarianza &lt;script type=&quot;math/tex&quot;&gt;C_i&lt;/script&gt;; i termini &lt;script type=&quot;math/tex&quot;&gt;a_i&lt;/script&gt; possono
essere variati arbitrariamente, o modellati in base a tessiture reali.
In figura √® rappresentata una texture bidimensionale
composta da due pdf gaussiane.&lt;/p&gt;

&lt;figure class=&quot;image&quot;&gt;
  &lt;img src=&quot;images/gaussianpdf.jpg&quot; alt=&quot;Texture bidimensionale composta da due pdf
gaussiane.&quot; /&gt;
  &lt;figcaption&gt;Texture bidimensionale composta da due pdf
gaussiane.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Tutte queste metodologie producono campioni indipendenti e identicamente
distribuiti, ottenendo cos√¨ uno spettro di potenza costante; per
modificare tale spettro √® sufficiente filtrarlo con tecniche che
affiancano trasformazioni statistiche al normale filtraggio. Un esempio
√® costituito dal moto Browniano visto nel paragrafo
&lt;a href=&quot;#sec:tessiture_superficie&quot;&gt;[sec:tessiture_superficie]&lt;/a&gt;{reference-type=‚Äùref‚Äù
reference=‚Äùsec:tessiture_superficie‚Äù}, un processo invariante rispetto
alla scala descritto da un parametro di auto‚Äìsimilarit√† &lt;script type=&quot;math/tex&quot;&gt;h&lt;/script&gt; e uno
spettro di potenza di tipo &lt;script type=&quot;math/tex&quot;&gt;1/f&lt;/script&gt; &lt;a class=&quot;citation&quot; href=&quot;#art:ebert&quot;&gt;(Ebert, D. S., Musgrave, F. K., Peachey, D., Perlin, K., &amp;amp; Worley, S., 1994)&lt;/a&gt; &lt;a class=&quot;citation&quot; href=&quot;#art:haruyama&quot;&gt;(Haruyama, S. &amp;amp; Barsky, B. A., 1984)&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;procedure-implicite-e-sintesi-spettrale-di-fourier&quot;&gt;Procedure implicite e sintesi spettrale di Fourier&lt;/h3&gt;

&lt;p&gt;La creazione di uno spettro di potenza √® possibile anche tramite
procedure implicite, usando ad esempio delle sinusoidi, che nel dominio
della frequenza diventano degli impulsi. Dalla teoria di Fourier si sa
che ogni segnale periodico pu√≤ essere visto come somma di infinite
sinusoidi: cos√¨ la creazione di una texture diventa un procedimento di
sintesi spettrale di Fourier. Le caratteristiche dello spettro vengono
modificate modulando l‚Äôampiezza e la frequenza delle sinusoidi, mentre
altri effetti spaziali possono essere ottenuti con variazioni di fase.
Possono essere utilizzate anche altre funzioni, come dente di sega o
onde quadre, le quali aggiungono componenti in alta frequenza ad
intervalli armonici; segnali di questo tipo diventano delle vibrazioni
in un‚Äôinterfaccia aptica, fatto che pu√≤ rendere pi√π realistica la
tessitura.&lt;/p&gt;

&lt;p&gt;Se il periodo dei segnali √® sufficientemente ampio, le funzioni
periodiche possono essere usate per creare tessiture che appaiono
casuali. Un altro modo per creare tessiture casuali con funzioni
periodiche √® l‚Äôuso di un processo stocastico: il vettore delle forze
della tessitura &lt;script type=&quot;math/tex&quot;&gt;F_t&lt;/script&gt; alla posizione &lt;script type=&quot;math/tex&quot;&gt;p&lt;/script&gt; pu√≤ essere espresso tramite la
seguente equazione: &lt;script type=&quot;math/tex&quot;&gt;F_t(p) = Sg(p)+N,&lt;/script&gt; dove &lt;script type=&quot;math/tex&quot;&gt;g(p)&lt;/script&gt; √® una funzione
implicita e deterministica, &lt;script type=&quot;math/tex&quot;&gt;S&lt;/script&gt; √® un rumore moltiplicativo e &lt;script type=&quot;math/tex&quot;&gt;N&lt;/script&gt;
rappresenta un rumore sovrapposto. Il rumore moltiplicativo pu√≤ essere
usato per scalare casualmente una funzione deterministica apportando
ulteriori variazioni alla tessitura.&lt;/p&gt;

&lt;h3 id=&quot;filtraggio&quot;&gt;Filtraggio&lt;/h3&gt;

&lt;figure class=&quot;image&quot;&gt;
  &lt;img src=&quot;images/filtraggio.jpg&quot; alt=&quot;Schema a blocchi per modellare una texture tramite filtraggio; H(z) √® la funzione di trasferimento complessiva.&quot; /&gt;
  &lt;figcaption&gt;Schema a blocchi per modellare una texture tramite filtraggio; H(z) √® la funzione di trasferimento complessiva.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Il filtraggio viene usato per modellare lo spettro di potenza allo scopo
di ottenere una tessitura con determinate caratteristiche. Una
classificazione qualitativa delle tessiture pu√≤ cos√¨ essere tradotta in
una descrizione spettrale. Ad esempio, le tessiture possono essere
ruvide, lisce, fini, granulate, regolari o irregolari; alcune di queste
caratteristiche denotano periodicit√†, mentre altre comportano delle
amplificazioni a determinate frequenze. Cos√¨, manipolando lo spettro di
potenza delle primitive in input √® possibile cambiare questi descrittori
qualitativi. Diverse tecniche di filtraggio possono essere impiegate a
tale scopo:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;I filtri lineari FIR sono filtri facili da progettare e implementare
e offrono una certa flessibilit√†. Il modellamento di un rumore
bianco pu√≤ ad esempio essere fatto mediante un banco di filtri FIR
passabanda (equalizzatore parametrico), nel quale i parametri
impostabili sono la frequenza centrale, la larghezza di banda e il
guadagno.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;I filtri IIR consentono un miglior controllo spettrale, pur essendo
pi√π complessi da progettare. Quando in input ad un filtro IIR viene
dato un rumore, il sistema √® anche noto come modello ARMA
(autoregressive moving average).&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;L‚Äôimplementazione del filtraggio √® computazionalmente piuttosto costosa.
Tale problema pu√≤ essere ovviato utilizzando un pre‚Äìfiltraggio, il
quale permette di risparmiare tempo durante la simulazione consumando
per√≤ pi√π memoria.&lt;/p&gt;

&lt;h2 id=&quot;riferimenti&quot;&gt;Riferimenti&lt;/h2&gt;

&lt;ol class=&quot;bibliography&quot;&gt;&lt;li&gt;&lt;span id=&quot;art:stone&quot;&gt;Stone, R. J. (2000). Haptic Feedback: A Potted History, From Telepresence to Virtual Reality. In S. Berlin (Ed.), &lt;i&gt;Haptic Human-Computer Interaction: First International Workshop, Glasgow, UK&lt;/i&gt;.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;art:barbagli1&quot;&gt;Barbagli, F., Salisbury, K., &amp;amp; Devengenzo, R. (2003). Enabling Multifinger, Multihand Virtualized Grasping. &lt;i&gt;Proc. IEEE Int‚Äôl Conf. Robotics and Automation (ICRA 03), Vol. 1&lt;/i&gt;, 806‚Äì815.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;art:basdogan1&quot;&gt;Basdogan, C., Ho, C. H., &amp;amp; Srinivasan, M. A. (1997). A Ray-Based Haptic Rendering Technique for Displaying Shape and Texture of 3D Objects in Virtual Environments. &lt;i&gt;Proc. ASME Dynamic Systems and Control Division&lt;/i&gt;, &lt;i&gt;61&lt;/i&gt;, 77‚Äì84.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;art:mcneely&quot;&gt;McNeely, W., Puterbaugh, K., &amp;amp; Troy, J. (1999). Six Degree-of-Freedom Haptic Rendering Using Voxel Sampling. &lt;i&gt;Proc. ACM Siggraph&lt;/i&gt;, 401‚Äì408.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;art:otaduy&quot;&gt;Otaduy, M.A., &amp;amp; Lin, M. (2003). Sensation Preserving Simplification for Haptic Rendering. &lt;i&gt;Proc. ACM Siggraph&lt;/i&gt;, 543‚Äì553.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;art:basdogan2&quot;&gt;Srinivasan, M. A., &amp;amp; Basdogan, C. (1997). Haptics In Virtual Enviroments: Taxonomy, Research Status, and Challenges. &lt;i&gt;Comput &amp;amp; Graphics&lt;/i&gt;, &lt;i&gt;21&lt;/i&gt;(4), 393‚Äì404.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;art:zilles&quot;&gt;Zilles, C. B., &amp;amp; Salisbury, J. K. (1995). A constraint based god-object method for haptic display. &lt;i&gt;IEEE International Conference on Intelligent Robots and Systems&lt;/i&gt;.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;art:srinivasan2&quot;&gt;Srinivasan, M. A., &amp;amp; Morgenbesser, H. B. (1996). Force Shading for Haptic Shape Perception. &lt;i&gt;Proceedings of the ASME Dynamic System and Control Division&lt;/i&gt;, &lt;i&gt;58&lt;/i&gt;, 407‚Äì412.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;art:max&quot;&gt;Max, N. L., &amp;amp; Becker, B. G. (1994). Bump shading for volume texture. &lt;i&gt;IEEE Computer Graphics and Applications&lt;/i&gt;, (14), 18‚Äì20.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;art:fritz&quot;&gt;Fritz, J. P., &amp;amp; Barner, K. E. (1996). Stochastic Models For Haptic Texture. &lt;i&gt;Proceedings of SPIE‚Äôs International Symposium on Intelligent System and Advanced Manufactoring - Telemanipulator and Telepresence Technologies III&lt;/i&gt;.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;art:siira&quot;&gt;Siira, J., &amp;amp; Pai, D. K. (1996). Haptic texturing - a stochastic approach. &lt;i&gt;International Conference on Robotics and Automation&lt;/i&gt;.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;art:ebert&quot;&gt;Ebert, D. S., Musgrave, F. K., Peachey, D., Perlin, K., &amp;amp; Worley, S. (1994). Texturing and Modeling: a procedural approach. &lt;i&gt;AP Professional&lt;/i&gt;.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;art:haruyama&quot;&gt;Haruyama, S., &amp;amp; Barsky, B. A. (1984). Using stochastic modeling for texture generation. &lt;i&gt;IEEE Computer Graphics and Applications&lt;/i&gt;, 7‚Äì19.&lt;/span&gt;&lt;/li&gt;&lt;/ol&gt;</content><author><name>Andrea Maglie</name></author><category term="haptic feedback" /><category term="pure data" /><summary type="html">Disclaimer: Il contenuto di questo post √® un estratto della mia tesi di laurea, realizzata nel 2006. Alcuni esempi potrebbero risultare obsoleti, tuttavia la componente teorica resta un buon riferimento per chi deve approcciarsi alla materia.</summary></entry><entry><title type="html">Pure Data e i modelli audio</title><link href="/pure-data-modelli-audio.html" rel="alternate" type="text/html" title="Pure Data e i modelli audio" /><published>2019-01-29T00:00:00+01:00</published><updated>2019-01-29T00:00:00+01:00</updated><id>/pure-data-modelli-audio</id><content type="html" xml:base="/pure-data-modelli-audio.html">&lt;hr /&gt;

&lt;p&gt;&lt;i&gt;Disclaimer: Il contenuto di questo post √® un estratto della mia tesi di laurea, realizzata nel 2006. Alcuni esempi potrebbero risultare obsoleti, tuttavia la componente teorica resta un buon riferimento per chi deve approcciarsi alla materia.&lt;/i&gt;&lt;/p&gt;
&lt;hr /&gt;

&lt;h2 id=&quot;il-modello-di-rotolamento&quot;&gt;Il modello di rotolamento&lt;/h2&gt;

&lt;p&gt;Tra le comuni interazioni meccaniche che coinvolgono oggetti solidi, il
rotolamento forma una categoria interessante anche dal punto di vista
dell‚Äôaudio: l‚Äôesperienza di tutti i giorni ci dice che il suono prodotto
da un oggetto rotolante viene spesso riconosciuto come tale, e in
generale √® distinto da altri suoni come quelli dovuti allo sfregamento
anche degli stessi oggetti. Ci√≤ potrebbe essere dovuto alla natura del
rotolamento come un processo di interazione continua, dove la forza
mutua sugli oggetti coinvolti √® descritta come un impatto senza
l‚Äôaggiunta di forze di frizione perpendicolari. Oltre ad essere
caratteristici, i suoni di rotolamento portano importanti informazioni:
in aggiunta alle caratteristiche di risonanza degli oggetti coinvolti
(che dipendono da forma, dimensione e materiale), altri attributi
vengono espressi nel suono, attributi &lt;em&gt;di trasformazione&lt;/em&gt;, come
velocit√†, gravit√† o accelerazione/decelerazione. Lo sviluppo di un
modello di rotolamento espressivo e in tempo reale da presupposti
fisici, acustici e implementativi √® descritto di seguito.&lt;/p&gt;

&lt;h3 id=&quot;linterazione-di-rotolamento-con-il-modello-di-impatto-come-blocco-di-base&quot;&gt;L‚Äôinterazione di rotolamento con il modello di impatto come blocco di base&lt;/h3&gt;

&lt;figure class=&quot;image&quot;&gt;
  &lt;img src=&quot;images/rolling1.jpg&quot; alt=&quot;Figura 1: Tracciato del movimento di una palla che segue un profilo di
superficie s(x). Non si tratta del moto reale ma di una idealizzazione utile per ricavare la curva usata dal modello di impatto&quot; /&gt;
  &lt;figcaption&gt;Figura 1: Tracciato del movimento di una palla che segue un profilo di
superficie s(x). Non si tratta del moto reale ma di una idealizzazione utile per ricavare la curva usata dal modello di impatto&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Contrariamente ad azioni quali lo sfregare o il grattare, la forza di
interazione dei due oggetti coinvolta in un semplice scenario di
rotolamento (l‚Äôoggetto rotolante e il piano) √® perpendicolare alla
superficie di contatto (la curva media macroscopica), diretta lungo la
linea che connette il punto di contatto e il centro di gravit√†
dell‚Äôoggetto rotolante. Le condizioni di contatto devono essere
modificate per riflettere le varie distanze della superficie di contatto.
L‚Äôoggetto rotolante √® assunto come localmente sferico, senza dettagli
macroscopici sulla superficie. E‚Äô possibile fare queste assunzioni dal
momento che i dettagli microscopici della superficie dell‚Äôoggetto
rotolante possono essere semplicemente aggiunti alla superficie sulla
quale l‚Äôoggetto rotola, e pu√≤ essere variato il raggio di curvatura
della superficie stessa; vedremo che anche l‚Äôassumere un raggio costante
pu√≤ essere soddisfacente per la maggior parte degli scopi. E‚Äô importante
notare che il contatto tra i due oggetti durante il rotolamento √®
ristretto a punti distinti: il piano non viene seguito nella sua
interezza.&lt;/p&gt;

&lt;p&gt;Il movimento reale dell‚Äôoggetto rotolante si differenzia da questa
idealizzazione a causa dell‚Äôelasticit√† e dell‚Äôinerzia. In buona
approssimazione, il movimento verticale del centro della palla √®
calcolato con un modello di impatto unidimensionale con la curva in figura 1. I
punti di contatto e la traiettoria risultante, che idealmente dovrebbe
essere applicata al modello di impatto unidimensionale, sono
rappresentati in figura 2. Il calcolo esatto dei punti di contatto √®
dispendioso in termini di risorse computazionali: in ogni punto &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt;
lungo la curva della superficie, cio√® per ogni punto di campionamento
nel caso discreto (dove la frequenza del campionamento √® la stessa del
campionamento audio), deve essere calcolata la seguente funzione che
descrive l‚Äôattuale punto &lt;script type=&quot;math/tex&quot;&gt;p_x&lt;/script&gt; :&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f_x(p_x) \stackrel{!}{=} max_{q\in[x-r,x+r]}f_x(q) \hspace{0,5cm}, \label{eq:rolling1}&lt;/script&gt;

&lt;p&gt;dove&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f_x(q) = s(q) + \sqrt{r^2 - (q-x)^2} \hspace{0,5cm},\hspace{0,5cm} q \in [x-r, x+r] \hspace{0,5cm}. \label{eq:rolling2}&lt;/script&gt;

&lt;p&gt;La curva ideale viene poi calcolata da questi punti di contatto. Una tecnica pi√π semplice (e quindi anche meno dispendiosa in termini di risorse di calcolo) √® rappresentata in figura 3.
La traiettoria in figura 2 converge alla curva ideale di figura 3 per raggi molto grandi se comparati alla ruvidit√† della superficie. 
Infatti, in una prima implementazione, anche le forti semplificazioni (riportate figura 4 realizzate con un algoritmo molto semplice, hanno dato risultati
convincenti.&lt;/p&gt;

&lt;figure class=&quot;image&quot;&gt;
  &lt;img src=&quot;images/rolling2.jpg&quot; alt=&quot;Figura 2: Tracciato della curva di offset effettiva risultante dalla superficie s(x).&quot; /&gt;
  &lt;figcaption&gt;Figura 2: Tracciato della curva di offset effettiva risultante dalla superficie s(x).&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;figure class=&quot;image&quot;&gt;
  &lt;img src=&quot;images/rolling3.jpg&quot; alt=&quot;Figura 3: Approssimazione del tracciato compiuto dalla palla durante il rotolamento.&quot; /&gt;
  &lt;figcaption&gt;Figura 3: Approssimazione del tracciato compiuto dalla palla durante il rotolamento.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;figure class=&quot;image&quot;&gt;
  &lt;img src=&quot;images/rolling4.jpg&quot; alt=&quot;Figura 4: Ulteriore approssimazione del tracciato di rotolamento.&quot; /&gt;
  &lt;figcaption&gt;Figura 4: Ulteriore approssimazione del tracciato di rotolamento.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h3 id=&quot;sec:superficie&quot;&gt;La superficie&lt;/h3&gt;

&lt;p&gt;Esistono diverse tecniche per realizzare il profilo della superficie
alla base del modello di rotolamento. Una possibilit√† √® quella di
campionare o effettuare una scansione di superfici reali e usare tali
segnali come input per lo stadio seguente del modello; questo approccio
per√≤ non si adatta ai nostri obiettivi: noi siamo interessati ad un
modello parametrico, flessibile ed efficiente piuttosto che ad una
singola simulazione realistica. Inoltre i segnali memorizzati sono
difficili da adattare alle variazioni degli attributi del modello;
preferiamo quindi usare modelli statistici di superfici che possano
efficientemente generare segnali per i vari attributi.&lt;/p&gt;

&lt;p&gt;E‚Äô comune nella computer graphics descrivere le superfici tramite metodi
frattali. L‚Äôapplicazione di questa idea al nostro modello
unidimensionale conduce all‚Äôutilizzo di un segnale di rumore con spettro
di potenza &lt;script type=&quot;math/tex&quot;&gt;1/f^{\beta}&lt;/script&gt;, o equivalentemente rumore bianco filtrato con
queste caratteristiche. Il parametro reale &lt;script type=&quot;math/tex&quot;&gt;\beta&lt;/script&gt; riflette la
dimensione frattale (o ruvidit√†). I risultati pratici di questo tipo di
modello sono diventati pi√π convincenti quando la banda del segnale della
superficie √® stata fortemente limitata; ci√≤ non deve sorprendere se
pensiamo che solitamente le superfici coinvolte nel rotolamento sono
molto smussate. Smussare su larga scala (che pu√≤ essere assimilato al
tagliare pezzi di pietra per pavimentazioni) corrisponde ad un
filtraggio passa‚Äìalto, mentre smussare a livello microscopico (come
lucidare una pietra) pu√≤ essere visto come un filtraggio di tipo
passa‚Äìbasso. Tramite queste elaborazioni per√≤ si possono perdere le
caratteristiche del rumore &lt;script type=&quot;math/tex&quot;&gt;1/f^{\beta}&lt;/script&gt; di partenza. Perci√≤ optiamo per
una approssimazione di questa curva con un filtro del secondo ordine la
cui ripidit√† √® proporzionale al grado di ruvidit√† a livello
microscopico.&lt;/p&gt;

&lt;p&gt;Tutte le frequenze in questo modello di basso livello devono variare
proporzionalmente ai parametri di velocit√†, perci√≤ l‚Äôampiezza del
segnale di superficie deve essere mantenuta costante. Naturalmente i
parametri dell‚Äôimpatto, in particolare la costante di elasticit√† &lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt;,
devono essere variati opportunamente a seconda della superficie che si
vuole simulare (cio√® in base alle propriet√† del materiale), in quanto
contribuiscono fortemente alla espressivit√† del modello.&lt;/p&gt;

&lt;h3 id=&quot;il-modello-di-impatto&quot;&gt;Il modello di impatto&lt;/h3&gt;

&lt;p&gt;Un suono di contatto √® descritto tramite due sistemi, uno per l‚Äôoggetto
risonante e uno per l‚Äôoggetto percussore. Supposto che la superficie di
contatto sia piccola, la forza di contatto viene espressa come:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
f(x(t),v(t)) = \left\{
                                            \begin{array}{ll}
                                            kx(t)^{\alpha}+\lambda x(t)^{\alpha}\cdot v(t) = kx(t)^{\alpha}(1+\mu v(t)) &amp; x &gt; 0\\
                                            0 &amp; x \leq 0
                                            \end{array}
                                \right.
\hspace{0,5cm}, \label{eq:impact1} %]]&gt;&lt;/script&gt;

&lt;p&gt;dove &lt;script type=&quot;math/tex&quot;&gt;v(t) = \dot{x}(t)&lt;/script&gt; √® la velocit√† di compressione, &lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt; √® il
coefficiente di rigidit√†, &lt;script type=&quot;math/tex&quot;&gt;\alpha&lt;/script&gt; √® un parametro che descrive la
geometria locale dell‚Äôimpatto (nel caso di due perfette sfere vale 1.5),
&lt;script type=&quot;math/tex&quot;&gt;\lambda&lt;/script&gt; √® un coefficiente di smorzamento e &lt;script type=&quot;math/tex&quot;&gt;\mu = \lambda/k&lt;/script&gt; √® un
termine matematico (senza significato fisico) detto &lt;em&gt;caratteristica
viscoelastica&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Il percussore √® considerato una massa ideale, quindi l‚Äôunico parametro
che lo caratterizza √® la massa; il risonatore invece √® un oggetto modale
ed √® caratterizzato dai parametri di frequenza, tempi di decadimento,
&lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;\alpha&lt;/script&gt; e &lt;script type=&quot;math/tex&quot;&gt;\lambda&lt;/script&gt;. Si assume inoltre che il percussore abbia un
elevato coefficiente di smorzamento: in tal modo diventa trascurabile
l‚Äôenergia acustica delle sue vibrazioni, e l‚Äôenergia viene trasferita al
risonatore che emette il suono. Per una descrizione matematica vengono
sintetizzati i modi di vibrazione (teoricamente infiniti), ognuno dei
quali fornisce un contributo allo spettro del segnale &lt;a class=&quot;citation&quot; href=&quot;#art:soundobj&quot;&gt;(Avanzini, F., Rath, M., &amp;amp; Rocchesso, D., 2003)&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&quot;caratteristiche-di-alto-livello&quot;&gt;Caratteristiche di alto livello&lt;/h3&gt;

&lt;p&gt;Oltre ai parametri di basso livello visti nella sezione precedente, i
tipici moti di rotolamento posseggono caratteristiche a livello
macroscopico che contribuiscono fortemente alla percezione acustica, e
non possono essere descritti come fatto in precedenza. Molte superfici
contengono dei pattern pi√π o meno regolari che non possono essere
classificati come rumore frattale filtrato, e tali periodicit√† possono
essere verificate attraverso l‚Äôesperienza di tutti i giorni: i pavimenti
in pietra, o i solchi pseudoperiodici in molte tavole di legno. Le
singole irregolarit√† sulla superficie dell‚Äôoggetto rotolante possono
essere raggruppate in una sola categoria, dal momento che sono
richiamate periodicamente nel movimento rotatorio. Tale caratteristica
pu√≤ essere modellata con segnali impulsivi di frequenza costante o
variante in un piccolo intervallo; potrebbero essere utili delle
approssimazioni sinusoidali o polinomiali, con un parametro di
smussamento legato al grado di approssimazione della funzione. Ancora,
le frequenze devono variare proporzionalmente alla velocit√†.&lt;/p&gt;

&lt;p&gt;Dev‚Äôessere fatta un‚Äôaltra osservazione a livello macroscopico: per
oggetti rotolanti che non sono perfettamente sferici (in maniera
rilevante per il movimento) la velocit√† del punto di contatto su
entrambe le superfici e l‚Äôeffettiva forza che preme l‚Äôoggetto rotolante
sulla superficie variano periodicamente; devono essere variati questi
due parametri per modellare tale deviazione dalla sfericit√† perfetta.&lt;/p&gt;

&lt;p&gt;Infine notiamo che, come nell‚Äôascolto di tutti i giorni, gli scenari
acustici del rotolamento di oggetti sono riconosciuti e accettati pi√π
facilmente se sono presenti dinamiche tipiche; ad esempio pensiamo al
suono di una palla che cade e che rimbalza fino a quando non raggiunge
un contatto costante con il suolo: a questo punto il rotolamento diventa
chiaro dal punto di vista uditivo e la velocit√† media lentamente
diminuisce fino diventare nulla.&lt;/p&gt;

&lt;h2 id=&quot;sec:tessiture_superficie&quot;&gt;Tessiture della superficie&lt;/h2&gt;

&lt;p&gt;Molti dei suoni di contatto ai quali siamo interessati non possono
essere ricreati in modo convincente usando solo modelli deterministici,
come nel caso dei suoni di rotolamento risultanti dalla sequenza di
micro impatti tra due oggetti risonanti, determinati dal profilo della
superficie di contatto. Affrontiamo quindi il problema di effettuare il
rendering delle tessiture di superfici attraverso processi frattali;
tali processi sono molto usati nella computer graphics, dal momento che
forniscono tessiture che sembrano naturali all‚Äôocchio umano. Dato che
nei modelli fisici le propriet√† delle superfici vengono tradotte
direttamente in segnali di forza e, di conseguenza, in suoni, sembra
naturale seguire lo stesso approccio per modellare le superfici.&lt;/p&gt;

&lt;p&gt;I frattali sono definiti &lt;a class=&quot;citation&quot; href=&quot;#book:fractal&quot;&gt;(Hastings, H. M. &amp;amp; Sugihara, G., 1993)&lt;/a&gt; come geometrie invarianti
rispetto alla scalatura. Sono auto‚Äìsimili se la scalatura √® isotropica
o uniforme in tutte le direzioni, auto‚Äìaffini se la scalatura √®
anisotropica o dipendente dalla direzione, staticamente auto‚Äìsimili se
sono l‚Äôunione di copie di se stessi scalate statisticamente. Pi√π
formalmente, un processo frattale unidimensionale pu√≤ essere definito
come una generalizzazione della definizione di moto standard Browniano
&lt;a class=&quot;citation&quot; href=&quot;#book:brownian&quot;&gt;(Resnick, S., 1992)&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;Il processo stocastico &lt;script type=&quot;math/tex&quot;&gt;x = \{x(t),t \geq 0\}&lt;/script&gt; √® un moto standard Browniano se:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;il processo stocastico &lt;script type=&quot;math/tex&quot;&gt;x&lt;/script&gt; ha incrementi indipendenti;&lt;/li&gt;
  &lt;li&gt;vale la propriet√†
&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
x(t) - x(s) \sim N(0,t-s) \hspace{0,5cm} per \hspace{0,5cm} 0 \leq s &lt; t; %]]&gt;&lt;/script&gt;
cio√® l‚Äôincremento &lt;script type=&quot;math/tex&quot;&gt;x(t) - x(s)&lt;/script&gt; √® normalmente distribuito con media
nulla e varianza &lt;script type=&quot;math/tex&quot;&gt;(t-s)&lt;/script&gt;;&lt;/li&gt;
  &lt;li&gt;√® vero che &lt;script type=&quot;math/tex&quot;&gt;x(0) = 0.&lt;/script&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;La definizione di moto standard Browniano pu√≤ essere generalizzata alla
definizione di &lt;em&gt;processo frattale&lt;/em&gt; se l‚Äôincremento &lt;script type=&quot;math/tex&quot;&gt;x(t)-x(s)&lt;/script&gt; √®
normalmente distribuito con media 0 e varianza proporzionale a
&lt;script type=&quot;math/tex&quot;&gt;(t-s)^{2H}&lt;/script&gt;. Il parametro &lt;em&gt;H&lt;/em&gt; √® chiamato &lt;em&gt;esponente di Hurst&lt;/em&gt; e
caratterizza il comportamento del processo frattale rispetto alla
scalatura: se &lt;script type=&quot;math/tex&quot;&gt;x=\{x(t),t \geq 0\}&lt;/script&gt; √® un processo frattale con esponente
di Hurst &lt;script type=&quot;math/tex&quot;&gt;H&lt;/script&gt;, allora, per ogni reale &lt;script type=&quot;math/tex&quot;&gt;a &gt; 0&lt;/script&gt;, obbedisce alla seguente
relazione di scala:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;x(t) \stackrel{P}{=} a^{-H}x(at) \hspace{0,5cm} , \label{eq:surface1}&lt;/script&gt;

&lt;p&gt;dove &lt;script type=&quot;math/tex&quot;&gt;\stackrel{P}{=}&lt;/script&gt; denota l‚Äôuguaglianza statistica. Questa √® la
definizione formale di &lt;em&gt;auto‚Äìsimilirarit√† statistica&lt;/em&gt;. La famiglia di
processi &lt;script type=&quot;math/tex&quot;&gt;1/f&lt;/script&gt; statisticamente auto‚Äìsimili, nota anche come rumore &lt;script type=&quot;math/tex&quot;&gt;1/f&lt;/script&gt;, √®
composta da processi aventi densit√† di spettro di potenza &lt;script type=&quot;math/tex&quot;&gt;S_x(\omega)&lt;/script&gt;
proporzionale a &lt;script type=&quot;math/tex&quot;&gt;1/ \omega^{\beta}&lt;/script&gt;, con &lt;script type=&quot;math/tex&quot;&gt;\beta&lt;/script&gt; legato all‚Äôesponente di
Hurst &lt;script type=&quot;math/tex&quot;&gt;H&lt;/script&gt; dalla relazione &lt;script type=&quot;math/tex&quot;&gt;\beta = 2H + 1&lt;/script&gt;. Per &lt;script type=&quot;math/tex&quot;&gt;\beta = 0&lt;/script&gt; la
definizione corrisponde al rumore bianco, per &lt;script type=&quot;math/tex&quot;&gt;\beta = 2&lt;/script&gt; si ottiene il
rumore Browniano, e per &lt;script type=&quot;math/tex&quot;&gt;\beta = 1&lt;/script&gt; il rumore risultante √® rumore rosa.
Il parametro &lt;script type=&quot;math/tex&quot;&gt;\beta&lt;/script&gt; √® in relazione anche con la dimensione frattale. La
dimensione frattale &lt;a class=&quot;citation&quot; href=&quot;#book:wornell&quot;&gt;(Wornell, G. W., 1998)&lt;/a&gt; di una funzione √® un parametro reale
che determina l‚Äôirregolarit√† di un oggetto frattale, √® legata al grafico
della funzione ed √® usata nella computer graphics per controllare la
ruvidit√† percepita &lt;a class=&quot;citation&quot; href=&quot;#art:pentland&quot;&gt;(Pentland, A. P., 1988)&lt;/a&gt;. Per i processi &lt;script type=&quot;math/tex&quot;&gt;1/f&lt;/script&gt;, tale dimensione
√® inversamente proporzionale all‚Äôesponente di Hurst &lt;script type=&quot;math/tex&quot;&gt;H&lt;/script&gt;: valori maggiori
di &lt;script type=&quot;math/tex&quot;&gt;H&lt;/script&gt; corrispondono a valori minori della dimensione frattale; &lt;script type=&quot;math/tex&quot;&gt;H&lt;/script&gt; √®
proporzionale a &lt;script type=&quot;math/tex&quot;&gt;\beta&lt;/script&gt;. Perci√≤, incrementando &lt;script type=&quot;math/tex&quot;&gt;\beta&lt;/script&gt; possiamo
raggiungere una redistribuzione della potenza dalle alte alle basse
frequenze, con uno smussamento complessivo della forma d‚Äôonda.&lt;/p&gt;

&lt;p&gt;Il problema di generare il rumore &lt;script type=&quot;math/tex&quot;&gt;1/f&lt;/script&gt; √® stato trattato estensivamente.
Uno degli approcci pi√π comuni risulta quello di filtrare una sorgente di
rumore bianco per ottenere lo spettro &lt;script type=&quot;math/tex&quot;&gt;1/f&lt;/script&gt;; seguendo questo procedimento
utilizzeremo il modello riportato in &lt;a class=&quot;citation&quot; href=&quot;#art:saletti&quot;&gt;(Saletti, R., Novembre 1986)&lt;/a&gt; e &lt;a class=&quot;citation&quot; href=&quot;#art:corsini&quot;&gt;(Corsini, G. &amp;amp; Saletti, R., Dicembre 1988)&lt;/a&gt;. Il
filtro √® una cascata di N filtri del primo ordine, ognuno con una coppia
di poli e zeri; la funzione di trasferimento &lt;script type=&quot;math/tex&quot;&gt;H(s)&lt;/script&gt; nel dominio di
Laplace √® la seguente:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;H(s)=A\frac{\prod_{i=1}^{N}(s-s_{0i})}{\prod_{i=1}^{N}(s-s_{pi})} \hspace{0,5cm} ,  \label{eq:surface2}&lt;/script&gt;

&lt;p&gt;dove &lt;script type=&quot;math/tex&quot;&gt;A&lt;/script&gt; √® una costante. Il generatore di rumore frattale √® ottenuto
impostando opportunamente i poli e gli zeri dei filtri nella cascata
&lt;a class=&quot;citation&quot; href=&quot;#art:saletti&quot;&gt;(Saletti, R., Novembre 1986)&lt;/a&gt;. In particolare, il polo e lo zero alle frequenze
&lt;script type=&quot;math/tex&quot;&gt;f_{pi}&lt;/script&gt; e &lt;script type=&quot;math/tex&quot;&gt;f_{0i}&lt;/script&gt; possono essere computati come funzioni di &lt;script type=&quot;math/tex&quot;&gt;\beta&lt;/script&gt;
con le seguenti formule:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f_{pi} = -\frac{s_{pi}}{2\pi} = f_{p(i-1)}10^{\frac{1}{h}} \hspace{0,5cm} , \label{eq:surface3a}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;f_{0i} = - \frac{s_{0i}}{2\pi} = f_{pi}10^{\frac{\beta}{2h}} \hspace{0,5cm} , \label{eq:surface3b}&lt;/script&gt;

&lt;p&gt;dove &lt;script type=&quot;math/tex&quot;&gt;f_{p1}&lt;/script&gt; √® il polo di frequenza pi√π bassa del filtro; perci√≤ il
limite inferiore della banda di frequenza per l‚Äôapprossimazione √®
&lt;script type=&quot;math/tex&quot;&gt;f_{p1}&lt;/script&gt;. La densit√† &lt;script type=&quot;math/tex&quot;&gt;h&lt;/script&gt; (densit√† dei poli per decade di frequenze) pu√≤
essere usata per controllare l‚Äôerrore tra lo spettro desiderato e lo
spettro approssimato ottenuto dal filtraggio del rumore bianco. La
dipendenza dell‚Äôerrore in relazione alla densit√† dei poli del filtro √®
discussa in &lt;a class=&quot;citation&quot; href=&quot;#art:corsini&quot;&gt;(Corsini, G. &amp;amp; Saletti, R., Dicembre 1988)&lt;/a&gt;. La figura mostra uno spettro &lt;script type=&quot;math/tex&quot;&gt;1/f^{\beta}&lt;/script&gt; ottenuto usando
il filtro &lt;script type=&quot;math/tex&quot;&gt;f_{pi}&lt;/script&gt;, con due diversi valori per &lt;script type=&quot;math/tex&quot;&gt;h&lt;/script&gt;.&lt;/p&gt;

&lt;figure class=&quot;image&quot;&gt;
  &lt;img src=&quot;images/noise.jpg&quot; alt=&quot;Spettro di ampiezza del rumore frattale generato con $$\beta=1.81$$, $$h=2$$ a sinistra e $$h=6$$ a destra.&quot; /&gt;
  &lt;figcaption&gt;Spettro di ampiezza del rumore frattale generato con $$\beta=1.81$$, $$h=2$$ a sinistra e $$h=6$$ a destra.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;La funzione di trasferimento nel dominio discreto del tempo pu√≤ essere
computata con il metodo della varianza della risposta all‚Äôimpulso
 &lt;a class=&quot;citation&quot; href=&quot;#book:mitra&quot;&gt;(Mitra, S. K., 1998)&lt;/a&gt;; ci√≤ corrisponde a mappare poli e zeri della funzione di
trasferimento &lt;script type=&quot;math/tex&quot;&gt;H(s)&lt;/script&gt; su poli e zeri della funzione di trasferimento
&lt;script type=&quot;math/tex&quot;&gt;H(z)&lt;/script&gt; nel dominio discreto del tempo attraverso la seguente
sostituzione:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;s-s_x \rightarrow 1-e^{s_xT_s}z^{-1} \hspace{0,5cm} , \label{eq:surface4}&lt;/script&gt;

&lt;p&gt;dove &lt;script type=&quot;math/tex&quot;&gt;T_s&lt;/script&gt; √® il periodo di campionamento e &lt;script type=&quot;math/tex&quot;&gt;s_x&lt;/script&gt; indica un polo &lt;script type=&quot;math/tex&quot;&gt;s_{pi}&lt;/script&gt;
o uno zero &lt;script type=&quot;math/tex&quot;&gt;s_{0i}&lt;/script&gt;. Si ottiene la seguente funzione di trasferimento
discreta:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;H(z)=A' \frac { \prod^{N}_{i=1}1-e^{s_{0i}T}z^{-1} }{ \prod^{N}_{i=1}1-e^{s_{pi}T}z^{-1} } \hspace{0,5cm} , \label{eq:surface5}&lt;/script&gt;

&lt;p&gt;dove &lt;script type=&quot;math/tex&quot;&gt;A'&lt;/script&gt; √® una costante di normalizzazione. In conclusione, lo spettro
&lt;script type=&quot;math/tex&quot;&gt;1/f^{\beta}&lt;/script&gt; √® approssimato da una cascata di filtri del primo ordine,
ognuno con la seguente funzione di trasferimento discreta:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
H^{(i)}(z)=\frac{1+b_iz^{-1}}{1+a_iz^{-1}} \hspace{0,5cm} , \hspace{0,5cm} con \hspace{0,5cm}
 \left\{
 \begin{array}{ll}
 a_i=e^{-2{\pi}f_{pi}T}, &amp; b_i=e^{-2{\pi}f_{0i}T} \\
 \\
 f_{pi}=f_{p(i-1)}10^{\frac{1}{h}}, &amp; f_{0i}=f_{pi}10^{\frac{\beta}{2h}} \\
 \end{array}
 %\hspace{0,5cm}
 \right .
 \label{eq:surface6} %]]&gt;&lt;/script&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;implementazioni-dei-modelli-in-pure-data&quot;&gt;Implementazioni dei modelli in Pure Data&lt;/h2&gt;

&lt;h3 id=&quot;cos√®-pure-data&quot;&gt;Cos‚Äô√® Pure Data&lt;/h3&gt;

&lt;figure class=&quot;image&quot;&gt;
  &lt;img src=&quot;images/pd1_bn.jpg&quot; alt=&quot;L'interfaccia grafica di Pure Data&quot; /&gt;
  &lt;figcaption&gt;L'interfaccia grafica di Pure Data&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Pure Data √® un software ideato da Miller Puckette: si tratta di un
ambiente di programmazione visuale in real‚Äìtime per l‚Äôelaborazione di
audio e grafica, basato sul sistema &lt;em&gt;Max/MSP&lt;/em&gt; ma pi√π semplice e
portabile di questo. Sono presenti due caratteristiche in PD molto
importanti: la possibilit√† di gestire contemporaneamente la simulazione
video e la simulazione audio utilizzando il pacchetto &lt;em&gt;GEM&lt;/em&gt; di Mark Dank
e delle facilitazioni nelle definizioni nell‚Äôaccesso alle strutture
dati.&lt;/p&gt;

&lt;p&gt;Ogni documento di PD √® chiamato &lt;em&gt;patch&lt;/em&gt;; una volta che tale file viene
aperto si presenta composto di una finestra principale e di eventuali
sotto‚Äìfinestre (che possono essere visualizzate o nascoste ma sono
sempre in esecuzione). In ogni finestra compaiono dei blocchi collegati
tra loro; i blocchi possono essere di quattro tipi:&lt;/p&gt;

&lt;h4 id=&quot;oggetti&quot;&gt;Oggetti&lt;/h4&gt;
&lt;p&gt;Un oggetto viene creato scrivendo del testo all‚Äôinterno del blocco;
il testo viene diviso in &lt;em&gt;atomi&lt;/em&gt;: il primo atomo definisce il tipo
di oggetto che viene creato, i successivi costituiscono gli
argomenti di creazione, i quali servono ad inizializzare l‚Äôoggetto.&lt;/p&gt;

&lt;p&gt;Ogni oggetto pu√≤ possedere zero o pi√π &lt;em&gt;inlet&lt;/em&gt; (collegamenti in
input) e zero o pi√π &lt;em&gt;outlet&lt;/em&gt; (collegamenti in output); il numero di
questi dipende dal tipo di oggetto. Ci sono due tipi di
collegamento: &lt;em&gt;collegamenti di segnale&lt;/em&gt; e &lt;em&gt;collegamenti di
controllo&lt;/em&gt;; i primi sono rappresentati da una linea marcata, mentre
i secondi da una linea sottile. La scelta del tipo di collegamento
dipende dall‚Äôoutlet dal quale provengono; un outlet pu√≤ essere
collegato ad un inlet solo se entrambi accettano collegamenti dello
stesso tipo (o entrambi di segnale o entrambi di controllo).&lt;/p&gt;

&lt;p&gt;In figura √® riportato un esempio di oggetto: l‚Äôatomo 1+1 definisce
la tipologia di oggetto (un blocco sommatore), mentre il secondo
atomo 13 indica il valore da sommare all‚Äôingresso.&lt;/p&gt;

&lt;figure class=&quot;image&quot;&gt;
  &lt;img src=&quot;images/obj1.jpg&quot; alt=&quot;&quot; /&gt;
  &lt;figcaption&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h4 id=&quot;messaggi&quot;&gt;Messaggi&lt;/h4&gt;
&lt;p&gt;I blocchi di messaggio interpretano il testo come un messaggio da
inviare ogni volta che il blocco viene attivato; l‚Äôinvio √® verso il
blocco al quale l‚Äôoutlet √® collegato e pu√≤ avvenire un numero
qualsiasi di volte durante l‚Äôesecuzione della patch. Il blocco di
messaggio possiede sempre un inlet e un outlet. Nell‚Äôesempio
seguente il primo blocco, quando viene attivato dal click del mouse,
invia il messaggio 21 all‚Äôoggetto che lo sommer√† a 13 all‚Äôultimo
blocco viene inviato il risultato dell‚Äôoperazione.&lt;/p&gt;

&lt;figure class=&quot;image&quot;&gt;
  &lt;img src=&quot;images/mess1.jpg&quot; alt=&quot;Semplice esempio di patch per Pure Data.&quot; /&gt;
  &lt;figcaption&gt;Semplice esempio di patch per Pure Data.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Un messaggio pu√≤ essere attivato cliccandoci sopra, da un altro
messaggio in ingresso o da un particolare blocco chiamato &lt;em&gt;bang&lt;/em&gt;.&lt;/p&gt;

&lt;h4 id=&quot;gui&quot;&gt;GUI&lt;/h4&gt;
&lt;p&gt;Il terzo blocco dell‚Äôesempio precedente fa parte dei blocchi GUI
(graphical user interface); tra questi sono inclusi i blocchi
numerici, blocchi contenenti simboli, controlli scorrevoli e
pulsanti. Mentre gli oggetti rimangono immutati durante l‚Äôesecuzione
di una patch, i blocchi GUI aggiornano il loro stato in base al
valore che contengono.&lt;/p&gt;

&lt;h4 id=&quot;commenti&quot;&gt;Commenti&lt;/h4&gt;

&lt;p&gt;I commenti sono costituiti da semplice testo e non sono contenuti
all‚Äôinterno di nessun rettangolo. Nella figura precedente i
testi alla destra dei blocchi sono commenti.&lt;/p&gt;

&lt;p&gt;Una patch pu√≤ essere in modalit√† &lt;em&gt;edit&lt;/em&gt; oppure in modalit√† &lt;em&gt;running&lt;/em&gt;:
nel primo caso la patch non √® in esecuzione ed √® permessa la creazione o
modifica dei blocchi e dei collegamenti; nel secondo caso la patch √® in
esecuzione, √® possibile ancora modificare i collegamenti, mentre la
modifica dei blocchi GUI ha l‚Äôeffetto di variare i parametri di
controllo della patch.&lt;/p&gt;

&lt;h3 id=&quot;lincapsulamento-in-pure-data&quot;&gt;L‚Äôincapsulamento in Pure Data&lt;/h3&gt;

&lt;p&gt;Come avviene con i linguaggi di programmazione quali C, C++ e Java, con
Pure Data √® possibile scrivere del codice che pu√≤ poi essere
riutilizzato in qualsiasi momento; uno o pi√π oggetti infatti possono
essere costituiti da &lt;em&gt;subpatch&lt;/em&gt;, ovvero delle patch separate che vengono
incapsulate all‚Äôinterno dell‚Äôoggetto. Si possono distinguere due tipi di
incapsulamento:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;one‚Äìoff subpatch ‚Äì se l‚Äôoggetto viene chiamato &lt;code class=&quot;highlighter-rouge&quot;&gt;pd&lt;/code&gt; o &lt;code class=&quot;highlighter-rouge&quot;&gt;pd my-name&lt;/code&gt;, viene creata una subpatch il cui contenuto viene salvato come parte della patch genitore che pu√≤ essere riutilizzata e modificata pi√π volte all‚Äôinterno di quest‚Äôultima;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;astrazione ‚Äì se l‚Äôoggetto ha il nome di una patch gi√† presente come file (omettendo l‚Äôestensione &lt;code class=&quot;highlighter-rouge&quot;&gt;.pd&lt;/code&gt;), PD caricher√† il contenuto del file all‚Äôinterno della subpatch; in tal caso un cambiamento alla patch si propaga a tutte le chiamate alla sua astrazione.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Per definire il numero di inlet e outlet che deve possedere l‚Äôoggetto
contenente la subpatch √® sufficiente utilizzare all‚Äôinterno di
quest‚Äôultima i blocchi &lt;code class=&quot;highlighter-rouge&quot;&gt;inlet&lt;/code&gt; e &lt;code class=&quot;highlighter-rouge&quot;&gt;outlet&lt;/code&gt; (oppure &lt;code class=&quot;highlighter-rouge&quot;&gt;inlet~&lt;/code&gt; e &lt;code class=&quot;highlighter-rouge&quot;&gt;outlet~&lt;/code&gt;
per i collegamenti di segnale).&lt;/p&gt;

&lt;h3 id=&quot;gestione-dei-segnali-audio&quot;&gt;Gestione dei segnali audio&lt;/h3&gt;

&lt;figure class=&quot;image&quot;&gt;
  &lt;img src=&quot;images/logicaltime.jpg&quot; alt=&quot;Linee del tempo per la computazione audio e la computazione di controllo in Pure Data con (a) blocchi di un campione e (b) blocchi di quattro campioni.&quot; /&gt;
  &lt;figcaption&gt;Linee del tempo per la computazione audio e la computazione di controllo in Pure Data con (a) blocchi di un campione e (b) blocchi di quattro campioni.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;In Pure Data i segnali audio vengono memorizzati come numeri in virgola
mobile a 32 bit; a seconda dell‚Äôhardware utilizzato per√≤ l‚Äôoutput viene
limitato a 16 o 24 bit. L‚Äôinput √® sempre compreso tra i valori 1 e -1,
mentre l‚Äôoutput viene tagliato al fine di restare compreso tra questi
due limiti. La frequenza di campionamento di default √® 44100 Hz
(modificabile da riga di comando o nel men√π &lt;em&gt;audio setup&lt;/em&gt;).&lt;/p&gt;

&lt;p&gt;Le computazioni audio vengono eseguite dai &lt;em&gt;blocchi tilde&lt;/em&gt;, cio√® quelli
che, per convenzione, hanno il nome seguito da una tilde, come &lt;code class=&quot;highlighter-rouge&quot;&gt;sc~&lt;/code&gt;;
essi comunicano attraverso connessioni di segnale. All‚Äôavvio della
computazione, o quando vengono cambiati i collegamenti, gli oggetti
tilde vengono ordinati secondo un ordine di esecuzione lineare; tale
lista viene poi eseguita in blocchi di 64 campioni ciascuno (a 44.1 KHz
significa che l‚Äôintera rete di blocchi audio viene eseguita una volta
ogni 1.45 millisecondi). Le connessioni nella rete audio devono essere
acicliche; la presenza di eventuali cicli viene rilevata al momento del
riordino dei blocchi. Ogni subpatch pu√≤ avere dei collegamenti di
segnale in entrata e in uscita tramite i blocchi ‚Äúinlet&lt;script type=&quot;math/tex&quot;&gt;\sim&lt;/script&gt;‚Äù e
‚Äúoutlet&lt;script type=&quot;math/tex&quot;&gt;\sim&lt;/script&gt;‚Äù.&lt;/p&gt;

&lt;p&gt;La computazione dei segnali non avviene in &lt;em&gt;real time&lt;/em&gt;, ma in &lt;em&gt;logical
time&lt;/em&gt;: quest‚Äôultimo √® definito come l‚Äôistante del successivo campione
audio che verr√† elaborato, ed √® sempre precedente al real time, definito
come l‚Äôistante in cui il campione arriva all‚Äôoutput. Tutto questo serve
a far s√¨ che la computazione audio sia indipendente dal tempo effettivo
di esecuzione del processore, il quale pu√≤ variare per molte ragioni. Si
pu√≤ dedurre che una computazione audio, se eseguita nel modo corretto, √®
deterministica: due esecuzioni dello stesso calcolo, una in tempo reale
e l‚Äôaltra no, devono dare lo stesso risultato. In
figura si vede come la computazione dell‚Äôaudio
viene svolta rispetto all‚Äôelaborazione dei segnali di controllo: i
campioni audio vengono calcolati a scadenze regolari, ma prima di ogni
scadenza devono essere effettuati tutti i calcoli di controllo che
possono influenzare il campione audio in quella scadenza. Se &lt;script type=&quot;math/tex&quot;&gt;N&lt;/script&gt; √® il
numero di campioni in un blocco, la prima computazione audio riguarda i
campioni da &lt;script type=&quot;math/tex&quot;&gt;0&lt;/script&gt; a &lt;script type=&quot;math/tex&quot;&gt;N-1&lt;/script&gt;, i quali vengono inviati in output tutti insieme
all‚Äôistante &lt;script type=&quot;math/tex&quot;&gt;N&lt;/script&gt; (logical time); prima di questo istante vengono
effettuate tutte le elaborazioni di controllo per gli &lt;script type=&quot;math/tex&quot;&gt;N&lt;/script&gt; campioni.&lt;/p&gt;

&lt;h4 id=&quot;conversione-tra-segnali-audio-e-segnali-di-controllo&quot;&gt;Conversione tra segnali audio e segnali di controllo&lt;/h4&gt;

&lt;p&gt;La conversione da segnale di controllo a segnale audio √® possibile
utilizzando l‚Äôoggetto &lt;code class=&quot;highlighter-rouge&quot;&gt;sig~&lt;/code&gt;. Per quanto riguarda la conversione inversa
(da segnale a controllo) deve essere specificato l‚Äôistante nel quale il
segnale viene campionato; questo pu√≤ essere gestito tramite l‚Äôoggetto
&lt;code class=&quot;highlighter-rouge&quot;&gt;snapshot~&lt;/code&gt; che campiona il segnale ogni volta che riceve in input un
&lt;em&gt;bang&lt;/em&gt;. Gli oggetti &lt;code class=&quot;highlighter-rouge&quot;&gt;+~&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;-~&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;*~&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;/~&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;osc~&lt;/code&gt; e &lt;code class=&quot;highlighter-rouge&quot;&gt;phasor~&lt;/code&gt; possono
essere configurati per accettare entrambi i tipi di segnale.&lt;/p&gt;

&lt;h4 id=&quot;selettori-e-blocchi&quot;&gt;Selettori e blocchi&lt;/h4&gt;

&lt;p&gt;Gli oggetti &lt;code class=&quot;highlighter-rouge&quot;&gt;switch~&lt;/code&gt; e &lt;code class=&quot;highlighter-rouge&quot;&gt;block~&lt;/code&gt; sono utilizzati per attivare o
disattivare parti della computazione audio e per controllare la
dimensione dei blocchi di calcolo; deve essere presente uno solo dei due
oggetti per ogni finestra della patch e il suo effetto verr√† esteso a
tutte le subpatch. Entrambi accettano due argomenti per la loro
costruzione: il primo √® la dimensione del blocco e il secondo un fattore
di sovrapposizione.&lt;/p&gt;

&lt;p&gt;L‚Äôoggetto &lt;code class=&quot;highlighter-rouge&quot;&gt;switch~&lt;/code&gt; pu√≤ essere usato per ridurre il carico
computazionale scegliendo, ad esempio, uno tra diversi algoritmi di
sintesi da utilizzare: per farlo √® sufficiente che ogni algoritmo sia
implementato in una subpatch diversa.&lt;/p&gt;

&lt;h4 id=&quot;connessioni-esterne&quot;&gt;Connessioni esterne&lt;/h4&gt;

&lt;p&gt;I segnali possono essere inviati non solo tra blocchi di una stessa
finestra, ma anche tra finestre diverse oppure possono essere dati in
input all‚Äôalgoritmo che li ha generati in una configurazione in
retroazione. Questo pu√≤ essere implementato attraverso tre coppie di
oggetti:&lt;/p&gt;

&lt;dl&gt;
  &lt;dt&gt;throw&lt;script type=&quot;math/tex&quot;&gt;\sim&lt;/script&gt;/catch&lt;script type=&quot;math/tex&quot;&gt;\sim&lt;/script&gt; ‚Äì&lt;/dt&gt;
  &lt;dd&gt;
    &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;throw~&lt;/code&gt; accumula dati in un bus, mentre &lt;code class=&quot;highlighter-rouge&quot;&gt;catch~&lt;/code&gt; legge i dati
accumulati e riazzera il bus per il ciclo successivo;&lt;/p&gt;
  &lt;/dd&gt;
  &lt;dt&gt;send&lt;script type=&quot;math/tex&quot;&gt;\sim&lt;/script&gt;/receive&lt;script type=&quot;math/tex&quot;&gt;\sim&lt;/script&gt; ‚Äì&lt;/dt&gt;
  &lt;dd&gt;
    &lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;send~&lt;/code&gt; salva un segnale che pu√≤ essere ricevuto pi√π volte da un
blocco &lt;code class=&quot;highlighter-rouge&quot;&gt;receive~&lt;/code&gt;, il quale per√≤ pu√≤ leggere un solo &lt;code class=&quot;highlighter-rouge&quot;&gt;send~&lt;/code&gt; alla
volta;&lt;/p&gt;
  &lt;/dd&gt;
  &lt;dt&gt;delread&lt;script type=&quot;math/tex&quot;&gt;\sim&lt;/script&gt;/delwrite&lt;script type=&quot;math/tex&quot;&gt;\sim&lt;/script&gt; ‚Äì&lt;/dt&gt;
  &lt;dd&gt;
    &lt;p&gt;se viene inviato un segnale ad un punto precedente nella rete audio,
esso viene ricevuto solo al ciclo successivo, con un ritardo quindi
di 1.45 millisecondi (con le impostazioni di default). Gli oggetti
&lt;code class=&quot;highlighter-rouge&quot;&gt;delread~&lt;/code&gt; e &lt;code class=&quot;highlighter-rouge&quot;&gt;delwrite~&lt;/code&gt; permettono di ridurre al minimo tale
ritardo.&lt;/p&gt;
  &lt;/dd&gt;
&lt;/dl&gt;

&lt;h4 id=&quot;scheduling&quot;&gt;Scheduling&lt;/h4&gt;

&lt;p&gt;Lo scheduler di Pure Data cerca di mantenere un certo &lt;em&gt;vantaggio&lt;/em&gt; sul
calcolo in modo da poter assorbire eventuali forti incrementi nel carico
computazionale; tale comportamento pu√≤ essere impostato tramite le flag
‚Äúaudiobuffer‚Äù o ‚Äúfrags‚Äù.&lt;/p&gt;

&lt;p&gt;Se durante l‚Äôelaborazione dell‚Äôaudio si accumulano dei ritardi, possono
verificarsi delle interruzioni nei flussi di input e output; tuttavia lo
streaming su disco non viene influenzato.&lt;/p&gt;

&lt;p&gt;Le operazioni di PD sono deterministiche, nel senso che le computazioni
vengono eseguite nel momento in cui vengono schedulate senza subire
cambiamenti di ordine in real‚Äìtime. Se un‚Äôoperazione viene attivata da
un evento esterno, viene associata ad un tempo; questo serve a garantire
che le esecuzioni siano consistenti con le scadenze temporali imposte
dallo scheduler (il tempo non deve mai decrescere).&lt;/p&gt;

&lt;h3 id=&quot;scrivere-external-per-pure-data&quot;&gt;Scrivere &lt;em&gt;external&lt;/em&gt; per Pure Data&lt;/h3&gt;

&lt;p&gt;Con il termine &lt;em&gt;external&lt;/em&gt; si indica un oggetto che non √® compreso in
Pure Data ma che pu√≤ essere caricato dinamicamente durante l‚Äôesecuzione
di PD; si differenziano dagli &lt;em&gt;internal&lt;/em&gt; in quanto questi ultimi sono le
primitive gi√† incluse in PD. Una volta che un external viene caricato in
memoria, non √® pi√π distinguibile dagli internal. Una libreria √® una
collezione di external compilati all‚Äôinterno di un unico file binario;
il nome di una libreria varia a seconda del sistema operativo per la
quale viene implementata: ad esempio, se viene creata la libreria
&lt;code class=&quot;highlighter-rouge&quot;&gt;my_lib&lt;/code&gt;, essa dovr√† essere chiamata &lt;code class=&quot;highlighter-rouge&quot;&gt;my_lib.pd_linux&lt;/code&gt; nei sistemi
Linux, &lt;code class=&quot;highlighter-rouge&quot;&gt;my_lib.pd_irix&lt;/code&gt; e &lt;code class=&quot;highlighter-rouge&quot;&gt;my_lib.dll&lt;/code&gt; nei sistemi Win32. Una libreria
elementare include esattamente un external avente lo stesso nome della
libreria.&lt;/p&gt;

&lt;p&gt;A differenza degli external, una libreria pu√≤ essere importata in due
modi:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;tramite opzione da riga di comando: &lt;code class=&quot;highlighter-rouge&quot;&gt;-lib my_lib&lt;/code&gt; (cos√¨ la libreria e tutti gli external in essa contenuti vengono caricati all‚Äôavvio di PD);&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;creando un oggetto &lt;code class=&quot;highlighter-rouge&quot;&gt;my_lib&lt;/code&gt; (consigliabile quando la libreria contiene un solo oggetto con il nome della libreria stessa).&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In entrambi i casi PD prima controlla se una libreria &lt;code class=&quot;highlighter-rouge&quot;&gt;my_lib&lt;/code&gt; √® gi√†
stata caricata; se cos√¨ non √®, viene cercato il file corrispondente e,
se trovato, tutti gli external inclusi vengono caricati.&lt;/p&gt;

&lt;p&gt;Pure Data √® scritto in C, quindi anche gli external vanno scritti in
questo linguaggio di programmazione; il codice per un semplice external
che stampa il messaggio ‚Äúhello world!‚Äù √® riportato di seguito
 &lt;a class=&quot;citation&quot; href=&quot;#pdexternal&quot;&gt;(Zm√∂lnig, J. M., n.d.)&lt;/a&gt;:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    #include &quot;m_pd.h&quot;

    static t_class *helloworld_class;

    typedef struct _helloworld {
    t_object x_obj;
    } t_helloworld;

    void helloworld_bang(t_helloworld *x)
    {
        post(&quot;Hello world !!&quot;);
    }

    void *helloworld_new(void)
    {
        t_helloworld *x = (t_helloworld *)pd_new(helloworld_class);
        return (void *)x;
    }

    void helloworld_setup(void)
    {
        helloworld_class = class_new(gensym(&quot;helloworld&quot;),
                                                (t_newmethod)helloworld_new,
                                                0, sizeof(t_helloworld),
                                                CLASS_DEFAULT, 0);
        class_addbang(helloworld_class, helloworld_bang);
    }
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Inizialmente viene definita la nuova &lt;em&gt;classe&lt;/em&gt; (qui il termine ‚Äúclasse‚Äù
viene usato con un significato diverso da quello usuale della
programmazione ad oggetti), dove &lt;code class=&quot;highlighter-rouge&quot;&gt;hello_worldclass&lt;/code&gt; √® un puntatore alla
nuova classe e la struttura &lt;code class=&quot;highlighter-rouge&quot;&gt;t_helloworld&lt;/code&gt; costituisce il &lt;em&gt;dataspace&lt;/em&gt;
della classe; la variabile &lt;code class=&quot;highlighter-rouge&quot;&gt;t_object&lt;/code&gt; √® assolutamente necessaria e serve
a memorizzare le propriet√† dell‚Äôoggetto, come la sua rappresentazione
grafica e le informazioni su inlet e outlet.&lt;/p&gt;

&lt;p&gt;Vengono poi definite le funzioni (&lt;em&gt;methods&lt;/em&gt;) per manipolare i dati;
quando l‚Äôistanza della classe riceve un dato, viene richiamato un
metodo; ogni metodo √® associato ad un inlet. La funzione implementata
viene eseguita solo quando un nuovo dato arriva a tale inlet.&lt;/p&gt;

&lt;p&gt;Al caricamento della libreria, PD richiama la funzione
&lt;code class=&quot;highlighter-rouge&quot;&gt;helloworld_setup()&lt;/code&gt;: tale funzione dichiara la nuova classe e le sue
propriet√†. L‚Äôistruzione &lt;code class=&quot;highlighter-rouge&quot;&gt;class_new&lt;/code&gt; crea una nuova classe e ritorna un
puntatore ad essa: il primo argomento √® il nome simbolico della classe;
il secondo e il terzo definiscono il costruttore e il distruttore; il
quarto definisce la dimensione della struttura dati; il quinto determina
l‚Äôaspetto grafico dell‚Äôoggetto; i rimanenti sono gli argomenti
dell‚Äôoggetto. L‚Äôistruzione successiva serve per aggiungere i metodi alla
classe (il primo argomento √® la classe, il secondo √® il metodo).&lt;/p&gt;

&lt;p&gt;L‚Äôinizializzazione dell‚Äôoggetto avviene tramite la funzione
&lt;code class=&quot;highlighter-rouge&quot;&gt;helloworld_new()&lt;/code&gt;, i cui argomenti dipendono dalla definizione data con
&lt;code class=&quot;highlighter-rouge&quot;&gt;class_new&lt;/code&gt;.&lt;/p&gt;

&lt;h3 id=&quot;sec:librerie_pd&quot;&gt;Librerie utili per Pure Data&lt;/h3&gt;

&lt;h4 id=&quot;gem&quot;&gt;GEM&lt;/h4&gt;

&lt;p&gt;GEM (acronimo per &lt;em&gt;Graphical Environment for Multimedia&lt;/em&gt;,
&lt;a href=&quot;http://gem.iem.at&quot;&gt;http://gem.iem.at&lt;/a&gt;) √® una collezione di external che permettono di
integrare elaborazioni grafiche OpenGL in una patch; sono disponibili
diversi tipi di forme geometriche, di luci e di texture; √® possibile
implementare il movimento della visuale e processare l‚Äôimmagine.&lt;/p&gt;

&lt;p&gt;Le elaborazioni della parte audio e della grafica vengono svolte
contemporaneamente: in tal modo si pu√≤ creare un vero e proprio scenario
virtuale semplicemente utilizzando una rete di blocchi creati e gestiti
come i blocchi nativi di PD.&lt;/p&gt;

&lt;h4 id=&quot;flext&quot;&gt;Flext&lt;/h4&gt;

&lt;p&gt;Si √® visto che Pure Data √® un software scritto in C, e gli external
devono essere scritti in tale linguaggio; l‚Äôutente per√≤ potrebbe avere
la necessit√† di usare le meno complesse strutture del C++, nonch√© il
pieno supporto alla programmazione ad oggetti che questo offre. Per
soddisfare questa esigenza nasce &lt;em&gt;flext&lt;/em&gt;
(&lt;a href=&quot;http://grrrr.org/ext/flext/&quot;&gt;http://grrrr.org/ext/flext/&lt;/a&gt;), una libreria per lo sviluppo di
external in C++. Con flext √® possibile creare librerie di external che
possono essere compilate per Pure Data, per Max/MSP e per differenti
piattaforme (Windows, Linux, OSX) e compilatori.&lt;/p&gt;

&lt;p&gt;Un semplice esempio di external basato su flext √® il seguente.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    // inclusione del header file
    #include &amp;lt;flext.h&amp;gt;

    // controllo sulla versione
    #if !defined(FLEXT_VERSION) || (FLEXT_VERSION &amp;lt; 400)
    #error You need at least flext version 0.4.0
    #endif

    // definizione della classe
    // Attenzione: il nome della classe deve essere lo stesso
    // nome dell'oggetto (senza l'eventuale ~)
    class simple1:

    public flext_base
    {
        FLEXT_HEADER(simple1,flext_base)

        public:
        // costruttore
        simple1()
        {
            // definizione degli inlets:
            // il primo deve essere sempre di tipo anything
            // (oppure signal per gli oggetti dsp)
            AddInAnything(); 
    
            // definizione degli outlets:
            AddOutFloat(); // aggiunta di un outlet float (indice 0)
    
            // registrazione dei metodi:
            // registra il metodo &quot;m_float&quot; per l'inlet 0
            FLEXT_ADDMETHOD(0,m_float); 
        }
    
        protected:
        // definizione del metodo
        void m_float(float input)
        {
            float result;
            if(input == 0) {
            post(&quot;%s - zero can't be inverted!&quot;,thisName());
            result = 0;
            }
            else
                result = 1/input;
                
            // manda il valore in output all'outlet
            ToOutFloat(0,result); // (0 √® l'indice dell'outlet)
        }

        private:
        // callback per il metodo &quot;m_float&quot;
        FLEXT_CALLBACK_1(m_float,float) 
    };

    // creazione della classe
    FLEXT_NEW(&quot;simple1&quot;,simple1)
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;figure class=&quot;image&quot;&gt;
  &lt;img src=&quot;images/flext_bn.jpg&quot; alt=&quot;Utilizzo del modulo simple1 basato su flext: accetta un numero in input e invia in output il suo inverso.&quot; /&gt;
  &lt;figcaption&gt;Utilizzo del modulo simple1 basato su flext: accetta un numero in input e invia in output il suo inverso.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Come si pu√≤ notare, √® stata utilizzata la programmazione ad oggetti, a
partire dalla creazione di una classe derivata dalla classe base
&lt;em&gt;flext_base&lt;/em&gt;, la quale contiene tutte le funzioni essenziali. Il
costruttore viene richiamato nel momento in cui l‚Äôoggetto √® incluso
nella patch; pi√π precisamente √® chiamato quando si crea un‚Äôistanza della
classe, contiene tutte le inizializzazioni necessarie e ha lo stesso
nome della classe. Tra le inizializzazioni devono essere presenti le
dichiarazioni di inlet e outlet e le associazioni tra metodi e
rispettivi inlet.&lt;/p&gt;

&lt;p&gt;Un &lt;em&gt;callback wrapper&lt;/em&gt; √® necessario per stabilire un collegamento con PD
per ogni metodo che deve essere lanciato ogni volta che un dato viene
ricevuto: ci√≤ avviene tramite l‚Äôistruzione
&lt;code class=&quot;highlighter-rouge&quot;&gt;FLEXT_CALLBACK_1(m_float, float)&lt;/code&gt;. Con l‚Äôultimo comando si informa il
sistema riguardo al nome della classe e ai suoi argomenti di creazione.&lt;/p&gt;

&lt;h3 id=&quot;implementazione-della-patch-generatrice-di-rumore-frattale&quot;&gt;Implementazione della patch generatrice di rumore frattale&lt;/h3&gt;

&lt;p&gt;Nella realizzazione della patch per Pure Data che implementa l‚Äôalgoritmo
di generazione di rumore frattale, per convenienze implementative, i
filtri sono stati riscritti come cascata di biquadri: perci√≤ la cascata
√® formata da &lt;script type=&quot;math/tex&quot;&gt;N/2&lt;/script&gt; filtri del secondo ordine, ognuno con le seguenti
funzioni di trasferimento:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\begin{aligned}
H^{(i)}(z) = H^{j}H^{j-1}(z) &amp; = &amp; \frac{(1+b_jz^{-1})(1+b_{j-1}z^{-1})}{(1+a_jz^{-1})(1+a_{j-1}z^{-1})} \label{eq:surface7} \\
                             &amp; = &amp;\mbox{} \frac{1+(b_j+b_{j-1})z^{-1}+(b_jb_{j-1})z^{-2}}{1+(a_j+a_{j-1})z^{-1}+(a_ja_{j-1})z^{-2}}\\
                                             &amp;   &amp; con~ j=2\cdot i, i=1...N/2.\end{aligned} %]]&gt;&lt;/script&gt;

&lt;figure class=&quot;image&quot;&gt;
  &lt;img src=&quot;images/surface_modeler1.jpg&quot; alt=&quot;La patch surface_modeler che implementa la generazione di rumore frattale.&quot; /&gt;
  &lt;figcaption&gt;La patch surface_modeler che implementa la generazione di rumore frattale.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Il parametro di controllo pi√π importante impostabile dall‚Äôutente √®
&lt;script type=&quot;math/tex&quot;&gt;\beta&lt;/script&gt;, che definisce lo spettro &lt;script type=&quot;math/tex&quot;&gt;1/f^{\beta}&lt;/script&gt;; deve essere impostato
anche il numero di poli della cascata di filtri assieme alla frequenza
del primo polo: con questi parametri viene controllata l‚Äôaccuratezza
dell‚Äôapprossimazione &lt;script type=&quot;math/tex&quot;&gt;1/f^{\beta}&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;Nella figura precedente √® riportata la patch &lt;em&gt;surface_modeler&lt;/em&gt;
che implementa la generazione di rumore frattale. Per avviare la
computazione √® sufficiente cliccare sul blocco (in modalit√† &lt;em&gt;running&lt;/em&gt;):&lt;/p&gt;

&lt;figure class=&quot;image&quot;&gt;
  &lt;img src=&quot;images/surfacemodeler2.jpg&quot; alt=&quot;&quot; /&gt;
  &lt;figcaption&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;subito dopo si seleziona il numero di poli desiderato (due, quattro o
sei). Il controllo sulla patch avviene variando il parametro &lt;script type=&quot;math/tex&quot;&gt;\beta&lt;/script&gt;
tramite lo slider:&lt;/p&gt;

&lt;figure class=&quot;image&quot;&gt;
  &lt;img src=&quot;images/surfacemodeler3.jpg&quot; alt=&quot;&quot; /&gt;
  &lt;figcaption&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;figure class=&quot;image&quot;&gt;
  &lt;img src=&quot;images/surfacemodeler4.jpg&quot; alt=&quot;Il modulo (subpatch) _initialize_fractal_noise&quot; /&gt;
  &lt;figcaption&gt;Il modulo (subpatch) _initialize_fractal_noise&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Il modulo &lt;em&gt;_cascade&lt;/em&gt; invece √® una subpatch nella quale viene
implementata una cascata di tre filtri; ogni
oggetto &lt;code class=&quot;highlighter-rouge&quot;&gt;biquad~&lt;/code&gt; √® un filtro biquadro a due poli e due zeri; ognuno di
questi filtri calcola le seguenti equazioni differenziali:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\begin{array}{ll}
y(n) = ff1 \cdot w(n) + ff2 \cdot w(n-1) + ff3 \cdot w(n-2) \\
w(n) = x(n) + fb1 \cdot w(n-1) + fb2 \cdot w(n-2)
\end{array}&lt;/script&gt;

&lt;p&gt;I valori &lt;script type=&quot;math/tex&quot;&gt;fb1, fb2, ff1, ff2, ff3&lt;/script&gt; vengono dati, in
quest‚Äôordine, come argomenti di creazione dell‚Äôoggetto.&lt;/p&gt;

&lt;figure class=&quot;image&quot;&gt;
  &lt;img src=&quot;images/cascade_bn.jpg&quot; alt=&quot;La subpatch _cascade&quot; /&gt;
  &lt;figcaption&gt;La subpatch _cascade&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h3 id=&quot;sec:patch_sliding&quot;&gt;Implementazione della patch generatrice di rumore di sfregamento&lt;/h3&gt;

&lt;figure class=&quot;image&quot;&gt;
  &lt;img src=&quot;images/patchsliding1.jpg&quot; alt=&quot;&quot; /&gt;
  &lt;figcaption&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;figure class=&quot;image&quot;&gt;
  &lt;img src=&quot;images/patchsliding2.jpg&quot; alt=&quot;&quot; /&gt;
  &lt;figcaption&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Come abbiamo visto √® stato sviluppato un modello che descrive il suono
per un corpo che rotola sopra una superficie. Se il corpo, invece di
rotolare, striscia sulla superficie, provoca sempre la generazione di
micro‚Äìcontatti, che tuttavia avvengono con modalit√† diverse rispetto al
rotolamento.&lt;/p&gt;

&lt;p&gt;Una prima distinzione si ha nella velocit√† con cui avviene il contatto:
se per il modello di rotolamento deve essere presa in considerazione la
velocit√† angolare dell‚Äôoggetto che rotola (che nel caso di una sfera si
calcola come &lt;script type=&quot;math/tex&quot;&gt;\omega = v/r&lt;/script&gt;, con &lt;script type=&quot;math/tex&quot;&gt;r&lt;/script&gt; raggio della sfera), per un corpo
che striscia si deve considerare la velocit√† tangenziale, cio√® la
velocit√† lungo il piano sul quale giace la superficie. In secondo luogo
il suono di un moto di rotolamento √® spesso caratterizzato da
irregolarit√† periodiche dovute alle caratteristiche particolari del
corpo che rotola. Se questo non √® perfettamente sferico e perfettamente
liscio, i micro‚Äìcontatti non saranno tutti uguali ma varieranno; in
particolare per un corpo che rotola i micro‚Äìcontatti si presentano con
le stesse caratteristiche a scadenze periodiche (variabili con la
velocit√† di movimento), e ci√≤ si riflette nel suono prodotto, il quale
sar√† caratterizzato da variazioni periodiche. Per un corpo che striscia
invece le irregolarit√† della sua superficie non comportano
caratteristiche periodiche nel suono. Tutto ci√≤ √® valido se la
dimensione del corpo √® sufficientemente grande rispetto alla tessitura
della superficie sulla quale si muove.&lt;/p&gt;

&lt;p&gt;La patch che implementa la generazione di rumore di sfregamento √® stata quindi elaborata a partire dalla
patch che implementa il modello di rotolamento.&lt;/p&gt;

&lt;h4 id=&quot;holy-rollersim&quot;&gt;holy-roller&lt;script type=&quot;math/tex&quot;&gt;\sim&lt;/script&gt;&lt;/h4&gt;

&lt;p&gt;Il cuore della computazione viene svolto dalla subpatch &lt;code class=&quot;highlighter-rouge&quot;&gt;holy_roller~&lt;/code&gt;; l‚Äôoggetto &lt;code class=&quot;highlighter-rouge&quot;&gt;holy_roller~&lt;/code&gt; possiede 13
inlet:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;inlet 0 ‚Äì accetta un oggetto di tipo messaggio contente il nome di un file &lt;code class=&quot;highlighter-rouge&quot;&gt;.wav&lt;/code&gt;; tale file √® stato precedentemente ottenuto registrando per circa 10 secondi l‚Äôoutput della patch generatrice di rumore frattale (e pertanto contiene a sua volta un rumore frattale);&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;inlet 1 e 2 ‚Äì non utilizzati in questa implementazione; accettano entrambi un segnale utilizzato poi come forza aggiuntiva da applicare all‚Äôoggetto rotolante;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;inlet 3 ‚Äì accetta un numero in virgola mobile proporzionale all‚Äôamplificazione in ampiezza che deve subire il rumore frattale;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;inlet 4 ‚Äì assumendo che l‚Äôoggetto che si muove sulla superficie sia una sfera, questo inlet riceve un numero in virgola mobile corrispondente al diametro della sfera in centimetri; sar√† poi utilizzato per calcolarne la massa;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;inlet 5 ‚Äì riceve un numero in virgola mobile indicante la velocit√† (in &lt;script type=&quot;math/tex&quot;&gt;m/s&lt;/script&gt;) dell‚Äôoggetto che si muove;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;inlet 6, 7, 8 e 9 ‚Äì ricevono tutti dei segnali di controllo per l‚Äôimpostazione delle frequenze e i tempi di decadimento degli oggetti modali usati nel modello di impatto (il tempo di decadimento √® definito come il tempo richiesto affinch√© l‚Äôampiezza decresca di un fattore &lt;script type=&quot;math/tex&quot;&gt;1/e&lt;/script&gt; rispetto al suo valore iniziale);&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;inlet 10 ‚Äì riceve un numero in virgola mobile (&lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt;) proporzionale alla rigidit√† dell‚Äôoggetto;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;inlet 11 e 12 ‚Äì ricevono due numeri in virgola mobile (&lt;script type=&quot;math/tex&quot;&gt;\alpha&lt;/script&gt; e &lt;script type=&quot;math/tex&quot;&gt;\lambda&lt;/script&gt;) utilizzati nel modulo &lt;code class=&quot;highlighter-rouge&quot;&gt;impact_modalb~&lt;/code&gt; per il calcolo della forza di impatto.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;elaborazione-di-diametro-e-velocit√†&quot;&gt;Elaborazione di diametro e velocit√†&lt;/h4&gt;

&lt;p&gt;Il valore del diametro viene elaborato da alcuni blocchi allo scopo di
calcolare dei valori indicanti il volume e la massa dell‚Äôoggetto
rotolante.&lt;/p&gt;

&lt;p&gt;Il modulo &lt;code class=&quot;highlighter-rouge&quot;&gt;_smoother&lt;/code&gt; ha lo scopo di trasformare una variazione
istantanea del valore del diametro secondo una rampa lineare della
durata di un millisecondo.&lt;/p&gt;

&lt;p&gt;Il valore di velocit√† invece viene inviato in input all‚Äôoggetto
&lt;code class=&quot;highlighter-rouge&quot;&gt;_clip_velo+fade&lt;/code&gt;; tale oggetto invia al primo outlet il valore di input
se questo √® maggiore del valore dato come primo argomento di costruzione
(che nell‚Äôimplementazione √® &lt;script type=&quot;math/tex&quot;&gt;0.01&lt;/script&gt;), altrimenti viene inviato
quest‚Äôultimo. Il ruolo di questo outlet √® impedire che alla patch
successiva &lt;code class=&quot;highlighter-rouge&quot;&gt;clip_exp~&lt;/code&gt; venga inviato costantemente un segnale di
controllo nullo; se ci√≤ si verificasse infatti si creerebbe un ciclo
infinito che porterebbe ad un funzionamento non corretto della patch. Al
secondo outlet viene inviato il valore ricevuto all‚Äôinlet opportunamente
scalato nell‚Äôintervallo delimitato dai due argomenti.&lt;/p&gt;

&lt;h4 id=&quot;clip_expsim&quot;&gt;clip_exp&lt;script type=&quot;math/tex&quot;&gt;\sim&lt;/script&gt;&lt;/h4&gt;

&lt;p&gt;I valori di raggio (in metri) e di velocit√† (in metri al secondo)
vengono inviati ai due oggetti &lt;code class=&quot;highlighter-rouge&quot;&gt;clip_exp~&lt;/code&gt;, la cui funzione √® quella di
limitare la variazione logaritmica dei segnali in input. Pi√π
precisamente, in ognuno dei due oggetti viene per prima cosa calcolato
il rapporto tra due campioni in input a distanza di un millisecondo
l‚Äôuno dall‚Äôaltro; se tale rapporto √® maggiore di &lt;code class=&quot;highlighter-rouge&quot;&gt;maxfact&lt;/code&gt; o minore di
&lt;code class=&quot;highlighter-rouge&quot;&gt;minfact&lt;/code&gt; (calcolati a partire dal parametro di costruzione), il valore
del campione corrente viene limitato e inviato all‚Äôoutlet. Se il
rapporto √® minore del valore predeterminato, il campione viene inviato
in output invariato. Il codice seguente mostra come questo algoritmo sia
implementato (per ragioni di efficienza) in C.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    static t_int *clip_exp_tilde_perform(t_int *w)
    {
      t_float *in = (t_float *)(w[1]);
      t_float *out = (t_float *)(w[2]);
    
    t_clip_exp_tilde_ctl *c = (t_clip_exp_tilde_ctl *)(w[3]);
    t_int buffersize = (t_int)(w[4]);

    t_float input, ratio;

        // esamina tutto il buffer
    while (buffersize--)
        {
        input = *in++;

            // se last √® diverso da zero
        if (c-&amp;gt;last != 0.)
            {
                // se ratio √® compreso tra maxfact e minfact
                // in ouput viene mandato input
                // altrimenti l'output √® impostato a maxfact o minfact
            ratio = input / c-&amp;gt;last;
        
            if (ratio &amp;gt; c-&amp;gt;maxfact)
                c-&amp;gt;last *= c-&amp;gt;maxfact;
            
            else if (ratio &amp;lt; c-&amp;gt;minfact)
                c-&amp;gt;last *= c-&amp;gt;minfact;
            
            else
                c-&amp;gt;last = input;
            }

      *out++ = c-&amp;gt;last;
    }

    return (w+5);
    }
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;I valori &lt;code class=&quot;highlighter-rouge&quot;&gt;maxfact&lt;/code&gt; e &lt;code class=&quot;highlighter-rouge&quot;&gt;minfact&lt;/code&gt; vengono calcolati nel seguente modo:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    static void set_expmax(t_clip_exp_tilde *x, t_floatarg expmax)
    {
      t_clip_exp_tilde_ctl *c = x-&amp;gt;x_ctl;

        // se l'argomento del modulo √® &amp;lt; 1
        // pongo maxfact e minfact = 1
      if (expmax &amp;lt;= 1.)
    {
        c-&amp;gt;maxfact = c-&amp;gt;minfact = 1.;
      post(&quot;clip_exp: expmax &amp;lt;= 1?! Is set to 1.&quot;);
    }
    // atrimenti:
    // maxfact = epmax^(1000/samprate)
    else
    {
        c-&amp;gt;maxfact = pow(expmax, 1000. / x-&amp;gt;samprate);
      c-&amp;gt;minfact = 1. / c-&amp;gt;maxfact;
    }
    }
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;h4 id=&quot;circ_max_filtersim&quot;&gt;circ_max_filter&lt;script type=&quot;math/tex&quot;&gt;\sim&lt;/script&gt;&lt;/h4&gt;

&lt;p&gt;Gli outlet dei due moduli &lt;code class=&quot;highlighter-rouge&quot;&gt;clip_exp~&lt;/code&gt; sono collegati al secondo e terzo
inlet dell‚Äôoggetto &lt;code class=&quot;highlighter-rouge&quot;&gt;circ_max_filter~&lt;/code&gt;:&lt;/p&gt;

&lt;p&gt;la sua funzione √® quella di tracciare il profilo della superficie sulla
quale l‚Äôoggetto rotola e calcolare i punti di contatto tra i due. Nel
primo inlet entra il controllo di segnale ottenuto dalla moltiplicazione
del rumore frattale per il fattore di amplificazione &lt;code class=&quot;highlighter-rouge&quot;&gt;surface_depth&lt;/code&gt;. Il
ciclo principale svolto dal modulo √® il seguente:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;static t_int *circ_max_filter_perform(t_int *w)
{
    t_float *in1 = (t_float *)(w[1]);
    t_float *in2 = (t_float *)(w[2]);
    t_float *in3 = (t_float *)(w[3]);
    t_float *out = (t_float *)(w[4]);

    t_circ_max_filter_ctl *c = (t_circ_max_filter_ctl *)(w[5]);
    t_float *p_samprate = (t_float *)(w[6]);
    t_int buffersize = (t_int)(w[7]);

    t_float input, radius, velocity;
    t_int range;

    while (buffersize--)
    {
        input = *in1++;
        radius = *in2++;
        velocity = *in3++;

        inc_bottom_ivalue1_circ_buff_1float2int(c-&amp;gt;p_peaks, -1);

        while ((range = bottom_ivalue1_circ_buff_1float2int(c-&amp;gt;p_peaks))
                &amp;lt; bottom_ivalue2_circ_buff_1float2int(c-&amp;gt;p_peaks))
        {
            delete_bottom_circ_buff_1float2int(c-&amp;gt;p_peaks);
            inc_bottom_ivalue1_circ_buff_1float2int(c-&amp;gt;p_peaks, range);
        }
    
        *out++ = up_circle(velocity * range / *p_samprate, radius)
                + bottom_fvalue_circ_buff_1float2int(c-&amp;gt;p_peaks);

        to_buffer(c-&amp;gt;p_peaks, *p_samprate, input, radius, velocity, 1);
    }

    return (w+8);
}
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;In particolare si pu√≤ notare come l‚Äôistruzione&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    *out++ = up_circle(velocity * range / *p_samprate, radius)
            + bottom_fvalue_circ_buff_1float2int(c-&amp;gt;p_peaks);
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;calcola i punti di contatto svolgendo la computazione della funzione &lt;script type=&quot;math/tex&quot;&gt;f_x(q)&lt;/script&gt;.&lt;/p&gt;

&lt;p&gt;La funzione &lt;code class=&quot;highlighter-rouge&quot;&gt;up_circle&lt;/code&gt; richiede due argomenti; dopo aver calcolato i
quadrati di questi, ritorna la radice quadrata della differenza dei due:&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;    static INLINE t_float up_circle(t_float x, t_float radius)
    {
        t_float x_2 = x*x, radius_2 = radius*radius;

        if (x_2 &amp;gt;= radius_2)
        {
          return(0.);
        }
        else
            return(sqrt(radius_2 - x_2));
    }
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;La funzione &lt;code class=&quot;highlighter-rouge&quot;&gt;to_buffer&lt;/code&gt; si occupa di aggiornare il profilo della
superficie in base al segnale ricevuto al primo inlet.&lt;/p&gt;

&lt;h4 id=&quot;_surface_tracersim&quot;&gt;_surface_tracer&lt;script type=&quot;math/tex&quot;&gt;\sim&lt;/script&gt;&lt;/h4&gt;

&lt;p&gt;Il file con estensione &lt;code class=&quot;highlighter-rouge&quot;&gt;.wav&lt;/code&gt; contenente il rumore frattale viene letto
dalla subpatch &lt;code class=&quot;highlighter-rouge&quot;&gt;_surface_tracer~&lt;/code&gt;. Questa subpatch, assieme alle subpatch
in essa contenute quali &lt;code class=&quot;highlighter-rouge&quot;&gt;pd tracer+calculation~&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;soundfiler_tracer~&lt;/code&gt; e
&lt;code class=&quot;highlighter-rouge&quot;&gt;table_tracer~&lt;/code&gt;, legge il file audio e lo scrive in un array;
successivamente, per inviare i campioni in output, esegue una ricerca di
tipo &lt;em&gt;table look‚Äìup&lt;/em&gt; con frequenza dipendente dalla velocit√† di
movimento dell‚Äôoggetto sulla superficie.&lt;/p&gt;

&lt;figure class=&quot;image&quot;&gt;
  &lt;img src=&quot;images/surfacetracer.jpg&quot; alt=&quot;&quot; /&gt;
  &lt;figcaption&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h4 id=&quot;impact_modalbsim&quot;&gt;impact_modalb&lt;script type=&quot;math/tex&quot;&gt;\sim&lt;/script&gt;&lt;/h4&gt;

&lt;p&gt;Le successive elaborazioni dei segnali finora calcolati sono svolte dal
modulo &lt;code class=&quot;highlighter-rouge&quot;&gt;impact_modalb~&lt;/code&gt;, un oggetto che implementa il modello descritto
in  &lt;a class=&quot;citation&quot; href=&quot;#art:soundobj&quot;&gt;(Avanzini, F., Rath, M., &amp;amp; Rocchesso, D., 2003)&lt;/a&gt; per i suoni di impatto.&lt;/p&gt;

&lt;figure class=&quot;image&quot;&gt;
  &lt;img src=&quot;images/impactmodalb.jpg&quot; alt=&quot;L'oggetto impact_modalb&quot; /&gt;
  &lt;figcaption&gt;L'oggetto impact_modalb&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;In questa implementazione vengono utilizzati due modi e tre punti di
interazione.&lt;/p&gt;

&lt;figure class=&quot;image&quot;&gt;
  &lt;img src=&quot;images/partolist.jpg&quot; alt=&quot;&quot; /&gt;
  &lt;figcaption&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;I parametri della forza di contatto e della massa vengono ricevuti dalla
subpatch &lt;code class=&quot;highlighter-rouge&quot;&gt;interaction+mass&lt;/code&gt; e la subpatch in essa contenuta
&lt;code class=&quot;highlighter-rouge&quot;&gt;_par_to_list4&lt;/code&gt;; in output (secondo outlet) viene mandata
una lista contenente queste informazioni. Nei quattro inlet vengono
ricevuti, in ordine: &lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;\alpha&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;\lambda&lt;/script&gt; e la massa del percussore.&lt;/p&gt;

&lt;p&gt;La subpatch &lt;code class=&quot;highlighter-rouge&quot;&gt;_modal_object_parameters3_2&lt;/code&gt;, dove 3 √® il numero di modi e
2 il numero di punti di interazione, raccoglie i parametri del
risonatore. Nei primi tre inlet entrano i fattori moltiplicativi per
frequenza, tempo di decadimento e guadagno; ai successivi tre inlet sono
collegati i controlli delle frequenze di tutti i modi, poi i tempi di
decadimento di tutti i modi. Gli ultimi inlet ricevono i livelli di
ciascun modo per ogni punto di interazione con l‚Äôeventuale possibilit√†
di invertire la fase (&lt;em&gt;phase‚Äìreverse&lt;/em&gt;). In uscita sono presenti cinque
outlet: il primo per la lista dei fattori, il secondo per la lista delle
frequenze, il terzo per la lista dei tempi di decadimento e un outlet
per ogni punto di interazione con l‚Äôindice del punto di interazione
seguito dalla lista dei livelli (con l‚Äôeventuale fattore di phase
reverse). Infine l‚Äôoggetto &lt;code class=&quot;highlighter-rouge&quot;&gt;_modal_object_parameters3_2&lt;/code&gt; deve essere
inizializzato con la seguente lista di argomenti: lista dei valori delle
frequenze, lista dei valori dei tempi di decadimento, valori dei punti
di interazione e del phase‚Äìreverse; un valore 1 per il livello
corrisponde ad una impostazione del relativo slider a 100, dato che
quest‚Äôultimo viene convertito in dB RMS.&lt;/p&gt;

&lt;p&gt;L‚Äôoggetto &lt;code class=&quot;highlighter-rouge&quot;&gt;impact_modalb~&lt;/code&gt; possiede i seguenti argomenti di costruzione:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;valori di default di &lt;script type=&quot;math/tex&quot;&gt;k&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;\alpha&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;\lambda&lt;/script&gt; e massa del
percussore;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;numero di modi e numero di punti di interazione;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;maschera dei punti di interazione;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;valori di default dei tre fattori di guadagno;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;valori di default delle frequenze;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;valori di default dei tempi di decadimento;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;per ogni punto di interazione il suo indice (partendo da 0) seguito
dai valori dei livelli.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;figure class=&quot;image&quot;&gt;
  &lt;img src=&quot;images/impactmodalb2.jpg&quot; alt=&quot;L'oggetto impact_modalb e la divisione tra i gruppi di argomenti di costruzione&quot; /&gt;
  &lt;figcaption&gt;L'oggetto impact_modalb e la divisione tra i gruppi di argomenti di costruzione&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h4 id=&quot;sec:vandendoel&quot;&gt;Dipendenza dell‚Äôampiezza del suono dalla forza normale&lt;/h4&gt;

&lt;p&gt;Secondo studi svolti da Van Den Doel, Kry e Pai [@art:vandendoel], nei
contatti che avvengono tra due corpi e che coinvolgono forze di
frizione, queste ultime sono calcolabili come:
&lt;script type=&quot;math/tex&quot;&gt;F_{frizione} = \mu F_{normale}&lt;/script&gt; e il volume del suono prodotto da
ogni contatto √® proporzionale a &lt;script type=&quot;math/tex&quot;&gt;\sqrt{v \cdot F_{normale}}&lt;/script&gt;, dove &lt;script type=&quot;math/tex&quot;&gt;v&lt;/script&gt; √®
la velocit√† alla quale avviene il contatto; in questo calcolo si assume
che l‚Äôenergia acustica sia proporzionale alla perdita di capacit√† da
parte della superficie di opporre una resistenza (di frizione) al moto.
Nella subpatch &lt;code class=&quot;highlighter-rouge&quot;&gt;holy_roller~&lt;/code&gt; tale caratteristica √® implementata tramite
gli oggetti in figura:&lt;/p&gt;

&lt;figure class=&quot;image&quot;&gt;
  &lt;img src=&quot;images/vandendoel.jpg&quot; alt=&quot;&quot; /&gt;
  &lt;figcaption&gt;&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;h2 id=&quot;riferimenti&quot;&gt;Riferimenti&lt;/h2&gt;

&lt;ol class=&quot;bibliography&quot;&gt;&lt;li&gt;&lt;span id=&quot;art:soundobj&quot;&gt;Avanzini, F., Rath, M., &amp;amp; Rocchesso, D. (2003). Low-level sound models: resonators, interactions, surface textures. &lt;i&gt;The Sounding Object&lt;/i&gt;, 119‚Äì148.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;book:fractal&quot;&gt;Hastings, H. M., &amp;amp; Sugihara, G. (1993). &lt;i&gt;Fractals: A User‚Äôs Guide for the Natural Sciences.&lt;/i&gt; Oxford University Press.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;book:brownian&quot;&gt;Resnick, S. (1992). &lt;i&gt;Adventures in Stochastic Processes&lt;/i&gt;. Birkh√§user Boston.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;book:wornell&quot;&gt;Wornell, G. W. (1998). &lt;i&gt;The Digital Signal Processing Handbook&lt;/i&gt;. CRC Press and IEEE Press.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;art:pentland&quot;&gt;Pentland, A. P. (1988). Fractal-Based Description Of Surfaces. &lt;i&gt;Natural Computation&lt;/i&gt;, 279‚Äì298.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;art:saletti&quot;&gt;Saletti, R. (Novembre 1986). A comparison between two methods to generate 1/f^Œ≥noise. &lt;i&gt;Proc. IEEE&lt;/i&gt;, &lt;i&gt;74&lt;/i&gt;, 1595‚Äì1596.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;art:corsini&quot;&gt;Corsini, G., &amp;amp; Saletti, R. (Dicembre 1988). A 1/f^Œ≥power spectrum noise sequence generator. &lt;i&gt;IEEE Trans. on Instrumentation and Measurement&lt;/i&gt;, &lt;i&gt;37&lt;/i&gt;(4), 615‚Äì619.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;book:mitra&quot;&gt;Mitra, S. K. (1998). &lt;i&gt;Digital Signal Processing: A Computer Based Approach&lt;/i&gt;. McGraw-Hill.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;pdexternal&quot;&gt;Zm√∂lnig, J. M. &lt;i&gt;HOWTO write an external for puredata&lt;/i&gt;. Institut for electronic music and acoustics.&lt;/span&gt;&lt;/li&gt;&lt;/ol&gt;</content><author><name>Andrea Maglie</name></author><category term="haptic feedback" /><category term="pure data" /><summary type="html">Disclaimer: Il contenuto di questo post √® un estratto della mia tesi di laurea, realizzata nel 2006. Alcuni esempi potrebbero risultare obsoleti, tuttavia la componente teorica resta un buon riferimento per chi deve approcciarsi alla materia.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="/patchsliding1.jpg" /></entry><entry><title type="html">La percezione uditiva</title><link href="/percezione-uditiva.html" rel="alternate" type="text/html" title="La percezione uditiva" /><published>2019-01-22T00:00:00+01:00</published><updated>2019-01-22T00:00:00+01:00</updated><id>/percezione-uditiva</id><content type="html" xml:base="/percezione-uditiva.html">&lt;hr /&gt;

&lt;p&gt;&lt;i&gt;Disclaimer: Il contenuto di questo post √® un estratto della mia tesi di laurea, realizzata nel 2006. Alcuni esempi potrebbero risultare obsoleti, tuttavia la componente teorica resta un buon riferimento per chi deve approcciarsi alla materia.&lt;/i&gt;&lt;/p&gt;
&lt;hr /&gt;

&lt;p&gt;I suoni che percepiamo ogni giorno nascono da interazioni tra oggetti
(un martello che colpisce un metallo, una moneta che cade) o da
cambiamenti nelle propriet√† di un singolo oggetto (come un palloncino
che scoppia). Ma siamo in grado di riconoscere tali eventi fisici e le
loro propriet√† solo sulla base del suono prodotto? Per rispondere a
questa domanda sono stati svolti numerosi studi, utilizzando diversi
approcci. Uno di questi consiste nel dividere l‚Äôoggetto di studio in tre
livelli:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;livello fisico;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;livello acustico;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;livello percettivo.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;L‚Äôanalisi delle relazioni tra il livello percettivo e il livello fisico
permette di capire se le caratteristiche di un evento vengono
riconosciute propriamente o vengono scalate; l‚Äôanalisi delle relazioni
tra livello fisico e livello acustico ci dice come variano le propriet√†
del segnale sonoro in base alle propriet√† fisiche dell‚Äôevento; infine
dall‚Äôanalisi delle relazioni tra livello acustico e percettivo si
capisce se e come i segnali acustici influenzano il riconoscimento e la
classificazione delle propriet√† dell‚Äôevento in esame.&lt;/p&gt;

&lt;p&gt;Le caratteristiche del segnale audio possono essere raggruppate in due
categorie:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;caratteristiche studiate dalla ricerca classica sulla percezione
sonora, come ampiezza, durata, tonalit√†, timbro e attacco;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;caratteristiche della sorgente sonora.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Durante l‚Äôascolto la nostra attenzione viene rivolta a l‚Äôuna o l‚Äôaltra
classe di propriet√†. In base a ci√≤ √® possibile dire che le persone
assumono due tipi di comportamenti durante l‚Äôascolto dei suoni
&lt;a class=&quot;citation&quot; href=&quot;#art:gaver1&quot;&gt;(Gaver, W. W., 1993)&lt;/a&gt;:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;l‚Äôascolto di tipo &lt;em&gt;musicale&lt;/em&gt; porta a riconoscere i suoni assieme
alle loro propriet√†;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;l‚Äôascolto &lt;em&gt;di tutti i giorni&lt;/em&gt; porta a riconoscere gli eventi e le
sorgenti sonore piuttosto che le propriet√† del suono.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;La maggior parte della nostra esperienza nell‚Äôascolto degli eventi pu√≤
essere classificata come ascolto di tutti i giorni: ascoltiamo ci√≤ che
avviene attorno a noi, imparando cosa √® importante evitare e cosa invece
pu√≤ offrirci una possibilit√† di interazione. Le dimensioni percettive e
gli attributi considerati sono quelli dell‚Äôevento che produce il suono,
e non quelli del suono di per s√©; tale esperienza √® molto diversa
dall‚Äôascolto di tipo musicale, e non pu√≤ essere studiata pienamente
utilizzando gli approcci tradizionali all‚Äôacustica. E‚Äô anche vero che i
suoni musicali non sono rappresentativi della classe di suoni che
regolarmente sentiamo:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;i suoni musicali sono armonici, hanno un‚Äôevoluzione temporale
semplice, non rivelano molte informazioni riguardo alla loro
sorgente e variano lungo dimensioni come tonalit√† e ampiezza;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;i suoni di tutti i giorni sono inarmonici o rumorosi, hanno
un‚Äôevoluzione temporale complessa, spesso rivelano molte
informazioni riguardo alla loro sorgente e variano lungo molte
dimensioni.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Per studiare quest‚Äôultima tipologia di suoni √® necessario espandere la
psicoacustica in due modi: considerando le dimensioni del suono e della
sua sorgente e trattando alcune variabili complesse come elementari.
Tali assunzioni guidano lo sviluppo dell‚Äô&lt;em&gt;approccio ecologico&lt;/em&gt; alla
percezione uditiva.&lt;/p&gt;

&lt;p&gt;In questo tipo di approccio gli stimoli elementari non necessariamente
corrispondono a dimensioni fisiche altrettanto elementari, ma in alcuni
casi sono costituite da eventi complessi; per questo, secondo
l‚Äôapproccio ecologico, lo studio della percezione deve essere rivolto a
scoprire le dimensioni rilevanti per la percezione e le informazioni
relative a queste.&lt;/p&gt;

&lt;h2 id=&quot;dallevento-allesperienza&quot;&gt;Dall‚Äôevento all‚Äôesperienza&lt;/h2&gt;

&lt;figure class=&quot;image&quot;&gt;
  &lt;img src=&quot;images/gaver.jpg&quot; alt=&quot;L'esempio dell'automobile come sorgente di onde sonore; alcune onde
raggiungono l'orecchio umano immutate, altre invece vengono modificate
dall'ambiente.&quot; /&gt;
  &lt;figcaption&gt;L'esempio dell'automobile come sorgente di onde sonore; alcune onde
raggiungono l'orecchio umano immutate, altre invece vengono modificate
dall'ambiente.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Immaginiamo di sentire l‚Äôavvicinarsi di un‚Äôautomobile. Possiamo
considerare tale evento come una propagazione continua di energia dalla
sorgente al soggetto; lungo il percorso che questo flusso di energia
compie si trovano vari ostacoli, ognuno con le sue caratteristiche ed
ognuno influenzante la propagazione.&lt;/p&gt;

&lt;p&gt;Nel caso in esame, la prima fonte di informazione √® l‚Äôautomobile (la
sorgente): il suono dipende da svariati fattori, come il movimento dei
cilindri del motore, lo sfregamento degli ingranaggi e le vibrazioni
della carrozzeria. Vengono cos√¨ determinate le onde di pressione che si
propagano radialmente dalla sorgente (contrariamente alla luce radiante,
la propagazione radiale del suono ha una struttura ricca e fornisce
molte informazioni riguardo la sua sorgente).&lt;/p&gt;

&lt;p&gt;Successivamente il suono viene modificato dagli ostacoli che incontra
nell‚Äôambiente circostante; in particolare il suono perde energia man
mano che si allontana dalla sorgente, specialmente alle alte frequenze
(anche se non sono presenti ostacoli), e ci√≤ fornisce un‚Äôinformazione
riguardo alla localizzazione della sorgente; se la sorgente si muove si
avverte un cambio di frequenze (effetto Doppler) e un cambiamento
nell‚Äôampiezza del suono indica un‚Äôallontanamento o avvicinamento della
sorgente stessa. Dato che il sistema uditivo umano √® mobile, possiamo
girare la testa al fine di cogliere i cambiamenti nei pattern e
migliorare la localizzazione. Vediamo cos√¨ come un suono dia
informazioni circa un‚Äô&lt;em&gt;interazione tra materiali&lt;/em&gt; in un certo &lt;em&gt;luogo&lt;/em&gt; e
in un determinato &lt;em&gt;ambiente&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Ulteriori studi &lt;a class=&quot;citation&quot; href=&quot;#art:gaver1&quot;&gt;(Gaver, W. W., 1993)&lt;/a&gt; hanno portato alla distinzione dei suoni
di tutti i giorni in tre grandi categorie: solidi, liquidi e
aerodinamici, in quanto raramente queste classi vengono confuse tra di
loro. Ogni classe viene poi suddivisa in base al tipo di interazione tra
i materiali: ad esempio i suoni generati da solidi vibranti sono divisi
in suoni di rotolamento, di sfregamento, di impatto e di deformazione.
Queste classi costituiscono gli eventi base che producono dei suoni.&lt;/p&gt;

&lt;h3 id=&quot;vibrazioni-dei-solidi&quot;&gt;Vibrazioni dei solidi&lt;/h3&gt;

&lt;p&gt;Questa classe comprende eventi come la rottura di un vetro, una porta
che sbatte o un‚Äôautomobile in moto. Gli oggetti vibrano quando su di
essi una forza viene impressa e rilasciata, portando il sistema fuori
dal suo stato di equilibrio; tale forza deforma l‚Äôoggetto dalla sua
configurazione originale, mentre la forza che l‚Äôoggetto oppone alla
deformazione viene trasformata in energia potenziale nella nuova
configurazione. Quando la forza smette di agire, l‚Äôoggetto cerca di
tornare nella posizione di riposo, l‚Äôenergia potenziale si trasforma in
energia cinetica e l‚Äôoggetto vibra. Le vibrazioni continuano fino a
quando tutta l‚Äôenergia accumulata viene persa e l‚Äôoggetto torna nella
posizione iniziale o trova un nuovo equilibrio. Il tipo di interazione
(un urto, uno sfregamento o un rotolamento) determina sia la variazione
nel tempo dell‚Äôampiezza che lo spettro della vibrazione; la forza invece
determina l‚Äôampiezza complessiva della vibrazione. Il pattern di
vibrazione √® determinato anche dal tipo di materiale di cui √® costituito
l‚Äôoggetto, quindi dalla sua rigidit√†. La dimensione determina la pi√π
bassa frequenza di vibrazione, mentre la forma determina frequenza e
pattern spettrale prodotto.&lt;/p&gt;

&lt;p&gt;Tutti i parametri considerati possono essere raggruppati in due domini:
il dominio della frequenza e il dominio temporale. Dal momento che la
frequenza √® il reciproco del tempo, √® difficile separare questi due
domini dal punto di vista fisico; tuttavia sono separabili dal punto di
vista psicologico: i parametri nel dominio della frequenza influenzano
le vibrazioni dell‚Äôoggetto, mentre i parametri nel dominio del tempo
provocano effetti che diventano evidenti solo dopo alcuni cicli di
vibrazioni. Gli attributi dell‚Äôoggetto (densit√†, dimensioni) hanno
effetti sul suono nel dominio della frequenza, mentre gli attributi
dell‚Äôinterazione (tipo e forza) influenzano i parametri nel dominio
temporale.&lt;/p&gt;

&lt;h3 id=&quot;eventi-aerodinamici&quot;&gt;Eventi aerodinamici&lt;/h3&gt;

&lt;p&gt;I suoni aerodinamici vengono prodotti quando una sorgente modifica
l‚Äôattuale pressione atmosferica circostante (come quando esplode un
palloncino). La variazione di pressione si propaga come un‚Äôonda, la
quale, se raggiunge l‚Äôorecchio e possiede particolari propriet√†, pu√≤
essere avvertita come suono. La maggior parte delle informazioni viene
data dalla banda di frequenza del suono, dipendente dalla forza e dalla
quantit√† della variazione di pressione. Le componenti in alta frequenza
indicano la velocit√† nel cambiamento di pressione, mentre le componenti
in bassa frequenza dipendono dal gas coinvolto.&lt;/p&gt;

&lt;p&gt;Un altro tipo di evento aerodinamico si verifica quando un cambiamento
nella pressione imprime energia ad un oggetto provocando una sua
vibrazione.&lt;/p&gt;

&lt;h3 id=&quot;liquidi&quot;&gt;Liquidi&lt;/h3&gt;

&lt;p&gt;Gli eventi che coinvolgono liquidi dipendono da una deformazione
iniziale come nella vibrazione dei solidi, ma la vibrazione non
influisce sull‚Äôaria circostante in modo da provocare un suono; il suono
invece √® il risultato della formazione e variazione di cavit√† risonanti
nella superficie del liquido. Per rendersene conto, basta pensare ad un
piccolo oggetto che cade in un bicchiere d‚Äôacqua: al momento del
contatto il liquido viene spostato dall‚Äôoggetto, formando una cavit√† che
risuona ad una frequenza caratteristica, amplificando e modificando
l‚Äôonda di pressione creata dall‚Äôimpatto. Successivamente la pressione
del liquido lo porta a chiudere la cavit√†, immergendo completamente
l‚Äôoggetto. Un tale suono √® quindi caratterizzato da un breve impulso
seguito da altri brevi impulsi di frequenza pi√π alta. I dettagli del
suono sono determinati da massa, dimensione, velocit√† dell‚Äôoggetto e
dalla viscosit√† del liquido.&lt;/p&gt;

&lt;h3 id=&quot;eventi-che-producono-suoni-complessi&quot;&gt;Eventi che producono suoni complessi&lt;/h3&gt;

&lt;p&gt;Molti suoni dipendono da pattern complessi degli eventi descritti, o una
combinazione di questi. Anche se la fisica non descrive tali eventi
complessi, esistono attributi di alto livello che producono importanti
effetti sul loro suono. Un esempio di attributo √® l‚Äôintervallo tra
eventi successivi: una sequenza di passi possono essere avvertiti come
una camminata se gli intervalli tra un passo e l‚Äôaltro cadono
all‚Äôinterno di un certo intervallo; altro esempio √® la presenza di
vincoli tra gli oggetti coinvolti nell‚Äôevento (sarebbe strano sentire il
cigolio di una porta che si chiude lentamente accompagnato dal forte
suono di una porta che viene chiusa con forza).&lt;/p&gt;

&lt;p&gt;In tutti questi casi le sorgenti sonore possono essere considerate
annidate (si pensi al suono di un‚Äôautomobile, del suo motore e dei
cilindri); in tal caso, un evento base pu√≤ essere definito come un
evento composto da una singola interazione e un singolo oggetto che
produce il suono. Gli eventi complessi possono essere considerati allora
combinazioni di eventi base, nelle quali la struttura della combinazione
aggiunge informazioni importanti a quelle gi√† fornite dagli eventi base.
Infine, √® proprio questa struttura pi√π complessa che permette di
estrarre pi√π facilmente informazioni.&lt;/p&gt;

&lt;p&gt;In base alla struttura delle combinazioni possiamo distinguere tre tipi
di eventi complessi:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;gli eventi definiti da un &lt;em&gt;pattern temporale&lt;/em&gt; di eventi base (come
il rimbalzo di una palla √® composto da un pattern di impatti);&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;gli eventi &lt;em&gt;residui&lt;/em&gt;, dati dalla sovrapposizione di diversi eventi
base;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;gli eventi &lt;em&gt;ibridi&lt;/em&gt;, dati dall‚Äôinterazione tra diversi tipi di
materiale.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Ognuno di questi eventi complessi potenzialmente produce lo stesso
profilo di sorgente sonora, pi√π eventualmente altre propriet√†
specifiche: ad esempio una serie di impatti spaziati opportunamente nel
tempo pu√≤ portare ad individuare il rimbalzo di una palla, dando anche
informazioni sul materiale e sulla simmetria della palla stessa.&lt;/p&gt;

&lt;p&gt;Il suono prodotto da questi eventi varia in base a molte dimensioni
fisiche di base; tuttavia spesso non percepiamo la variazione del suono
in ogni singola dimensione, bens√¨ avvertiamo una variazione lungo una
nuova dimensione (fittizia) che incorpora tutte le altre.&lt;/p&gt;

&lt;h2 id=&quot;descrivere-gli-eventi&quot;&gt;Descrivere gli eventi&lt;/h2&gt;

&lt;figure class=&quot;image&quot;&gt;
  &lt;img src=&quot;images/gerarchia_eventi.jpg&quot; alt=&quot;Gerarchia degli eventi&quot; /&gt;
  &lt;figcaption&gt;Gerarchia degli eventi&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Mentre i suoni vengono descritti in base ad attributi come frequenza,
ampiezza e durata, pi√π difficile √® trovare degli attributi che
permettano di descrivere gli eventi di tutti i giorni.&lt;/p&gt;

&lt;p&gt;Una possibilit√† √® quella di classificare questi eventi in base al
contesto nel quale si verificano. Ci√≤ pu√≤ essere utile se si vuole
trovare un suono all‚Äôinterno di una classificazione di tale tipo, ma non
permette una descrizione efficiente di ci√≤ che sentiamo, in quanto le
classi non sono mutuamente esclusive (uno stesso evento pu√≤ verificarsi
in contesti diversi). Pi√π interessante sembra essere una descrizione di
tipo gerarchico: in questo modo gli eventi che si trovano ad un livello
superiore nella gerarchia danno informazioni utili sul tipo di eventi di
livello subordinato, mentre dimensioni e caratteristiche servono a
descrivere le differenze tra i membri di una stessa categoria.&lt;/p&gt;

&lt;p&gt;In base a quanto detto possiamo quindi classificare i suoni di tutti i
giorni come riportato in figura: al pi√π alto livello tutti gli eventi
sono visti come interazione di materiali; al livello successivo invece
vengono divisi in vibrazioni di solidi, aerodinamici e liquidi; al terzo
livello la distinzione √® tra eventi base.&lt;/p&gt;

&lt;h2 id=&quot;percezione-e-psicofisica&quot;&gt;Percezione e psicofisica&lt;/h2&gt;

&lt;p&gt;Come per la percezione aptica, anche la percezione sonora √® un processo
che si articola in tre livelli: livello fisico, neurale e mentale. In
pi√π, nel riconoscimento dei suoni le persone cercano di trarre vantaggio
dall‚Äôesperienza acquisita nel passato (anche se non √® possibile
effettuare una stima di tale esperienza).&lt;/p&gt;

&lt;p&gt;Esiste una differenza, nella nostra percezione uditiva, tra &lt;em&gt;ci√≤ che √®&lt;/em&gt;
e &lt;em&gt;ci√≤ che sentiamo&lt;/em&gt;:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;siamo in grado di sentire suoni che non esistono (la fondamentale
mancante);&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;non riusciamo a sentire suoni che esistono (mascheramento);&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;sentiamo due diversi eventi con lo stesso insieme di stimoli (ritmi
reversibili);&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;possiamo sentire suoni che sono prodotti da sorgenti inesistenti
nell‚Äôambiente (musica elettronica o qualsiasi manipolazione
spettrale di suoni reali).&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;L‚Äôuomo agisce in base a ci√≤ che sente, non in base a ci√≤ che esiste.&lt;/p&gt;

&lt;h2 id=&quot;ascoltare-e-riconoscere-la-sorgente&quot;&gt;Ascoltare e riconoscere la sorgente&lt;/h2&gt;

&lt;p&gt;Dire che l‚Äôuomo ‚Äúsente‚Äù la sorgente del suono pu√≤ essere corretto se si
considera un ambiente naturale: se ad esempio ci troviamo vicino ad un
violinista che suona, possiamo dire che sentiamo il violino suonare. Ma
ascoltando il suono di un violino proveniente da un impianto hi‚Äìfi non
ci verrebbe mai in mente di dire che stiamo sentendo il cono
dell‚Äôaltoparlante (anche se ci√≤ √® vero); piuttosto continuiamo a
sostenere che stiamo sentendo un violino. E‚Äô opportuno quindi dire che
l‚Äôuomo non sente la sorgente del suono, bens√¨ l‚Äôuomo ‚Äúrappresenta‚Äù la
sorgente.&lt;/p&gt;

&lt;p&gt;Il riconoscimento del suono e della sua sorgente √® per√≤ un concetto
diverso rispetto alla percezione:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;si pu√≤ avere percezione senza riconoscimento;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;il riconoscimento pu√≤ avvenire a vari livelli: √® possibile
riconoscere un rumore ma non una tonalit√†, il rombo di un‚Äôauto ma
non quello di una motocicletta;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;si possono verificare riconoscimenti fasulli: pi√π precisamente un
riconoscimento √® sempre ‚Äúvero‚Äù all‚Äôinizio, ma pu√≤ diventare ‚Äúfalso‚Äù
dopo aver percepito ulteriori informazioni sull‚Äôevento (come quando
si pensa di sentire la pioggia e invece si tratta del rumore delle
foglie mosse dal vento);&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;ci sono riconoscimenti ambigui (ad esempio quando non siamo in grado
di identificare una voce come maschile o femminile);&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;lo stesso stimolo acustico pu√≤ dare vita a diversi riconoscimenti a
seconda che si verifichi da solo o che si ripeti (un singolo colpo
di pistola viene identificato come tale, mentre una sequenza veloce
di tali colpi fa pensare ad una mitragliatrice);&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;a volte la sorgente che viene riconosciuta √® immateriale (si pensi
all‚Äôaccelerazione o decelerazione di un ritmo);&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;alcuni riconoscimenti avvengono in tempo reale, mentre altri possono
avvenire ‚Äúin ritardo‚Äù, riguardando suoni gi√† sentiti e memorizzati
nella nostra mente.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;i-modelli-fisici&quot;&gt;I modelli fisici&lt;/h2&gt;

&lt;p&gt;I modelli fisici hanno lo scopo di sintetizzare, tramite un modello
matematico, le propriet√† degli oggetti reali; in particolare tramite i
modelli audio si cerca di riprodurre i suoni associati a determinati
eventi.&lt;/p&gt;

&lt;p&gt;I modelli sono per√≤ sempre delle idealizzazioni, e quindi non possono
rappresentare fedelmente i fenomeni che si verificano in natura. Inoltre
gli stimoli provenienti da sorgenti reali variano lungo molte
dimensioni, ma non tutte possono essere sintetizzate: √® possibile ad
esempio riprodurre le variazioni di ampiezza, timbro, durata, e
dinamica; ma diventa molto difficile sintetizzare propriet√† come
presenza, brillantezza e contenuto espressivo. Inoltre, nella fisica
reale sono presenti molti oggetti che vibrano simultaneamente, mentre
l‚Äôorecchio riceve una unica onda sonora; √® necessario cos√¨ estrarre da
quest‚Äôultima i segnali o le caratteristiche che possano ricreare la
molteplicit√† di oggetti, la loro disposizione, il percorso dell‚Äôonda
sonora ed eventuali ostacoli lungo il suo cammino.&lt;/p&gt;

&lt;h2 id=&quot;riferimenti&quot;&gt;Riferimenti&lt;/h2&gt;

&lt;ol class=&quot;bibliography&quot;&gt;&lt;li&gt;&lt;span id=&quot;art:gaver1&quot;&gt;Gaver, W. W. (1993). What in the world do we hear? An ecological approach to auditory event perception. &lt;i&gt;Ecological Psychology&lt;/i&gt;, &lt;i&gt;5&lt;/i&gt;(1), 1‚Äì29.&lt;/span&gt;&lt;/li&gt;&lt;/ol&gt;</content><author><name>Andrea Maglie</name></author><category term="haptic feedback" /><category term="pure data" /><summary type="html">Disclaimer: Il contenuto di questo post √® un estratto della mia tesi di laurea, realizzata nel 2006. Alcuni esempi potrebbero risultare obsoleti, tuttavia la componente teorica resta un buon riferimento per chi deve approcciarsi alla materia.</summary></entry><entry><title type="html">La percezione aptica</title><link href="/percezione-aptica.html" rel="alternate" type="text/html" title="La percezione aptica" /><published>2019-01-15T00:00:00+01:00</published><updated>2019-01-15T00:00:00+01:00</updated><id>/percezione-aptica</id><content type="html" xml:base="/percezione-aptica.html">&lt;hr /&gt;

&lt;p&gt;&lt;i&gt;Disclaimer: Il contenuto di questo post √® un estratto della mia tesi di laurea, realizzata nel 2006. Alcuni esempi potrebbero risultare obsoleti, tuttavia la componente teorica resta un buon riferimento per chi deve approcciarsi alla materia.&lt;/i&gt;&lt;/p&gt;
&lt;hr /&gt;

&lt;h2 id=&quot;il-tatto&quot;&gt;Il tatto&lt;/h2&gt;

&lt;p&gt;Il senso del tatto pu√≤ essere visto come un sistema multisensoriale
attivo. Il termine multisensoriale indica che le modalit√† del tatto
riguardano vari sistemi sensoriali, pi√π precisamente:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;sistema cutaneo;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;sistema cinestetico;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;sistema aptico.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Il sistema cutaneo riceve degli input sensoriali dai meccanorecettori
(le terminazioni nervose che rispondono a stimolazioni meccaniche)
situati nella pelle. Il sistema cinestetico (cio√® quella parte del
sistema nervoso che si occupa della percezione della posizione e del
movimento degli arti) riceve informazioni dai meccanorecettori posti nei
muscoli, nei tendini e nelle giunture. Infine, il sistema aptico
utilizza le informazioni che provengono da questi due sistemi. Il
termine &lt;em&gt;aptico&lt;/em&gt; √® associato al tocco attivo, e nella vita di tutti i
giorni il tocco √® proprio di questo tipo: muovendo gli arti e la pelle
su superfici ed oggetti, i sensori tattili vengono stimolati, rivelando
moltissime importanti propriet√† del mondo che ci circonda.&lt;/p&gt;

&lt;p&gt;Nel corso degli anni sono state formulate diverse definizioni di tocco
attivo o passivo. Secondo Gibson &lt;a class=&quot;citation&quot; href=&quot;#art:gibson&quot;&gt;(Gibson, J. J., 1962)&lt;/a&gt; il tocco √® passivo quando
√® assente il movimento volontario dei muscoli, mentre il tocco attivo √®
costituito dall‚Äôesplorazione degli oggetti tramite comandi inviati dal
cervello ai muscoli. Loomis e Lederman hanno rielaborato il pensiero di
Gibson per ottenere una classificazione del sistema sensoriale in base
agli input usati; secondo tale classificazione esistono cinque
differenti modalit√† di tocco &lt;a class=&quot;citation&quot; href=&quot;#art:loomis&quot;&gt;(Loomis, J. M. &amp;amp; Lederman, S. J., 1986)&lt;/a&gt;:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;
    &lt;p&gt;percezione tattile (cutanea);&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;percezione cinestetica passiva (risposta cinestetica senza movimento
volontario);&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;percezione apitca passiva (risposte cinestetica e cutanea senza
movimento volontario);&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;percezione cinestetica attiva;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;percezione aptica attiva.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Solo negli ultimi due casi l‚Äôosservatore ha un controllo motorio sul
processo di esplorazione tattile.&lt;/p&gt;

&lt;p&gt;Un‚Äôulteriore classificazione pu√≤ essere fatta tra le sensazioni
meccaniche del tocco (come pressione e posizione) e le sensazioni legate
alla temperatura e al dolore; in tal caso non √® diverso solo il tipo di
sensazione, ma √® diversa anche la tipologia di percezione neurale.&lt;/p&gt;

&lt;h2 id=&quot;neurofisiologia-del-tatto&quot;&gt;Neurofisiologia del tatto&lt;/h2&gt;

&lt;figure class=&quot;image&quot;&gt;
  &lt;img src=&quot;images/meccanorecettori.jpg&quot; alt=&quot;Sezione verticale della pelle della mano, con la locazione dei quattro
tipi dimeccanorecettori.&quot; /&gt;
  &lt;figcaption&gt;Sezione verticale della pelle della mano, con la locazione dei quattro
tipi dimeccanorecettori.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;La pelle √® l‚Äôorgano sensoriale pi√π grande del nostro corpo; nell‚Äôadulto
medio si estende per circa 2 metri e pesa dai 3 ai 5 chilogrammi. E‚Äô
composta di due strati: l‚Äô&lt;em&gt;epidermide&lt;/em&gt; (la parte pi√π esterna) e il
&lt;em&gt;derma&lt;/em&gt; (la parte interna); in entrambi gli strati si trovano i
meccanorecettori, i responsabili della traduzione
degli stimoli meccanici in stimoli neurali. Si pu√≤ considerare un
ulteriore strato compreso tra il derma e i muscoli: l‚Äô&lt;em&gt;ipoderma&lt;/em&gt;; esso
contiene tessuti connettivi e grasso sottocutaneo, oltre alle
terminazioni dei meccanorecettori.&lt;/p&gt;

&lt;p&gt;La pelle della mano contiene quattro diversi tipi di meccanorecettori,
distinguibili in base all‚Äôarea percettiva e alla risposta agli stimoli.&lt;/p&gt;

&lt;dl&gt;
  &lt;dt&gt;Unit√† ad adattamento veloce ‚Äì&lt;/dt&gt;
  &lt;dd&gt;
    &lt;p&gt;Le unit√† ad adattamento veloce (FA - fast adapting) mostrano una
rapida risposta alle deformazioni della pelle. Se la zona di
ricezione degli stimoli √® piccola e ben definita si parla di unit√†
FAI, mentre le unit√† FAII sono costituite da zone ricettive pi√π
grandi e con confini poco definiti.&lt;/p&gt;
  &lt;/dd&gt;
  &lt;dt&gt;Unit√† ad adattamento lento ‚Äì&lt;/dt&gt;
  &lt;dd&gt;
    &lt;p&gt;Le unit√† ad adattamento lento (SA - slow adapting) esibiscono una
risposta continua alle deformazioni sostenute della pelle; le unit√†
SAI hanno una forte sensibilit√† dinamica e mostrano una risposta
irregolare alle stimolazioni sostenute, mentre le unit√† SAII
esibiscono maggiore regolarit√† nella risposta pur essendo meno
sensibili.&lt;/p&gt;
  &lt;/dd&gt;
&lt;/dl&gt;

&lt;p&gt;Altri tipi di recettori rispondono alle stimolazioni termiche; sono
presenti unit√† di recettori che rispondono al caldo e altre che
rispondono al freddo, e tutte sono localizzate nella parte pi√π esterna
della pelle. Assieme a questi si trovano anche i recettori del dolore.&lt;/p&gt;

&lt;p&gt;I meccanorecettori presenti nei muscoli, nei tendini, nelle giunture e
nella pelle delle mani contribuiscono al senso cinestetico di movimento
degli arti; le terminazioni di diametro pi√π largo codificano il tasso di
cambiamento della lunghezza delle fibre muscolari e le vibrazioni; le
terminazioni pi√π piccole sono pi√π sensibili nella fase statica
dell‚Äôattivit√† muscolare.&lt;/p&gt;

&lt;h2 id=&quot;aspetti-sensoriali-del-tatto&quot;&gt;Aspetti sensoriali del tatto&lt;/h2&gt;

&lt;h3 id=&quot;sensibilit√†-e-risoluzione&quot;&gt;Sensibilit√† e risoluzione&lt;/h3&gt;

&lt;p&gt;Molti esperimenti sono stati fatti per capire quali sono le soglie di
reazione umane alle deformazioni meccaniche della pelle, per poi scalare
l‚Äôampiezza delle sensazioni in corrispondenza all‚Äôampiezza degli stimoli
e trovare le relazioni tra le reazioni recettive e le caratteristiche
degli stimoli alle diverse soglie:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;La capacit√† di risoluzione spaziale della pelle √® stata misurata
come la minima distanza tra due punti tale che questi vengano
percepiti come distinti; utilizzando una griglia di punti, la
capacit√† di risoluzione valutata √® di circa 1 millimetro.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Gli esperimenti sulla capacit√† di risoluzione temporale, valutata in
termini di sensibilit√† alle vibrazioni, hanno mostrato che gli
adulti sono in grado di cogliere vibrazioni fino a 700 Hz, cio√®
possono distinguere intervalli temporali di circa 1.4
millisecondi. Analizzando invece la capacit√† di individuare come
successivi due impulsi, ciascuno di un millisecondo, si √® trovato
che i due devono essere separati almeno di 5.5 millisecondi.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Tutto ci√≤ dimostra che la mano ha una capacit√† risolutiva spaziale
migliore dell‚Äôorecchio ma minore dell‚Äôocchio, mentre per la risoluzione
temporale la situazione si inverte, avendosi che la mano √® migliore
dell‚Äôocchio ma peggiore dell‚Äôudito.&lt;/p&gt;

&lt;h3 id=&quot;effetti-della-posizione-del-corpo-e-dellet√†&quot;&gt;Effetti della posizione del corpo e dell‚Äôet√†&lt;/h3&gt;

&lt;p&gt;La sensibilit√† della pelle varia in base alla parte del corpo che viene
stimolata: ad esempio il viso rileva forze di bassa entit√†, mentre le
dita sono pi√π efficienti nell‚Äôelaborare informazioni spaziali. Gli
effetti dell‚Äôet√† sono stati studiati esaminando le soglie di rilevamento
delle vibrazioni: con l‚Äôavanzare dell‚Äôet√† queste soglie aumentano,
principalmente a causa della perdita di recettori. Lo stesso si verifica
per le soglie di rilevamento della densit√† spaziale; dai 20 agli 80 anni
si verifica un aumento di circa 1% all‚Äôanno delle soglie di
discriminazione della distanza tra due punti e nel rilevamento del loro
orientamento rispetto alle dita.&lt;/p&gt;

&lt;h3 id=&quot;manipolazioni-basate-sulla-sensibilit√†&quot;&gt;Manipolazioni basate sulla sensibilit√†&lt;/h3&gt;

&lt;p&gt;Le informazioni ricevute dalla cute giocano un ruolo fondamentale
nell‚Äôinterazione con gli oggetti. Basti pensare al fatto che le persone
dotate di alte soglie di discriminazione tendono ad afferrare gli
oggetti con pi√π forza per poterli manipolare; per soggetti con gravi
problemi la manipolazione pu√≤ diventare impossibile.&lt;/p&gt;

&lt;p&gt;Afferrare e manipolare un oggetto richiede che la presa e le forze
vengano coordinate lungo una sequenza di stadi; i recettori forniscono
le informazioni necessarie a dosare e coordinare le forze per compiere
queste azioni. Ma le persone non usano solo le informazioni che vengono
ricevute istantaneamente dal cervello: vengono sfruttate anche le
conoscenze acquisite nelle esperienze passate circa il peso e le altre
propriet√† degli oggetti. Ci√≤ conduce all‚Äôuso di movimenti muscolari gi√†
programmati e nell‚Äôadattamento di questi alle nuove propriet√† degli
oggetti manipolati.&lt;/p&gt;

&lt;h2 id=&quot;percezione-aptica-delle-propriet√†-degli-oggetti-e-delle-superfici&quot;&gt;Percezione aptica delle propriet√† degli oggetti e delle superfici&lt;/h2&gt;

&lt;p&gt;Secondo Klatzky e Lederman &lt;a class=&quot;citation&quot; href=&quot;#art:lederman5&quot;&gt;(Lederman, S. J. &amp;amp; Klatzky, R. L., 1999)&lt;/a&gt;, il sistema aptico inizia
l‚Äôestrazione di informazioni gi√† a partire dalle unit√† periferiche (i
recettori). Ci√≤ contrasta con il metodo di funzionamento degli organi
visivi: la percezione visiva di un oggetto diventa ben definita solo in
seguito a varie elaborazioni di alto livello.&lt;/p&gt;

&lt;p&gt;Sono state fatte varie distinzioni tra le tipologie di informazioni che
possono essere estratte dalla manipolazione di un oggetto o una
superficie. Una prima discriminazione √® tra propriet√† &lt;em&gt;geometriche&lt;/em&gt; e
propriet√† del &lt;em&gt;materiale&lt;/em&gt;:&lt;/p&gt;

&lt;dl&gt;
  &lt;dt&gt;Propriet√† geometriche ‚Äì&lt;/dt&gt;
  &lt;dd&gt;
    &lt;p&gt;Le propriet√† geometriche sono specifiche di un oggetto e possono
essere divise in &lt;em&gt;dimensione&lt;/em&gt; e &lt;em&gt;forma&lt;/em&gt;; inoltre possiamo
considerare geometrie a livello microscopico e a livello
macroscopico. A livello microscopico un oggetto √® abbastanza piccolo
da ricoprire solo una limitata regione della pelle, come la punta di
un dito; ci√≤ produce una deformazione sulla pelle che viene
codificata dai meccanorecettori (in particolare dai SAI), ricavando
una mappa della disposizione dell‚Äôoggetto e le sue profondit√†. Nel
caso macroscopico l‚Äôoggetto viene avvolto dalle mani (o dagli arti
in generale), raccogliendo informazioni dai sensori cinestetici e da
zone della pelle che non sono continue (come le dita); la
determinazione della geometria avviene integrando tra loro tutte
queste informazioni.&lt;/p&gt;
  &lt;/dd&gt;
  &lt;dt&gt;Propriet√† del materiale ‚Äì&lt;/dt&gt;
  &lt;dd&gt;
    &lt;p&gt;Le propriet√† del materiale possono essere distinte, secondo Klatzky
e Lederman, in &lt;em&gt;tessitura&lt;/em&gt;, &lt;em&gt;rigidit√†&lt;/em&gt;, &lt;em&gt;temperatura apparente&lt;/em&gt; e
&lt;em&gt;peso&lt;/em&gt;. Le tessiture comprendono propriet√† quali ruvidit√†, densit√†
spaziale e viscosit√†. La rigidit√† non sempre corrisponde alla
resistenza che oppone un oggetto (si pensi al tasto di un
pianoforte: il tasto √® rigido, ma nel momento in cui viene premuto
non oppone una forte resistenza fino a quando non viene premuto fino
in fondo); ci√≤ significa che in questo caso entrano in gioco sia le
informazioni cutanee che quelle cinestetiche.&lt;/p&gt;
  &lt;/dd&gt;
&lt;/dl&gt;

&lt;h3 id=&quot;ruvidit√†&quot;&gt;Ruvidit√†&lt;/h3&gt;

&lt;p&gt;Una superficie ruvida √® costituita da asperit√† poste sopra un substrato
relativamente omogeneo. La tessitura pu√≤ essere microscopica o
macroscopica: si parla di micro‚Äìtessitura se le asperit√† sono spaziate
ad intervalli dell‚Äôordine del millesimo di millimetro; si parla di
macro‚Äìtessitura se gli intervalli sono di uno o due ordini di grandezza
pi√π ampi. Con intervalli oltre i 3‚Äì4 millimetri, la superficie non
appare pi√π come dotata di tessitura ma come una superficie liscia con
delle irregolarit√† puntuali.&lt;/p&gt;

&lt;p&gt;Secondo vari studi, la ruvidit√† percepita aumenta all‚Äôaumentare dello
spazio tra le asperit√†; l‚Äôaumento della grandezza delle asperit√† invece
contribuisce a far avvertire la superficie come meno ruvida. La
percezione √® influenzata anche dalle modalit√† con le quali questa
avviene: l‚Äôapplicazione di una pressione sulla superficie con le dita
porta ad aumentare la ruvidit√† percepita; anche la diversa velocit√† di
esplorazione porta a considerare diversi livelli di ruvidit√†. Ci√≤ che
invece non influisce √® se il controllo √® attivo o passivo; quindi le
informazioni cinestetiche giocano un ruolo marginale. Secondo il modello
formulato da Taylor e Lederman, la percezione della ruvidit√† √® basata
sulla complessiva deformazione che lo stimolo provoca sulla pelle;
inoltre la deformazione pu√≤ essere assunta come unidimensionale.&lt;/p&gt;

&lt;p&gt;Quando gli stimoli vengono presentati sulla pelle nuda, l‚Äôuomo tende a
non considerare le vibrazioni nella valutazione delle macro‚Äìtessiture
(quindi lo spazio tra le asperit√† e la velocit√† di esplorazione
influiscono minimamente sul giudizio); il contrario avviene nella
percezione delle micro‚Äìtessiture, per le quali le vibrazioni permettono
di discriminare tessiture con asperit√† comprese tra 0.6 e 1.6 micron
spaziate di circa 100 micron.&lt;/p&gt;

&lt;h3 id=&quot;peso&quot;&gt;Peso&lt;/h3&gt;

&lt;p&gt;Weber nel 1834 not√≤ che un oggetto viene percepito come pi√π pesante
quando viene impugnato rispetto a quando viene semplicemente appoggiato
sulla pelle, suggerendo che il peso percepito non dipende solamente dal
suo valore oggettivo. Successivamente Chapentier e Dresslar
&lt;a class=&quot;citation&quot; href=&quot;#book:chapentier&quot;&gt;(Chapentier, A., 1891)&lt;/a&gt; capirono che altri fattori intervengono nella
percezione del peso, dato che, tra due oggetti dello stesso peso e
diverse dimensioni, quello pi√π piccolo sembra pi√π pesante. Tali
osservazioni sono state confermate dai risultati di alcuni studi,
secondo i quali la percezione del peso dipende dalla resistenza che
questo oppone alla rotazione (in particolare dipende dagli autovalori
della matrice di rotazione).&lt;/p&gt;

&lt;p&gt;Un altro fattore che influenza questo tipo di percezione √® il materiale
di cui √® costituito l‚Äôoggetto: oggetti composti da materiali pi√π densi
vengono avvertiti come pi√π pesanti; oggetti pi√π scivolosi necessitano di
una presa pi√π forte per essere manipolati, e ci√≤ pu√≤ condurre ad
avvertirli come pi√π pesanti. L‚Äôinfluenza del materiale non √® presente
tuttavia quando si considerano oggetti di grande massa o quando oggetti
di piccola massa vengono impugnati con forza.&lt;/p&gt;

&lt;p&gt;In tutte queste percezioni √® sempre presente una componente cognitiva,
che porta l‚Äôuomo ad utilizzare informazioni gi√† acquisite in passato per
questo tipo di valutazioni.&lt;/p&gt;

&lt;h3 id=&quot;curvatura&quot;&gt;Curvatura&lt;/h3&gt;

&lt;p&gt;Con il termine &lt;em&gt;curvatura&lt;/em&gt; si intende il tasso di cambiamento
dell‚Äôangolo della tangente ad una curva al variare del punto per il
quale passa la tangente. La percezione della curvatura viene influenzata
da eventuali altre superfici toccate in precedenza, oppure dal fatto che
la curvatura sia orientata lungo le dita, che tocchi il palmo o il dorso
della mano.&lt;/p&gt;

&lt;p&gt;Durante l‚Äôesplorazione di una superficie curva, l‚Äôinformazione pi√π
importante (quella che viene valutata di pi√π nella determinazione del
livello di curvatura o nella distinzione tra oggetti curvi e oggetti
piani) √® data dalla differenza nelle posizioni relative delle dita.&lt;/p&gt;

&lt;h3 id=&quot;lesplorazione-manuale-nella-percezione-delle-propriet√†-degli-oggetti&quot;&gt;L‚Äôesplorazione manuale nella percezione delle propriet√† degli oggetti&lt;/h3&gt;

&lt;figure class=&quot;image&quot;&gt;
  &lt;img src=&quot;images/esplorazioni1.jpg&quot; alt=&quot;Procedure di esplorazione aptica delle propriet√† degli
oggetti.&quot; /&gt;
  &lt;figcaption&gt;Procedure di esplorazione aptica delle propriet√† degli
oggetti.&lt;/figcaption&gt;
&lt;/figure&gt;

&lt;p&gt;Lederman e Klatzky &lt;a class=&quot;citation&quot; href=&quot;#art:lederman7&quot;&gt;(Lederman, S. J. &amp;amp; Klatzky, R. L., 1987)&lt;/a&gt; hanno individuato l‚Äôesistenza di
movimenti particolari che vengono eseguiti dalle persone
nell‚Äôesplorazione delle propriet√† di un oggetto, e ogni tipo di
propriet√† √® associata ad una diversa &lt;em&gt;procedura di esplorazione&lt;/em&gt;:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;il &lt;em&gt;movimento laterale&lt;/em&gt; √® associato all‚Äôesplorazione delle
tessiture;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;con il &lt;em&gt;contatto statico&lt;/em&gt; si cerca di massimizzare la zona di
contatto con la superficie per individuarne la temperatura;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;impugnare&lt;/em&gt; un oggetto serve per capire a grandi linee la sua forma
e il suo volume;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;esercitare una &lt;em&gt;pressione&lt;/em&gt; fornisce informazioni sulla resistenza di
un oggetto;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;em&gt;tenere in mano l‚Äôoggetto&lt;/em&gt; d√† informazioni circa il suo peso;&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;l‚Äô&lt;em&gt;esplorazione dei contorni&lt;/em&gt; serve a individuare i contorni precisi
(e quindi la forma precisa) dell‚Äôoggetto in esame.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;percezione-aptica-dello-spazio&quot;&gt;Percezione aptica dello spazio&lt;/h2&gt;

&lt;p&gt;Non esiste ancora una definizione universalmente accettata di &lt;em&gt;spazio
aptico&lt;/em&gt;; Lederman, Klatzky, Collins e Wardell &lt;a class=&quot;citation&quot; href=&quot;#art:lederman6&quot;&gt;(Lederman, S. J., Klatzky, R. L., Collins, A., &amp;amp; Wardell, J., 1987)&lt;/a&gt; hanno
introdotto una distinzione tra lo spazio che viene raggiunto ed
esplorato dalle mani e lo spazio che viene esplorato tramite movimenti
del corpo. Il primo tipo di spazio si pu√≤ definire &lt;em&gt;spazio
manipolatorio&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;La percezione aptica dello spazio √® anisotropica, in quanto non √®
possibile applicare una metrica alle percezioni di questo tipo: esse
risultano distorte rispetto alla realt√† e non sono uniformi lungo lo
spazio esplorato. Un primo tipo di distorsione √® costituito
dall‚Äôillusione verticale‚Äìorizzontale: la lunghezza di linee verticali
viene sovrastimata rispetto alle stesse poste in orizzontale. Tale
illusione √® stata riscontrata sia in soggetti ciechi che in soggetti
senza problemi di vista (quindi non √® dovuta a illusioni visive) ed √®
fortemente influenzata dal movimento delle braccia usato durante
l‚Äôesplorazione (dipende quindi dalla distanza relativa tra soggetto e
oggetto esaminato).&lt;/p&gt;

&lt;p&gt;Un altro tipo di illusione riguarda il movimento radiale o tangenziale:
i movimenti radiali (verso il corpo e lontano dal corpo) tendono a dare
un giudizio sovrastimato rispetto ai movimenti tangenziali di uguale
estensione. Anche in questo caso √® forte l‚Äôinfluenza della posizione
degli arti; ad esempio una distanza percepita √® pi√π grande se la mano si
trova vicina al corpo.&lt;/p&gt;

&lt;p&gt;Il terzo tipo di illusione riguarda l‚Äôorientamento obliquo: riprodurre
l‚Äôorientamento di un‚Äôasta √® pi√π difficile quando questa √® obliqua (a 45
gradi ad esempio). Si √® visto inoltre che l‚Äôeffetto √® maggiore quando √®
presente la forza gravitazionale rispetto a quando questa viene
annullata con dei pesi che la controbilanciano; infatti, se l‚Äôasta √®
posta sul piano orizzontale, l‚Äôeffetto non si presenta.&lt;/p&gt;

&lt;h3 id=&quot;percezione-di-pattern-bidimensionali-e-tridimensionali&quot;&gt;Percezione di pattern bidimensionali e tridimensionali&lt;/h3&gt;

&lt;p&gt;Di seguito sono riportati i risultati di vari studi eseguiti su diversi
tipi di pattern aptici.&lt;/p&gt;

&lt;dl&gt;
  &lt;dt&gt;Pattern vibrotattili ‚Äì&lt;/dt&gt;
  &lt;dd&gt;
    &lt;p&gt;Un pattern vibrotattile viene generato stimolando una parte del
corpo (usualmente le dita) tramite un insieme di punti di contatto.
I punti di contatto sono costituiti da una matrice di spilli che
vibrano circa 230 volte al secondo; le righe sono distanti tra loro
circa un millimetro mentre le colonne circa $2.5$ millimetri. Due
pattern che vengono presentati ad una certa distanza temporale tra
loro possono sommarsi formando un nuovo pattern o produrre due
distinte risposte; tali effetti si verificano anche se i pattern
vengono presentati allo stesso istante su dita diverse della mano.
Le interazioni spaziali si verificano se il pattern stimola aree
comuni della pelle o quando provocano uno stesso tipo di reazione su
zone diverse della pelle; l‚Äôabilit√† di discriminare i pattern √®
inversamente proporzionale all‚Äôarea di pelle che viene stimolata da
tutti i pattern.&lt;/p&gt;
  &lt;/dd&gt;
  &lt;dt&gt;Pattern bidimensionali e forme libere ‚Äì&lt;/dt&gt;
  &lt;dd&gt;
    &lt;p&gt;I pattern bidimensionali sono composti da un insieme di sporgenze
lineari o puntuali, come ad esempio il Braille. I meccanorecettori
coinvolti principalmente nella percezione di queste trame sono i
SAI, i quali agiscono su piccole aree e quindi sono pi√π sensibili
alle discontinuit√† di una superficie, e insieme producono una
risposta che preserva la forma della superficie stessa. Se i pattern
bidimensionali rappresentano oggetti reali, il riconoscimento di
questi √® guidato in gran parte dalle conoscenze visuali che si
posseggono dell‚Äôoggetto in esame.&lt;/p&gt;
  &lt;/dd&gt;
  &lt;dt&gt;Oggetti tridimensionali ‚Äì&lt;/dt&gt;
  &lt;dd&gt;
    &lt;p&gt;Contrariamente alle riproduzioni bidimensionali, gli oggetti reali
vengono riconosciuti molto bene al tatto. Una causa di ci√≤ √® che
nella riproduzione bidimensionale viene eliminata una dimensione, la
profondit√†, la quale pu√≤ essere ricostruita tramite la vista ma non
tramite il tocco. L‚Äôesplorazione solitamente inizia con
l‚Äôindividuazione delle caratteristiche locali, per poi passare
all‚Äôestrazione delle caratteristiche globali dell‚Äôoggetto. Oggetti
simili nella forma ma con diverse caratteristiche locali vengono
tendenzialmente giudicati come diversi se l‚Äôesplorazione √® aptica,
mentre con l‚Äôesplorazione visiva vengono giudicati simili; il
risultato dell‚Äôesplorazione aptica si avvicina a quello
dell‚Äôesplorazione visuale man mano che il tempo di analisi
dell‚Äôoggetto aumenta. Inoltre sembra che le persone tendano ad
esaminare la parte frontale di un oggetto se viene usa la vista,
mentre in un‚Äôanalisi aptica si considera maggiormente la parte
posteriore (quella che pi√π spesso viene esplorata con le dita).&lt;/p&gt;
  &lt;/dd&gt;
&lt;/dl&gt;

&lt;p&gt;Sono stati eseguiti degli studi anche sull‚Äôinterazione tra percezione
aptica e percezione visuale, allo scopo di verificare se l‚Äôattenzione
del soggetto viene rivolta automaticamente verso una certa zona dello
spazio o piuttosto viene attirata da degli stimoli. Se vengono stimolati
entrambi i sensi (tatto e vista) concordemente, allora l‚Äôattenzione pu√≤
essere rivolta spontaneamente sia nella modalit√† visiva che nella
modalit√† aptica; al contrario, una stimolazione visiva incongruente con
quella aptica pu√≤ condurre ad una direzione errata dell‚Äôattenzione
aptica. Tale effetto √® dovuto alla dominazione del senso della vista sul
senso del tatto e dipende dall‚Äôet√† del soggetto (all‚Äôaumentare dell‚Äôet√†
aumenta il ruolo del tatto) e dalla sua esperienza.&lt;/p&gt;

&lt;h2 id=&quot;memoria-aptica&quot;&gt;Memoria aptica&lt;/h2&gt;

&lt;p&gt;Nello studio della memoria umana √® sempre stata data molta importanza
agli effetti degli stimoli visivi e uditivi; d‚Äôaltro canto lo studio
della memoria aptica viene reso pi√π difficile dal fatto che gli stimoli
aptici possono essere facilmente modulati da quelli visivi (oppure da
una memoria visiva).&lt;/p&gt;

&lt;p&gt;Secondo Millar &lt;a class=&quot;citation&quot; href=&quot;#art:millar&quot;&gt;(Millar, S., 1999)&lt;/a&gt;, esiste nell‚Äôuomo una memoria aptica che √®
a breve termine e si limita a ricordare due o tre oggetti. Quando
vengono raccolte informazioni tramite il tatto, la loro rappresentazione
in memoria pu√≤ essere intrinseca a questa modalit√† o pi√π generica. Ad
esempio nell‚Äôesplorazione di piccoli pattern come il Braille o altre
tessiture, la rappresentazione avviene in termini di propriet√† aptiche;
se invece i pattern possono essere organizzati secondo strutture
spaziali, le informazioni su queste strutture vengono memorizzate
assieme alle informazioni puramente aptiche. La rappresentazione aptica
√® intramodale anche nel senso che tali informazioni possono essere usate
non solo dal tatto ma anche dalla vista; infatti spesso i pattern
esplorati apticamente vengono poi riconosciuti alla vista.&lt;/p&gt;

&lt;p&gt;I bambini riescono a discriminare quasi sempre gli oggetti che vengono
loro presentati sia utilizzando sempre una stessa modalit√† di analisi
(solo aptica o solo visiva), sia utilizzando modalit√† diverse (a volte
aptica, a volte visiva). L‚Äôaccuratezza di questi riconoscimenti tuttavia
decrementa se vengono analizzati oggetti non conosciuti; si pensa che
questo dipenda dal fatto che i soggetti effettuano una &lt;em&gt;categorizzazione
aptica&lt;/em&gt; degli oggetti in base alle loro propriet√† e alla disponibilit√† o
meno di una percezione visiva degli stessi.&lt;/p&gt;

&lt;p&gt;La memoria umana pu√≤ essere &lt;em&gt;esplicita&lt;/em&gt; o &lt;em&gt;implicita&lt;/em&gt;: la differenza tra
le due √® che nell‚Äôuso della memoria esplicita il soggetto
volontariamente ricerca informazioni tra i suoi ricordi. La memoria
aptica pu√≤ essere sia implicita che esplicita, ma con modalit√† diverse
rispetto alla memoria visiva.&lt;/p&gt;

&lt;h2 id=&quot;feedback-aptico&quot;&gt;Feedback aptico&lt;/h2&gt;

&lt;p&gt;Se si pensa al principio di azione‚Äìreazione di Newton, si capisce come
in natura non esiste il concetto di &lt;em&gt;feedback aptico&lt;/em&gt;. Le variabili
intensive (come la forza) descrivono in modo astratto le interazioni,
mentre le variabili estensive descrivono lo stato del fenomeno
osservato, e non √® possibile eseguire una separazione di questi due tipi
di variabili; le forze sono individuabili attraverso i loro effetti
sullo stato del fenomeno e tali effetti non sono necessariamente gli
stessi per fenomeni sotto la stessa influenza.&lt;/p&gt;

&lt;p&gt;Non si pu√≤ parlare quindi di feedback aptico nell‚Äôinterazione tra
oggetti fisici, ma solo nell‚Äôinterazione uomo‚Äìuomo o uomo‚Äìmacchina.
Sia l‚Äôuomo che le macchine infatti possono essere visti come formati da
sensori e attuatori (gli oggetti invece costituiscono entit√† non
separabili da questo punto di vista). Come abbiamo visto, nell‚Äôuomo
esistono vari tipi di recettori che raccolgono le informazioni e le
trasmettono al cervello; per quanto riguarda le macchine invece √®
evidente che, per poter ricevere e trasmettere una forza, devono essere
equipaggiate con sensori e attuatori artificiali. Come vedremo nel
capitolo dedicato ai dispositivi aptici, i dispositivi e le interfacce
aptiche posseggono queste caratteristiche.&lt;/p&gt;

&lt;h2 id=&quot;riferimenti&quot;&gt;Riferimenti&lt;/h2&gt;

&lt;ol class=&quot;bibliography&quot;&gt;&lt;li&gt;&lt;span id=&quot;art:gibson&quot;&gt;Gibson, J. J. (1962). Observation On Active Touch. &lt;i&gt;Psychological Review&lt;/i&gt;, &lt;i&gt;69&lt;/i&gt;, 477‚Äì490.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;art:loomis&quot;&gt;Loomis, J. M., &amp;amp; Lederman, S. J. (1986). Tactual perception. &lt;i&gt;Handbook of Perception and Human Performances&lt;/i&gt;, &lt;i&gt;2&lt;/i&gt;, 31‚Äì41.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;art:lederman5&quot;&gt;Lederman, S. J., &amp;amp; Klatzky, R. L. (1999). The haptic glance: A route to rapid object identification and manipulation. &lt;i&gt;Attention and Performance&lt;/i&gt;, &lt;i&gt;17: Cognitive regulation of performance: Interaction of theory and application&lt;/i&gt;, 165‚Äì196.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;book:chapentier&quot;&gt;Chapentier, A. (1891). &lt;i&gt;Experimental study of some aspects of weight perception&lt;/i&gt; (Vol. 3, pp. 122‚Äì135). Archives de Physiologie Normales et Pathologiques.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;art:lederman7&quot;&gt;Lederman, S. J., &amp;amp; Klatzky, R. L. (1987). Hand movements: A window into haptic object recognition. &lt;i&gt;Cognitive Psychology&lt;/i&gt;, &lt;i&gt;19&lt;/i&gt;, 342‚Äì368.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;art:lederman6&quot;&gt;Lederman, S. J., Klatzky, R. L., Collins, A., &amp;amp; Wardell, J. (1987). Exploring environments by hand or foot: Time-based heuristics for encoding distance in movement space. &lt;i&gt;Journal of Experimental Psychology: Learning, Memory and Cognition&lt;/i&gt;, &lt;i&gt;13&lt;/i&gt;, 606‚Äì614.&lt;/span&gt;&lt;/li&gt;
&lt;li&gt;&lt;span id=&quot;art:millar&quot;&gt;Millar, S. (1999). Memory in Touch. &lt;i&gt;Psicothema&lt;/i&gt;, &lt;i&gt;11&lt;/i&gt;, 747‚Äì767.&lt;/span&gt;&lt;/li&gt;&lt;/ol&gt;</content><author><name>Andrea Maglie</name></author><category term="haptic feedback" /><category term="pure data" /><summary type="html">Disclaimer: Il contenuto di questo post √® un estratto della mia tesi di laurea, realizzata nel 2006. Alcuni esempi potrebbero risultare obsoleti, tuttavia la componente teorica resta un buon riferimento per chi deve approcciarsi alla materia.</summary></entry><entry><title type="html">Una piattaforma per il rendering audio aptico di interazioni continue</title><link href="/piattaforma-rendering-audio-aptico-intro.html" rel="alternate" type="text/html" title="Una piattaforma per il rendering audio aptico di interazioni continue" /><published>2019-01-10T00:00:00+01:00</published><updated>2019-01-10T00:00:00+01:00</updated><id>/piattaforma-rendering-audio-aptico-intro</id><content type="html" xml:base="/piattaforma-rendering-audio-aptico-intro.html">&lt;p&gt;&lt;em&gt;Nel 2006 pubblicavo questa tesi dal titolo ‚ÄúUna piattaforma per il rendering audio aptico di interazioni continue‚Äù per la mia laurea in Ingegneria Informatica all‚ÄôUniversit√† di Padova. 9 mesi passati a lavorare su quello che sicuramente √® il progetto pi√π complesso e interessante sul quale abbia mai lavorato.&lt;/em&gt;&lt;/p&gt;

&lt;h1 id=&quot;indice-dei-capitoli&quot;&gt;Indice dei capitoli&lt;/h1&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;/percezione-aptica.html&quot;&gt;La percezione aptica&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/percezione-uditiva.html&quot;&gt;La percezione uditiva&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/pure-data-modelli-audio.html&quot;&gt;Pure Data e i modelli audio&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/dispositivi-aptici&quot;&gt;Dispositivi aptici&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/phantom-omni.html&quot;&gt;Il Phantom Omni&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/applicazione-phantom-friction.html&quot;&gt;L‚Äôapplicazione Phantom Friction&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;/esperimenti-percezione-audio-aptica.html&quot;&gt;Esperimenti sulla percezione audio-aptica&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h1 id=&quot;introduzione&quot;&gt;Introduzione&lt;/h1&gt;
&lt;p&gt;Se nel campo della computer graphics le tessiture (texture) sono state studiate estensivamente allo scopo di rendere sempre piuÃÄ reali gli ambienti virtuali, altrettanto non si puoÃÄ dire per quanto riguarda la computer haptics, dove con tale termine si intende la scienza che studia la simulazione e lo scambio tra utente e computer di informazioni legate al senso del tatto. Tuttavia, per creare un ambiente virtuale realistico, eÃÄ indispensabile fornire all‚Äôutente un ritorno di suono in corrispondenza al verificarsi di un evento (come un contatto tra oggetti).&lt;/p&gt;

&lt;p&gt;Quando tocchiamo la superficie di un oggetto reale ad occhi chiusi, raccogliamo principalmente due tipi di informazione: il primo tipo eÃÄ costituito dalle informazioni percepite tramite le dita e le mani, ovvero tramite il senso del tatto; il secondo tipo eÃÄ invece costituito dalle informazioni uditive, catturate dai suoni che vengono generati nel momento in cui muoviamo le nostre dita sulla superficie. La piattaforma descritta eÃÄ stata realizzata proprio basandosi su questi concetti, estendendoli all‚Äôinterazione con oggetti simulati.&lt;/p&gt;

&lt;p&gt;L‚Äôobiettivo a cui mira questa tesi eÃÄ la realizzazione di un sistema che permetta l‚Äôinterazione visiva, aptica e sonora con un ambiente virtuale (dove la parola aptica eÃÄ legata al senso di tocco attivo):
visiva in quanto l‚Äôutente puoÃÄ visualizzare su uno schermo gli oggetti virtuali con cui interagisce;
aptica percheÃÅ gli oggetti virtuali possono essere sentiti al tatto;
audio percheÃÅ, come in un ambiente reale, sia possibile ascoltare i suoni provocati dalle interazioni.&lt;/p&gt;

&lt;p&gt;In particolare l‚Äôattenzione viene concentrata sulla realizzazione di tessiture sulle superfici, le quali, attraverso il controllo di alcuni parametri di significato fisico, possano far sentire (sia tramite il tatto che l‚Äôudito) all‚Äôutente super ci di diverso livello di ruviditaÃÄ (non ci preoccupiamo invece delle tessiture grafiche). La sintesi dell‚Äôaudio non ha piuÃÄ come elemento centrale il segnale che viene prodotto e percepito, ma la sorgente del suono: attraverso modelli fisici si cerca quindi di descrivere la sorgente (o le sorgenti) sonore unitamente agli eventi che producono il suono stesso. Variando un insieme ristretto di parametri eÃÄ cosiÃÄ possibile simulare diverse sorgenti; gli stessi parametri possono poi essere utilizzati per controllare la simulazione aptica.&lt;/p&gt;

&lt;p&gt;La rappresentazione grafica dell‚Äôambiente tridimensionale virtuale eÃÄ stata fatta sfruttando le API OpenGL; si eÃÄ scelto di mantenere questa componente molto semplice, infatti l‚Äôambiente eÃÄ costituito da un solo oggetto che si presenta liscio e monocromatico alla vista. Per rendere possibile la rappresentazione aptica invece eÃÄ necessario eseguire un interfacciamento tra l‚Äôapplicazione grafica e un dispositivo di force feedback (il Phantom¬Æ Omni di SensAble Technologies in questo caso); utilizzando tale dispositivo saraÃÄ possibile toccare ogni oggetto dell‚Äôambiente virtuale. Infine la sintesi dell‚Äôaudio eÃÄ stata realizzata separatamente tramite l‚Äôambiente di programmazione visuale Pure Data, implementando dei modelli fisici la cui validitaÃÄ eÃÄ giaÃÄ consolidata. Le tre componenti possono essere eseguite contemporaneamente, realizzando una completa simulazione in tempo reale che puoÃÄ prestarsi ad una molteplicitaÃÄ di usi diversi.&lt;/p&gt;

&lt;p&gt;Nel primo capitolo si descrive brevemente il senso del tatto, in particolare come il corpo umano raccoglie ed elabora le informazioni tattili; nel secondo invece viene analizzato il modo di percepire da parte dell‚Äôuomo i suoni nell‚Äôambiente circostante.&lt;/p&gt;

&lt;p&gt;Nel terzo capitolo si illustrano per prima cosa i modelli fisici su cui viene basata la sintesi dell‚Äôaudio. I modelli comprendono la realizzazione di tessiture stocastiche a partire da un rumore filtrato e il rendering di suoni di contatto tramite una sintesi modale. Successivamente viene descritto il funzionamento dell‚Äôambiente di programmazione Pure Data e dei moduli esterni utilizzati; segue una spiegazione dettagliata su come sono stati implementati i modelli fisici in patch eseguibili in tale ambiente.&lt;/p&gt;

&lt;p&gt;Il capitolo 4 riporta una panoramica sui dispositivi aptici, mentre il capitolo successivo illustra nel dettaglio il dispositivo Phantom¬Æ Omni , unitamente al toolkit OpenHaptics utilizzato per la programmazione di tale dispositivo.&lt;/p&gt;

&lt;p&gt;Nel capitolo 6 si tratta la programmazione dell‚Äôinterfaccia grafica e dell‚Äôinterfaccia aptica; viene spiegato come avviene il rendering delle forze e come sono realizzate le tessiture aptiche. Infine si spiega come queste componenti siano state integrate tra loro e come avviene la comunicazione tra l‚Äôinterfaccia grafica/aptica e l‚Äôalgoritmo di sintesi audio creato ed eseguito in Pure Data.&lt;/p&gt;

&lt;p&gt;La parte finale (capitolo 7) eÃÄ dedicata agli esperimenti di percezione bimodale audio aptica: dopo aver presentato una panoramica sugli studi che sono giaÃÄ stati condotti in questo campo, vengono illustrate le differenze rispetto a questi e gli aspetti innovativi degli studi svolti in questa tesi. Sono stati svolti tre esperimenti: il primo in condizione di variazione concorde degli stimoli audio e aptico, gli altri in condizioni di variazione discorde dei due stimoli. In conclusione vengono riportati i risultati e le discussioni sulle percezioni avute dai partecipanti.&lt;/p&gt;</content><author><name>Andrea Maglie</name></author><category term="haptic feedback" /><category term="pure data" /><summary type="html">Nel 2006 pubblicavo questa tesi dal titolo ‚ÄúUna piattaforma per il rendering audio aptico di interazioni continue‚Äù per la mia laurea in Ingegneria Informatica all‚ÄôUniversit√† di Padova. 9 mesi passati a lavorare su quello che sicuramente √® il progetto pi√π complesso e interessante sul quale abbia mai lavorato.</summary><media:thumbnail xmlns:media="http://search.yahoo.com/mrss/" url="/hero-phantomomni.jpg" /></entry></feed>